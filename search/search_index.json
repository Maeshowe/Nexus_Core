{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OmniData Nexus Core","text":"<p>Modular, asynchronous DataLoader framework for financial and macroeconomic data aggregation.</p>"},{"location":"#overview","title":"Overview","text":"<p>OmniData Nexus Core provides a unified interface for fetching and normalizing data from multiple financial APIs:</p> Provider Endpoints Data Types FMP Ultimate 13 Fundamentals, financials, ratios, insider trading Polygon.io 4 Market data, trades, options snapshots FRED 32 series Macroeconomic indicators"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Unified Interface - Single <code>DataLoader</code> class for all providers</li> <li>Async/Await - Non-blocking I/O with <code>aiohttp</code></li> <li>Resilience Patterns - Circuit breaker, exponential backoff, rate limiting</li> <li>Intelligent Caching - Filesystem JSON with atomic writes and TTL</li> <li>QoS Management - Provider-specific concurrency limits</li> <li>Health Monitoring - Real-time API status and error tracking</li> <li>Operating Modes - LIVE (API calls) and READ_ONLY (cache only)</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import asyncio\nimport aiohttp\nfrom data_loader import DataLoader\n\nasync def main():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # Fetch company profile from FMP\n        profile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        print(f\"Company: {profile.data['companyName']}\")\n\n        # Fetch macroeconomic data from FRED\n        cpi = await loader.get_fred_data(session, \"series\", series_id=\"CPIAUCSL\")\n        print(f\"CPI observations: {len(cpi.data)}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      DataLoader                              \u2502\n\u2502                   (Unified Interface)                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502 FMP Provider\u2502  \u2502Polygon Prov.\u2502  \u2502 FRED Prov.  \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Resilience Layer                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502Circuit Breaker\u2502 \u2502 Retry/Backoff\u2502 \u2502 QoS Router  \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502  HTTP Client \u2502 \u2502 Cache Manager\u2502 \u2502Health Monitor\u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"-   :material-download:{ .lg .middle } __Installation__      ---      Install OmniData Nexus Core from source      [:octicons-arrow-right-24: Installation](getting-started/installation.md)  -   :material-cog:{ .lg .middle } __Configuration__      ---      Configure API keys and settings      [:octicons-arrow-right-24: Configuration](getting-started/configuration.md)  -   :material-rocket-launch:{ .lg .middle } __Quick Start__      ---      Start fetching data in minutes      [:octicons-arrow-right-24: Quick Start](getting-started/quickstart.md)"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"ACTION_PLAN/","title":"Action Plan","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: \u2705 COMPLETE Created by: Lead Agent</p>"},{"location":"ACTION_PLAN/#1-executive-summary","title":"1. Executive Summary","text":"Metric Value Total Tasks 28 Total Effort 186-248 hours (realistic estimate) Duration 5-7 weeks at sustainable pace (8-10 hours/week) Sustainable Pace 8-10 hours/week (research-driven, flexible) Milestones 4 (Foundation, Core Providers, Resilience, Testing &amp; Polish)"},{"location":"ACTION_PLAN/#milestone-timeline","title":"Milestone Timeline","text":"<pre><code>Week 1-2        Week 3-4        Week 5-6        Week 7+\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nM1: Foundation  M2: Providers   M3: Resilience  M4: Testing\n[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588]    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588]    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588]    [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588]\n \u2705 COMPLETE     \u2705 COMPLETE      \u2705 COMPLETE      \u2705 COMPLETE\n</code></pre>"},{"location":"ACTION_PLAN/#final-results","title":"Final Results","text":"Metric Target Actual Tests ~200 441 Coverage &gt;80% 92% TIER 1 Coverage &gt;90% 95-99% CI/CD GitHub Actions \u2705 Configured"},{"location":"ACTION_PLAN/#reality-check","title":"Reality Check","text":"<p>This is a realistic timeline for a solo developer working 8-10 hours per week on a research tool with no hard deadlines. Effort estimates include: - Actual coding time - Testing time (not just \"add tests later\") - Debugging and iteration - Documentation updates - Learning curve for new patterns</p> <p>Critical Path: M1 \u2192 M2 \u2192 M3 \u2192 M4 (sequential dependencies)</p>"},{"location":"ACTION_PLAN/#2-milestones","title":"2. Milestones","text":""},{"location":"ACTION_PLAN/#m1-foundation-infrastructure","title":"M1: Foundation &amp; Infrastructure","text":"<p>Target: Week 1-2 (16-20 hours) Effort: 38-52 hours (realistic) Status: \u2705 COMPLETE</p> ID Task Effort Priority Deps Status T-001 Project setup and dependencies 2h P0 - \u2610 T-002 Configuration manager (.env loading) 4-6h P0 T-001 \u2610 T-003 HTTP client layer (aiohttp wrapper) 6-8h P0 T-001 \u2610 T-004 Cache manager (filesystem JSON) 8-12h P0 T-001 \u2610 T-005 Health monitor (metrics tracking) 6-8h P0 T-001 \u2610 T-006 Base data provider interface 6-8h P0 T-003 \u2610 T-007 Logging setup with API key sanitization 6-8h P0 T-002 \u2610 <p>Success Criteria: - [ ] Project structure created (<code>src/data_loader/</code>, <code>tests/</code>, <code>docs/</code>, <code>data/</code>) - [ ] Dependencies installed (aiohttp, python-dotenv, pytest) - [ ] .env.example created with placeholder keys - [ ] Cache manager can write/read JSON atomically - [ ] Health monitor tracks basic counters - [ ] BaseDataProvider abstract class defined - [ ] API keys sanitized in all log output</p> <p>Deliverables: - Working project skeleton - Passing lint (ruff/flake8) and type checks (mypy) - Basic unit tests for config, cache, health monitor - Documentation: README.md with setup instructions</p>"},{"location":"ACTION_PLAN/#m2-core-providers","title":"M2: Core Providers","text":"<p>Target: Week 3-4 (16-20 hours) Effort: 64-88 hours (realistic) Status: \u2705 COMPLETE</p> ID Task Effort Priority Deps Status T-008 FMP provider (13 endpoints) 24-32h P0 T-006 \u2610 T-009 Polygon provider (4 endpoints) 12-16h P0 T-006 \u2610 T-010 FRED provider (32 series + base) 16-24h P0 T-006 \u2610 T-011 Provider integration tests (mocked HTTP) 12-16h P0 T-008,009,010 \u2610 <p>Success Criteria: - [ ] FMP provider supports all 13 endpoints (screener, profile, quote, historical_price, earnings_calendar, balance_sheet, income_statement, cash_flow, ratios, growth, key_metrics, insider_trading, institutional_ownership) - [ ] Polygon provider supports 4 endpoints (aggs_daily, trades, options_snapshot, market_snapshot) - [ ] FRED provider supports 32 series + base API - [ ] All providers normalize responses to consistent format - [ ] Cache keys generated correctly per provider strategy - [ ] Integration tests passing with aioresponses mocks - [ ] Provider-specific error handling (429, 5xx, timeouts)</p> <p>Deliverables: - 3 working provider implementations - ~40 integration tests (provider + HTTP mocking) - JSON fixtures in <code>tests/fixtures/{fmp,polygon,fred}/</code> - Provider documentation with endpoint examples</p> <p>Technical Notes: - FMP is the most complex (13 endpoints, varying response structures) - FRED has 32 series but simpler endpoint pattern - Polygon has date range complexity for aggregates - Mock all HTTP responses using aioresponses library</p>"},{"location":"ACTION_PLAN/#m3-resilience-layer","title":"M3: Resilience Layer","text":"<p>Target: Week 5-6 (16-20 hours) Effort: 52-72 hours (realistic) Status: \u2705 COMPLETE</p> ID Task Effort Priority Deps Status T-012 QoS Semaphore Router 8-12h P0 T-006 \u2610 T-013 Circuit Breaker Manager 12-16h P0 T-005 \u2610 T-014 Retry handler (exponential backoff + jitter) 8-12h P0 T-003 \u2610 T-015 Rate limit handling (HTTP 429) 6-8h P0 T-014 \u2610 T-016 DataLoader unified interface 10-14h P0 T-012,013,014 \u2610 T-017 Operating modes (LIVE/READ-ONLY) 4-6h P0 T-016 \u2610 T-018 Resilience integration tests 4-8h P0 T-012,013,014 \u2610 <p>Success Criteria: - [ ] QoS Router enforces concurrency: FMP=3, Polygon=10, FRED=1 - [ ] Circuit Breaker opens at &gt;20% error rate - [ ] Circuit Breaker recovery flow: OPEN \u2192 HALF-OPEN \u2192 CLOSED - [ ] Exponential backoff: ~1s, ~2s, ~4s with jitter - [ ] Retry only on 5xx/timeout (not 4xx) - [ ] HTTP 429 handling: parse Retry-After header - [ ] DataLoader orchestrates all resilience components - [ ] READ-ONLY mode prevents API calls (cache-only) - [ ] LIVE mode normal operation - [ ] Integration tests validate state machine transitions</p> <p>Deliverables: - QoS router with per-provider semaphores - Circuit breaker with 3-state FSM - Retry handler with backoff calculator - DataLoader unified interface - ~30 unit tests for resilience components - ~15 integration tests for orchestration</p> <p>Technical Notes: - Circuit Breaker is TIER 1 (&gt;90% coverage required) - QoS Router is TIER 1 (&gt;90% coverage required) - Use asyncio.Semaphore for concurrency control - Store circuit breaker state in-memory (no persistence) - DataLoader is the main entry point for consumers</p>"},{"location":"ACTION_PLAN/#m4-testing-polish","title":"M4: Testing &amp; Polish","text":"<p>Target: Week 7+ (variable) Effort: 32-36 hours (realistic) Status: \u2705 COMPLETE</p> ID Task Effort Priority Deps Status T-019 Unit test suite completion (TIER 1 &gt;90%) 12-16h P0 All \u2610 T-020 E2E test scenarios (10 critical paths) 8-12h P0 T-016 \u2610 T-021 Security tests (API key sanitization) 4-6h P0 T-007 \u2610 T-022 Coverage analysis and gap filling 4-6h P0 T-019,020 \u2610 T-023 CI/CD pipeline setup (GitHub Actions) 4-6h P1 T-022 \u2610 T-024 Documentation polish (README, examples) 6-8h P1 All \u2610 T-025 Pre-commit hooks (secret scanning) 2-3h P1 T-007 \u2610 T-026 Manual smoke test with real APIs 2-3h P1 All \u2610 <p>Success Criteria: - [ ] Overall coverage &gt;80% (pytest-cov) - [ ] TIER 1 coverage &gt;90% (circuit_breaker, qos_router, sanitization, cache atomic) - [ ] All ~200 tests passing - [ ] Test execution time &lt;2 minutes - [ ] TC-001 through TC-106 passing (critical test cases) - [ ] No API keys in logs/cache/git (security tests) - [ ] CI pipeline green - [ ] README.md with setup instructions &lt;5 minutes - [ ] Working examples in <code>examples/</code> directory - [ ] Pre-commit hook blocks commits with secrets - [ ] Manual validation: fetch real data from all 3 providers</p> <p>Deliverables: - ~150 unit tests - ~10 E2E tests - ~10 security tests - Coverage reports (HTML + terminal) - CI/CD pipeline (.github/workflows/) - Pre-commit configuration - Updated README.md - Example usage scripts - Manual test report</p> <p>Technical Notes: - This is where the effort multiplier really applies - \"Add tests\" is never quick - expect 8-16h minimum - Coverage gap filling is detective work (2-4h) - E2E tests need careful orchestration (mocking all providers)</p>"},{"location":"ACTION_PLAN/#3-task-details","title":"3. Task Details","text":""},{"location":"ACTION_PLAN/#t-001-project-setup-and-dependencies","title":"T-001: Project Setup and Dependencies","text":"Field Value Milestone M1 Effort 2 hours Priority P0 Dependencies None <p>Description: Create project directory structure, initialize git repository, set up Python virtual environment, and install core dependencies.</p> <p>Implementation Steps: 1. Create directory structure matching architecture design 2. Initialize git repository with .gitignore 3. Create Python virtual environment (Python 3.9+) 4. Create requirements.txt with pinned versions 5. Create requirements-dev.txt for development dependencies 6. Create .env.example with placeholder API keys 7. Create basic README.md</p> <p>Acceptance Criteria: - [ ] Directory structure exists: <code>src/data_loader/</code>, <code>tests/</code>, <code>docs/</code>, <code>data/</code>, <code>logs/</code> - [ ] Virtual environment activated - [ ] Dependencies installed: aiohttp&gt;=3.8, python-dotenv&gt;=0.19 - [ ] Dev dependencies: pytest&gt;=7.0, pytest-cov&gt;=3.0, pytest-asyncio&gt;=0.21, aioresponses&gt;=0.7, mypy&gt;=1.0, ruff - [ ] .gitignore includes: <code>.env</code>, <code>__pycache__</code>, <code>*.pyc</code>, <code>venv/</code>, <code>data/*</code>, <code>logs/*</code>, <code>.mypy_cache/</code>, <code>.pytest_cache/</code>, <code>htmlcov/</code> - [ ] .env.example created with: <code>FMP_KEY=your_fmp_key_here</code>, <code>POLYGON_KEY=</code>, <code>FRED_KEY=</code> - [ ] README.md has installation instructions</p> <p>Files to Create: - <code>requirements.txt</code> - <code>requirements-dev.txt</code> - <code>.gitignore</code> - <code>.env.example</code> - <code>README.md</code> - <code>src/data_loader/__init__.py</code> - <code>tests/conftest.py</code></p> <p>Commands: <pre><code># Create project structure\nmkdir -p src/data_loader/providers tests/{unit,integration,e2e,fixtures} docs data logs\n\n# Initialize git\ngit init\n# (create .gitignore first)\ngit add .\ngit commit -m \"Initial project structure\"\n\n# Create virtual environment\npython3.9 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\npip install -r requirements-dev.txt\n\n# Verify installation\npython -c \"import aiohttp; import dotenv; print('Dependencies OK')\"\npytest --version\nmypy --version\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-002-configuration-manager-env-loading","title":"T-002: Configuration Manager (.env loading)","text":"Field Value Milestone M1 Effort 4-6 hours Priority P0 Dependencies T-001 <p>Description: Implement configuration manager that loads API keys and settings from environment variables (.env file). Includes validation, defaults, and error handling for missing keys.</p> <p>Implementation Notes: - Use python-dotenv for .env loading - Separate required keys (API keys) from optional settings (TTL, timeouts) - Validate keys at startup (not empty strings) - Provide sensible defaults for non-secret settings - Support environment variable override (for CI)</p> <p>Acceptance Criteria: - [ ] Config loads from .env file if present - [ ] Config loads from environment variables (higher priority than .env) - [ ] Missing required keys raise ConfigError at startup - [ ] Optional settings have defaults: <code>CACHE_TTL_DAYS=7</code>, <code>MAX_RETRIES=3</code>, <code>CIRCUIT_BREAKER_THRESHOLD=0.2</code> - [ ] Config is a singleton (single instance per process) - [ ] Unit tests verify: .env parsing, defaults, validation, missing keys error</p> <p>Files to Create: - <code>src/data_loader/config.py</code> - <code>tests/unit/test_config_manager.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/config.py\nfrom dotenv import load_dotenv\nimport os\n\nclass Config:\n    def __init__(self):\n        load_dotenv()  # Load from .env file\n\n        # Required API keys\n        self.fmp_key = self._get_required(\"FMP_KEY\")\n        self.polygon_key = self._get_required(\"POLYGON_KEY\")\n        self.fred_key = self._get_required(\"FRED_KEY\")\n\n        # Optional settings with defaults\n        self.cache_ttl_days = int(os.getenv(\"CACHE_TTL_DAYS\", \"7\"))\n        self.max_retries = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n        self.circuit_breaker_threshold = float(os.getenv(\"CIRCUIT_BREAKER_THRESHOLD\", \"0.2\"))\n        # ... more settings\n\n    def _get_required(self, key: str) -&gt; str:\n        value = os.getenv(key)\n        if not value:\n            raise ConfigError(f\"Required environment variable {key} not set\")\n        return value\n\n# Singleton pattern\n_config_instance = None\ndef get_config() -&gt; Config:\n    global _config_instance\n    if _config_instance is None:\n        _config_instance = Config()\n    return _config_instance\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-003-http-client-layer-aiohttp-wrapper","title":"T-003: HTTP Client Layer (aiohttp wrapper)","text":"Field Value Milestone M1 Effort 6-8 hours Priority P0 Dependencies T-001 <p>Description: Create async HTTP client wrapper around aiohttp.ClientSession with timeout management, connection pooling, TLS enforcement, and request/response logging.</p> <p>Implementation Notes: - Enforce HTTPS only (reject http:// URLs) - Configure timeouts: connect=5s, read=30s, total=60s - Connection pooling: limit=100 per host - Timeout error handling (aiohttp.ClientTimeout) - Request/response logging (sanitize URLs with API keys) - Session reuse (long-lived ClientSession)</p> <p>Acceptance Criteria: - [ ] HTTP client enforces HTTPS (raises error for http://) - [ ] Configurable timeouts per request - [ ] Connection pooling configured - [ ] TLS certificate validation enabled - [ ] Request logging includes: method, URL (sanitized), status code - [ ] Response logging includes: status, headers (selected), body size - [ ] Error handling for: timeout, connection errors, TLS errors - [ ] Unit tests verify: HTTPS enforcement, timeout config, error handling</p> <p>Files to Create: - <code>src/data_loader/http_client.py</code> - <code>tests/unit/test_http_client.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/http_client.py\nimport aiohttp\nfrom typing import Optional, Dict, Any\n\nclass HTTPClient:\n    def __init__(self, timeout: aiohttp.ClientTimeout = None):\n        self.timeout = timeout or aiohttp.ClientTimeout(\n            total=60, connect=5, sock_read=30\n        )\n        self._session: Optional[aiohttp.ClientSession] = None\n\n    async def get_session(self) -&gt; aiohttp.ClientSession:\n        if self._session is None or self._session.closed:\n            connector = aiohttp.TCPConnector(limit=100)\n            self._session = aiohttp.ClientSession(\n                connector=connector,\n                timeout=self.timeout\n            )\n        return self._session\n\n    async def get(self, url: str, **kwargs) -&gt; Dict[str, Any]:\n        self._enforce_https(url)\n        session = await self.get_session()\n        async with session.get(url, **kwargs) as response:\n            response.raise_for_status()\n            return await response.json()\n\n    def _enforce_https(self, url: str):\n        if not url.startswith(\"https://\"):\n            raise ValueError(f\"HTTPS required, got: {url}\")\n\n    async def close(self):\n        if self._session and not self._session.closed:\n            await self._session.close()\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-004-cache-manager-filesystem-json","title":"T-004: Cache Manager (filesystem JSON)","text":"Field Value Milestone M1 Effort 8-12 hours Priority P0 Dependencies T-001 <p>Description: Implement cache manager with atomic writes (temp + rename pattern), TTL expiration, provider-specific directory structure, and JSON serialization. This is TIER 1 (&gt;90% coverage required).</p> <p>Implementation Notes: - Atomic writes: write to temp file \u2192 rename to final path (prevents corruption) - Provider-specific paths: <code>data/fmp_cache/{endpoint}/{date}/{symbol}.json</code> - Cache key generation: MD5 hash of (provider, endpoint, params) - TTL check: compare file mtime to current time - Thread-safe file operations (though system is single-process) - Validate JSON before write (catch serialization errors early)</p> <p>Acceptance Criteria: - [ ] Atomic write pattern: temp file created first, renamed on success - [ ] TTL expiration: files older than TTL considered stale (return None) - [ ] Provider-specific directory creation (auto-create if missing) - [ ] Cache key generation consistent (same params \u2192 same key) - [ ] get_cached() returns None for: missing file, expired TTL, corrupted JSON - [ ] set_cache() rollback: temp file deleted if error during write - [ ] Unit tests verify: atomic writes, TTL expiration, concurrent writes (mock), error rollback - [ ] &gt;90% test coverage (TIER 1)</p> <p>Files to Create: - <code>src/data_loader/cache.py</code> - <code>tests/unit/test_cache_manager.py</code> - <code>tests/integration/test_cache_filesystem.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/cache.py\nimport json\nimport hashlib\nimport os\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Dict, Any\n\nclass CacheManager:\n    def __init__(self, base_dir: str = \"data\", ttl_days: int = 7):\n        self.base_dir = Path(base_dir)\n        self.ttl_seconds = ttl_days * 86400\n\n    def get_cached(self, cache_key: str, provider: str) -&gt; Optional[Dict[str, Any]]:\n        cache_path = self._get_cache_path(cache_key, provider)\n        if not cache_path.exists():\n            return None\n\n        # Check TTL\n        mtime = cache_path.stat().st_mtime\n        age_seconds = datetime.now().timestamp() - mtime\n        if age_seconds &gt; self.ttl_seconds:\n            return None\n\n        try:\n            with open(cache_path, 'r') as f:\n                return json.load(f)\n        except (json.JSONDecodeError, IOError):\n            return None\n\n    def set_cache(self, cache_key: str, provider: str, data: Dict[str, Any]):\n        cache_path = self._get_cache_path(cache_key, provider)\n        cache_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Atomic write: temp file + rename\n        temp_path = cache_path.with_suffix('.tmp')\n        try:\n            with open(temp_path, 'w') as f:\n                json.dump(data, f, indent=2)\n            temp_path.rename(cache_path)  # Atomic on POSIX\n        except Exception as e:\n            if temp_path.exists():\n                temp_path.unlink()  # Cleanup temp file\n            raise CacheWriteError(f\"Cache write failed: {e}\")\n\n    def _get_cache_path(self, cache_key: str, provider: str) -&gt; Path:\n        # Provider-specific path strategy\n        return self.base_dir / f\"{provider}_cache\" / f\"{cache_key}.json\"\n\n    @staticmethod\n    def generate_cache_key(provider: str, endpoint: str, params: Dict[str, Any]) -&gt; str:\n        # MD5 hash for consistent keys\n        key_data = f\"{provider}:{endpoint}:{sorted(params.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-005-health-monitor-metrics-tracking","title":"T-005: Health Monitor (metrics tracking)","text":"Field Value Milestone M1 Effort 6-8 hours Priority P0 Dependencies T-001 <p>Description: Implement health monitor to track per-provider metrics: request counts, error counts, error rate, rate limit consumption, circuit breaker state. Provides aggregated health report.</p> <p>Implementation Notes: - In-memory counters (no persistence) - Per-provider metrics (separate counters for FMP, Polygon, FRED) - Thread-safe increments (though system is single-process async) - Error rate calculation: errors / total_requests - Reset method for testing - Health report format: JSON-serializable dict</p> <p>Acceptance Criteria: - [ ] Track per-provider: total_requests, error_count, last_error_message - [ ] Calculate error_rate dynamically - [ ] Store circuit_state (external update from CircuitBreaker) - [ ] get_health_report() returns dict with all provider stats - [ ] reset() clears all counters (for testing) - [ ] Unit tests verify: counter increments, error rate calculation, reset</p> <p>Files to Create: - <code>src/data_loader/health.py</code> - <code>tests/unit/test_health_monitor.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/health.py\nfrom typing import Dict, Any\nfrom datetime import datetime\n\nclass HealthMonitor:\n    def __init__(self):\n        self._stats = {\n            'fmp': {'requests': 0, 'errors': 0, 'last_error': None, 'circuit_state': 'CLOSED'},\n            'polygon': {'requests': 0, 'errors': 0, 'last_error': None, 'circuit_state': 'CLOSED'},\n            'fred': {'requests': 0, 'errors': 0, 'last_error': None, 'circuit_state': 'CLOSED'},\n        }\n\n    def record_request(self, provider: str):\n        self._stats[provider]['requests'] += 1\n\n    def record_error(self, provider: str, error_message: str):\n        self._stats[provider]['errors'] += 1\n        self._stats[provider]['last_error'] = error_message\n\n    def update_circuit_state(self, provider: str, state: str):\n        self._stats[provider]['circuit_state'] = state\n\n    def get_health_report(self) -&gt; Dict[str, Any]:\n        report = {'timestamp': datetime.now().isoformat(), 'providers': {}}\n        for provider, stats in self._stats.items():\n            total = stats['requests']\n            errors = stats['errors']\n            error_rate = errors / total if total &gt; 0 else 0.0\n            report['providers'][provider] = {\n                'total_requests': total,\n                'errors': errors,\n                'error_rate': round(error_rate, 3),\n                'circuit_state': stats['circuit_state'],\n                'last_error': stats['last_error']\n            }\n        return report\n\n    def reset(self):\n        for provider in self._stats:\n            self._stats[provider] = {'requests': 0, 'errors': 0, 'last_error': None, 'circuit_state': 'CLOSED'}\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-006-base-data-provider-interface","title":"T-006: Base Data Provider Interface","text":"Field Value Milestone M1 Effort 6-8 hours Priority P0 Dependencies T-003 <p>Description: Define abstract base class for data providers with template methods for authentication, fetching, normalization, and cache key generation. Enables plugin architecture for future providers.</p> <p>Implementation Notes: - Use Python ABC (Abstract Base Class) - Template method pattern: fetch() orchestrates auth \u2192 request \u2192 normalize - Abstract methods: _authenticate(), _make_request(), _normalize_response() - Concrete methods: fetch(), generate_cache_key() - Type hints for all methods - Docstrings with examples</p> <p>Acceptance Criteria: - [ ] BaseDataProvider is abstract (cannot instantiate) - [ ] Abstract methods defined: _authenticate(), _make_request(), _normalize_response() - [ ] fetch() template method orchestrates workflow - [ ] generate_cache_key() concrete implementation - [ ] Type hints for all parameters and returns - [ ] Google-style docstrings - [ ] Unit tests verify: abstract enforcement, template method flow</p> <p>Files to Create: - <code>src/data_loader/providers/base.py</code> - <code>src/data_loader/providers/__init__.py</code> - <code>tests/unit/test_base_provider.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/providers/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nimport hashlib\n\nclass BaseDataProvider(ABC):\n    \"\"\"\n    Abstract base class for data providers.\n\n    Subclasses must implement:\n    - _authenticate(): Return authentication parameters (headers, query params)\n    - _make_request(): Execute HTTP request and return raw response\n    - _normalize_response(): Transform provider-specific response to standard format\n\n    Example:\n        class FMPProvider(BaseDataProvider):\n            def _authenticate(self):\n                return {'apikey': self.api_key}\n\n            def _make_request(self, endpoint, params):\n                url = f\"{self.base_url}/{endpoint}\"\n                return await self.http_client.get(url, params={**params, **self._authenticate()})\n    \"\"\"\n\n    def __init__(self, api_key: str, http_client):\n        self.api_key = api_key\n        self.http_client = http_client\n\n    async def fetch(self, endpoint: str, **params) -&gt; Dict[str, Any]:\n        \"\"\"\n        Fetch data from provider endpoint.\n\n        Template method that orchestrates:\n        1. Authentication\n        2. HTTP request\n        3. Response normalization\n        \"\"\"\n        auth_params = self._authenticate()\n        raw_response = await self._make_request(endpoint, {**params, **auth_params})\n        normalized = self._normalize_response(raw_response, endpoint)\n        return normalized\n\n    @abstractmethod\n    def _authenticate(self) -&gt; Dict[str, str]:\n        \"\"\"Return authentication parameters (headers or query params).\"\"\"\n        pass\n\n    @abstractmethod\n    async def _make_request(self, endpoint: str, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute HTTP request and return raw JSON response.\"\"\"\n        pass\n\n    @abstractmethod\n    def _normalize_response(self, response: Dict[str, Any], endpoint: str) -&gt; Dict[str, Any]:\n        \"\"\"Transform provider-specific response to standard format.\"\"\"\n        pass\n\n    def generate_cache_key(self, endpoint: str, params: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate consistent cache key from endpoint and parameters.\"\"\"\n        key_data = f\"{self.__class__.__name__}:{endpoint}:{sorted(params.items())}\"\n        return hashlib.md5(key_data.encode()).hexdigest()\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-007-logging-setup-with-api-key-sanitization","title":"T-007: Logging Setup with API Key Sanitization","text":"Field Value Milestone M1 Effort 6-8 hours Priority P0 Dependencies T-002 <p>Description: Configure Python logging with rotating file handler, structured log format, and API key sanitization filter. This is TIER 1 security requirement (&gt;90% coverage).</p> <p>Implementation Notes: - Use Python logging module (standard library) - Rotating file handler: 10MB max size, 5 backup files - Log format: timestamp, level, component, message - Sanitization filter: regex-based key redaction - Redact patterns: API keys in URLs, headers, error messages - Replace with: <code>***REDACTED***</code> - Unit tests verify: keys redacted in all log levels, regex patterns</p> <p>Acceptance Criteria: - [ ] Logging configured to file: <code>logs/nexus_core.log</code> - [ ] Rotating handler: max 10MB, 5 backups - [ ] Log format includes: timestamp (ISO 8601), level, logger name, message - [ ] Sanitization filter active on all handlers - [ ] API keys redacted in: URLs, error messages, debug logs - [ ] Regex patterns cover: <code>apikey=XXX</code>, <code>api_key=XXX</code>, <code>Authorization: Bearer XXX</code> - [ ] Unit tests verify: redaction patterns, no keys in output - [ ] &gt;90% test coverage (TIER 1)</p> <p>Files to Create: - <code>src/data_loader/logging_config.py</code> - <code>tests/unit/test_logging_sanitization.py</code> - <code>tests/integration/test_logging_integration.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/logging_config.py\nimport logging\nfrom logging.handlers import RotatingFileHandler\nimport re\n\nclass APIKeySanitizer(logging.Filter):\n    \"\"\"Filter to redact API keys from log messages.\"\"\"\n\n    PATTERNS = [\n        (re.compile(r'(apikey|api_key)=[^&amp;\\s]+', re.IGNORECASE), r'\\1=***REDACTED***'),\n        (re.compile(r'Authorization:\\s*Bearer\\s+[^\\s]+', re.IGNORECASE), r'Authorization: Bearer ***REDACTED***'),\n        (re.compile(r'[a-f0-9]{32,}', re.IGNORECASE), r'***REDACTED***'),  # MD5-like keys\n    ]\n\n    def filter(self, record: logging.LogRecord) -&gt; bool:\n        record.msg = self._sanitize(str(record.msg))\n        if record.args:\n            record.args = tuple(self._sanitize(str(arg)) for arg in record.args)\n        return True\n\n    def _sanitize(self, message: str) -&gt; str:\n        for pattern, replacement in self.PATTERNS:\n            message = pattern.sub(replacement, message)\n        return message\n\ndef setup_logging(log_file: str = \"logs/nexus_core.log\", level: int = logging.INFO):\n    \"\"\"Configure logging with API key sanitization.\"\"\"\n    import os\n    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n\n    handler = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n        datefmt='%Y-%m-%dT%H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n    handler.addFilter(APIKeySanitizer())\n\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n    root_logger.addHandler(handler)\n\n    return root_logger\n</code></pre></p>"},{"location":"ACTION_PLAN/#t-008-fmp-provider-13-endpoints","title":"T-008: FMP Provider (13 endpoints)","text":"Field Value Milestone M2 Effort 24-32 hours Priority P0 Dependencies T-006 <p>Description: Implement FMP Ultimate provider with support for 13 endpoints: screener, profile, quote, historical_price, earnings_calendar, balance_sheet, income_statement, cash_flow, ratios, growth, key_metrics, insider_trading, institutional_ownership. Most complex provider due to endpoint variety.</p> <p>Implementation Notes: - Inherit from BaseDataProvider - Base URL: <code>https://financialmodelingprep.com/api/v3/</code> - Authentication: <code>apikey</code> query parameter - Endpoint-specific normalization (each has different schema) - Cache key strategy: <code>{endpoint}/{date}/{symbol}.json</code> - Error handling: 429 rate limit, 401 invalid key, 404 not found - JSON fixtures for testing: record real responses once</p> <p>Acceptance Criteria: - [ ] All 13 endpoints implemented - [ ] Authentication via <code>apikey</code> query param - [ ] Normalization handles: missing fields, null values, varying schemas - [ ] Cache key generation per endpoint - [ ] Error handling: 401 (bad key), 429 (rate limit), 404 (not found), 5xx (server error) - [ ] Unit tests per endpoint: normalization logic - [ ] Integration tests: HTTP mocking with aioresponses - [ ] JSON fixtures in <code>tests/fixtures/fmp/</code></p> <p>Files to Create: - <code>src/data_loader/providers/fmp.py</code> - <code>tests/unit/test_fmp_provider.py</code> - <code>tests/integration/test_fmp_integration.py</code> - <code>tests/fixtures/fmp/profile_AAPL.json</code> (and 12 more)</p> <p>Implementation Example: <pre><code># src/data_loader/providers/fmp.py\nfrom .base import BaseDataProvider\nfrom typing import Dict, Any\n\nclass FMPProvider(BaseDataProvider):\n    \"\"\"Financial Modeling Prep API provider.\"\"\"\n\n    BASE_URL = \"https://financialmodelingprep.com/api/v3\"\n\n    ENDPOINTS = {\n        'screener': '/stock-screener',\n        'profile': '/profile/{symbol}',\n        'quote': '/quote/{symbol}',\n        'historical_price': '/historical-price-full/{symbol}',\n        'earnings_calendar': '/earnings-calendar/{symbol}',\n        'balance_sheet': '/balance-sheet-statement/{symbol}',\n        'income_statement': '/income-statement/{symbol}',\n        'cash_flow': '/cash-flow-statement/{symbol}',\n        'ratios': '/ratios/{symbol}',\n        'growth': '/financial-growth/{symbol}',\n        'key_metrics': '/key-metrics/{symbol}',\n        'insider_trading': '/insider-trading/{symbol}',\n        'institutional_ownership': '/institutional-holder/{symbol}',\n    }\n\n    def _authenticate(self) -&gt; Dict[str, str]:\n        return {'apikey': self.api_key}\n\n    async def _make_request(self, endpoint: str, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Replace {symbol} in path\n        path = self.ENDPOINTS[endpoint].format(**params)\n        url = f\"{self.BASE_URL}{path}\"\n\n        # Add authentication\n        query_params = {**params, **self._authenticate()}\n        query_params.pop('symbol', None)  # Symbol already in path\n\n        return await self.http_client.get(url, params=query_params)\n\n    def _normalize_response(self, response: Dict[str, Any], endpoint: str) -&gt; Dict[str, Any]:\n        # Endpoint-specific normalization\n        if endpoint == 'profile':\n            return self._normalize_profile(response)\n        elif endpoint == 'quote':\n            return self._normalize_quote(response)\n        # ... more endpoints\n        return response\n\n    def _normalize_profile(self, data: Any) -&gt; Dict[str, Any]:\n        # Handle both single object and array responses\n        if isinstance(data, list):\n            data = data[0] if data else {}\n\n        return {\n            'symbol': data.get('symbol'),\n            'company_name': data.get('companyName'),\n            'sector': data.get('sector'),\n            'industry': data.get('industry'),\n            'market_cap': data.get('mktCap'),\n            # ... more fields\n        }\n</code></pre></p> <p>Effort Breakdown: - Base implementation: 6h - 13 endpoint implementations: 12h (1h each, some simpler) - Normalization logic: 6h (complex schemas) - Unit tests: 8h - Integration tests: 4h - JSON fixtures: 2h (record from real API) - Debugging: 2-4h - Total: 40-44h (realistic)</p>"},{"location":"ACTION_PLAN/#t-009-polygon-provider-4-endpoints","title":"T-009: Polygon Provider (4 endpoints)","text":"Field Value Milestone M2 Effort 12-16 hours Priority P0 Dependencies T-006 <p>Description: Implement Polygon.io provider with 4 endpoints: aggs_daily, trades, options_snapshot, market_snapshot. Simpler than FMP but has date range complexity.</p> <p>Implementation Notes: - Base URL: <code>https://api.polygon.io/v2/</code> - Authentication: <code>apiKey</code> query parameter - Date range handling for aggregates (from/to parameters) - Hash-based cache keys (complex query params) - Rate limit: 5 req/min (free tier), 10 concurrent (QoS) - Options endpoint has nested data structures</p> <p>Acceptance Criteria: - [ ] 4 endpoints implemented: aggs_daily, trades, options_snapshot, market_snapshot - [ ] Authentication via <code>apiKey</code> query param - [ ] Date range handling for aggs_daily - [ ] Hash-based cache key for complex params - [ ] Normalization handles nested structures (options) - [ ] Unit tests: normalization per endpoint - [ ] Integration tests: date range edge cases - [ ] JSON fixtures in <code>tests/fixtures/polygon/</code></p> <p>Files to Create: - <code>src/data_loader/providers/polygon.py</code> - <code>tests/unit/test_polygon_provider.py</code> - <code>tests/integration/test_polygon_integration.py</code> - <code>tests/fixtures/polygon/aggs_daily_SPY.json</code> (and 3 more)</p> <p>Effort Breakdown: - Base implementation: 3h - 4 endpoint implementations: 4h - Date range logic: 2h - Normalization: 3h - Unit tests: 4h - Integration tests: 2h - JSON fixtures: 1h - Debugging: 1-3h - Total: 20-22h (realistic)</p>"},{"location":"ACTION_PLAN/#t-010-fred-provider-32-series-base","title":"T-010: FRED Provider (32 series + base)","text":"Field Value Milestone M2 Effort 16-24 hours Priority P0 Dependencies T-006 <p>Description: Implement FRED (Federal Reserve Economic Data) provider supporting 32 predefined series (CPIAUCSL, UNRATE, DGS10, etc.) plus generic series endpoint.</p> <p>Implementation Notes: - Base URL: <code>https://api.stlouisfed.org/fred/</code> - Authentication: <code>api_key</code> query parameter - Series endpoint: <code>/series/observations</code> - 32 predefined series as constants (see DECISIONS.md Section 14) - Date range handling (start_date, end_date) - Time series normalization (date + value pairs) - Simple endpoint pattern (less variety than FMP)</p> <p>Acceptance Criteria: - [ ] Generic series fetch supports any series_id - [ ] 32 predefined series documented (constants or enum) - [ ] Authentication via <code>api_key</code> query param - [ ] Date range parameters: observation_start, observation_end - [ ] Normalization to consistent time series format: <code>[{date, value}, ...]</code> - [ ] Cache key: <code>fred_cache/{series_id}/{date_range}.json</code> - [ ] Unit tests: sample series (5-10), normalization - [ ] Integration tests: date range edge cases - [ ] JSON fixtures for representative series</p> <p>Files to Create: - <code>src/data_loader/providers/fred.py</code> - <code>tests/unit/test_fred_provider.py</code> - <code>tests/integration/test_fred_integration.py</code> - <code>tests/fixtures/fred/CPIAUCSL.json</code> (and 5-10 more samples)</p> <p>Effort Breakdown: - Base implementation: 4h - Series endpoint: 3h - Date range logic: 2h - Normalization: 3h - 32 series constants: 1h - Unit tests: 6h - Integration tests: 3h - JSON fixtures: 2h - Debugging: 2-4h - Total: 26-28h (realistic)</p>"},{"location":"ACTION_PLAN/#t-011-provider-integration-tests-mocked-http","title":"T-011: Provider Integration Tests (mocked HTTP)","text":"Field Value Milestone M2 Effort 12-16 hours Priority P0 Dependencies T-008, T-009, T-010 <p>Description: Create comprehensive integration tests for all 3 providers using aioresponses to mock HTTP responses. Validates provider behavior with realistic mocked API responses.</p> <p>Implementation Notes: - Use aioresponses library to mock aiohttp requests - Test scenarios: success (200), rate limit (429), server error (5xx), timeout - Verify provider handles errors correctly - Validate normalization with realistic response fixtures - Test retry logic integration (without actual delays) - Mock circuit breaker interactions</p> <p>Acceptance Criteria: - [ ] ~40 integration tests total (~13 per provider + orchestration) - [ ] HTTP 200 success scenarios (all endpoints) - [ ] HTTP 429 rate limit handling - [ ] HTTP 5xx retry logic - [ ] Timeout handling - [ ] Malformed JSON response handling - [ ] All tests use aioresponses (no real API calls) - [ ] Tests run in &lt;60 seconds</p> <p>Files to Create/Extend: - <code>tests/integration/test_fmp_integration.py</code> - <code>tests/integration/test_polygon_integration.py</code> - <code>tests/integration/test_fred_integration.py</code></p> <p>Effort Breakdown: - FMP integration tests: 5h (13 endpoints) - Polygon integration tests: 3h (4 endpoints) - FRED integration tests: 3h (sample series) - Error scenario tests: 3h (429, 5xx, timeout) - Debugging flaky tests: 2-4h - Total: 16-18h (realistic)</p>"},{"location":"ACTION_PLAN/#t-012-qos-semaphore-router","title":"T-012: QoS Semaphore Router","text":"Field Value Milestone M3 Effort 8-12 hours Priority P0 Dependencies T-006 <p>Description: Implement QoS (Quality of Service) router with provider-specific semaphores to enforce concurrency limits: FMP=3, Polygon=10, FRED=1. TIER 1 component (&gt;90% coverage).</p> <p>Implementation Notes: - Use asyncio.Semaphore per provider - Context manager for acquire/release pattern - Timeout handling (raise error if cannot acquire) - Configurable limits (from config) - Log semaphore acquisition/release - Thread-safe (though system is single async process)</p> <p>Acceptance Criteria: - [ ] Separate semaphores for FMP (max=3), Polygon (max=10), FRED (max=1) - [ ] acquire(provider) blocks until semaphore available - [ ] release(provider) releases semaphore - [ ] Context manager support: <code>async with qos_router.acquire(provider):</code> - [ ] Timeout parameter (default 30s) - [ ] Unit tests verify: concurrency limits enforced, timeout handling - [ ] &gt;90% test coverage (TIER 1)</p> <p>Files to Create: - <code>src/data_loader/qos_router.py</code> - <code>tests/unit/test_qos_router.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/qos_router.py\nimport asyncio\nfrom contextlib import asynccontextmanager\nfrom typing import AsyncIterator\n\nclass QoSRouter:\n    \"\"\"Quality of Service router with provider-specific concurrency limits.\"\"\"\n\n    def __init__(self, fmp_limit: int = 3, polygon_limit: int = 10, fred_limit: int = 1):\n        self._semaphores = {\n            'fmp': asyncio.Semaphore(fmp_limit),\n            'polygon': asyncio.Semaphore(polygon_limit),\n            'fred': asyncio.Semaphore(fred_limit),\n        }\n\n    @asynccontextmanager\n    async def acquire(self, provider: str, timeout: float = 30.0) -&gt; AsyncIterator[None]:\n        \"\"\"\n        Acquire semaphore for provider with timeout.\n\n        Usage:\n            async with qos_router.acquire('fmp'):\n                # Make API request\n                pass\n        \"\"\"\n        semaphore = self._semaphores.get(provider)\n        if not semaphore:\n            raise ValueError(f\"Unknown provider: {provider}\")\n\n        try:\n            await asyncio.wait_for(semaphore.acquire(), timeout=timeout)\n            yield\n        except asyncio.TimeoutError:\n            raise QoSTimeoutError(f\"Could not acquire semaphore for {provider} within {timeout}s\")\n        finally:\n            semaphore.release()\n</code></pre></p> <p>Effort Breakdown: - Implementation: 3h - Unit tests: 4h (test concurrent scenarios) - Integration with providers: 2h - Debugging async edge cases: 1-3h - Total: 10-12h (realistic)</p>"},{"location":"ACTION_PLAN/#t-013-circuit-breaker-manager","title":"T-013: Circuit Breaker Manager","text":"Field Value Milestone M3 Effort 12-16 hours Priority P0 Dependencies T-005 <p>Description: Implement circuit breaker pattern with 3 states (CLOSED, OPEN, HALF-OPEN) and per-provider failure tracking. Opens at &gt;20% error rate. TIER 1 component (&gt;90% coverage).</p> <p>Implementation Notes: - Finite state machine: CLOSED \u2192 OPEN \u2192 HALF-OPEN \u2192 CLOSED (or back to OPEN) - Per-provider state (FMP, Polygon, FRED independent) - Sliding window error tracking (last N requests) - Threshold: &gt;20% error rate - Timeout in OPEN state (e.g., 60s before HALF-OPEN) - HALF-OPEN: single test request, success \u2192 CLOSED, failure \u2192 OPEN - Integration with HealthMonitor for state reporting</p> <p>Acceptance Criteria: - [ ] 3 states: CLOSED (normal), OPEN (failing), HALF-OPEN (testing recovery) - [ ] Per-provider state tracking - [ ] Error rate threshold: &gt;20% (configurable) - [ ] Sliding window: last 10 requests (configurable) - [ ] OPEN timeout: 60s (configurable) - [ ] HALF-OPEN recovery: 1 test request - [ ] Raises CircuitOpenError when state=OPEN - [ ] Unit tests verify: state transitions, threshold calculation, timeout, recovery - [ ] Integration tests: full state machine cycle - [ ] &gt;90% test coverage (TIER 1)</p> <p>Files to Create: - <code>src/data_loader/circuit_breaker.py</code> - <code>tests/unit/test_circuit_breaker.py</code> - <code>tests/integration/test_circuit_breaker_integration.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/circuit_breaker.py\nfrom enum import Enum\nfrom collections import deque\nfrom datetime import datetime, timedelta\nfrom typing import Dict\n\nclass CircuitState(Enum):\n    CLOSED = \"CLOSED\"\n    OPEN = \"OPEN\"\n    HALF_OPEN = \"HALF_OPEN\"\n\nclass CircuitBreaker:\n    \"\"\"Per-provider circuit breaker with error rate threshold.\"\"\"\n\n    def __init__(self, threshold: float = 0.2, window_size: int = 10, timeout_seconds: int = 60):\n        self.threshold = threshold\n        self.window_size = window_size\n        self.timeout = timedelta(seconds=timeout_seconds)\n\n        self._states: Dict[str, CircuitState] = {\n            'fmp': CircuitState.CLOSED,\n            'polygon': CircuitState.CLOSED,\n            'fred': CircuitState.CLOSED,\n        }\n        self._windows: Dict[str, deque] = {\n            'fmp': deque(maxlen=window_size),\n            'polygon': deque(maxlen=window_size),\n            'fred': deque(maxlen=window_size),\n        }\n        self._open_times: Dict[str, datetime] = {}\n\n    def record_success(self, provider: str):\n        self._windows[provider].append(True)\n        if self._states[provider] == CircuitState.HALF_OPEN:\n            self._states[provider] = CircuitState.CLOSED\n            self._open_times.pop(provider, None)\n\n    def record_failure(self, provider: str):\n        self._windows[provider].append(False)\n        error_rate = self._calculate_error_rate(provider)\n\n        if error_rate &gt; self.threshold:\n            self._states[provider] = CircuitState.OPEN\n            self._open_times[provider] = datetime.now()\n\n        if self._states[provider] == CircuitState.HALF_OPEN:\n            self._states[provider] = CircuitState.OPEN\n            self._open_times[provider] = datetime.now()\n\n    def check(self, provider: str):\n        \"\"\"Check if circuit allows request. Raises CircuitOpenError if OPEN.\"\"\"\n        state = self._states[provider]\n\n        if state == CircuitState.OPEN:\n            # Check if timeout elapsed \u2192 HALF_OPEN\n            open_time = self._open_times.get(provider)\n            if open_time and datetime.now() - open_time &gt; self.timeout:\n                self._states[provider] = CircuitState.HALF_OPEN\n            else:\n                raise CircuitOpenError(f\"Circuit breaker OPEN for {provider}\")\n\n    def _calculate_error_rate(self, provider: str) -&gt; float:\n        window = self._windows[provider]\n        if not window:\n            return 0.0\n        failures = sum(1 for success in window if not success)\n        return failures / len(window)\n</code></pre></p> <p>Effort Breakdown: - State machine implementation: 4h - Error rate tracking: 2h - Timeout and recovery: 2h - Integration with HealthMonitor: 1h - Unit tests: 5h (complex state transitions) - Integration tests: 2h - Debugging edge cases: 2-4h - Total: 18-20h (realistic)</p>"},{"location":"ACTION_PLAN/#t-014-retry-handler-exponential-backoff-jitter","title":"T-014: Retry Handler (exponential backoff + jitter)","text":"Field Value Milestone M3 Effort 8-12 hours Priority P0 Dependencies T-003 <p>Description: Implement retry logic with exponential backoff and jitter for transient failures (5xx, timeouts). Does NOT retry 4xx errors or 429 rate limits.</p> <p>Implementation Notes: - Exponential backoff: delay = base * (2 ** attempt) - Jitter: randomize delay \u00b120% to prevent thundering herd - Max retries: 3 (configurable) - Retry only: 5xx errors, timeouts (aiohttp.ClientTimeout) - Do NOT retry: 4xx errors, 429 (handled separately) - Log each retry attempt - Total time budget: ~7-8s max (1s + 2s + 4s)</p> <p>Acceptance Criteria: - [ ] Exponential backoff: attempt 1 \u2192 ~1s, attempt 2 \u2192 ~2s, attempt 3 \u2192 ~4s - [ ] Jitter: randomize delay \u00b120% - [ ] Max retries: 3 (configurable) - [ ] Retry only on: 5xx, timeouts - [ ] Do NOT retry: 4xx, 429 - [ ] Log each retry: attempt number, delay, error - [ ] Unit tests verify: backoff calculation, jitter range, retry logic, max retries - [ ] Integration tests: mock 5xx \u2192 success on 3rd retry</p> <p>Files to Create: - <code>src/data_loader/retry.py</code> - <code>tests/unit/test_retry_handler.py</code> - <code>tests/integration/test_retry_integration.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/retry.py\nimport asyncio\nimport random\nfrom typing import Callable, Any\n\nclass RetryHandler:\n    \"\"\"Retry handler with exponential backoff and jitter.\"\"\"\n\n    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, jitter: float = 0.2):\n        self.max_retries = max_retries\n        self.base_delay = base_delay\n        self.jitter = jitter\n\n    async def execute(self, func: Callable, *args, **kwargs) -&gt; Any:\n        \"\"\"Execute function with retry logic.\"\"\"\n        last_error = None\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                return await func(*args, **kwargs)\n            except Exception as e:\n                if not self._should_retry(e, attempt):\n                    raise\n\n                last_error = e\n                if attempt &lt; self.max_retries:\n                    delay = self._calculate_delay(attempt)\n                    logging.warning(f\"Retry {attempt+1}/{self.max_retries} after {delay:.2f}s: {e}\")\n                    await asyncio.sleep(delay)\n\n        raise last_error\n\n    def _should_retry(self, error: Exception, attempt: int) -&gt; bool:\n        \"\"\"Determine if error is retryable.\"\"\"\n        if attempt &gt;= self.max_retries:\n            return False\n\n        # Retry 5xx and timeouts\n        if isinstance(error, aiohttp.ClientError):\n            if hasattr(error, 'status'):\n                return 500 &lt;= error.status &lt; 600\n            return True  # Timeout\n\n        return False\n\n    def _calculate_delay(self, attempt: int) -&gt; float:\n        \"\"\"Calculate delay with exponential backoff and jitter.\"\"\"\n        delay = self.base_delay * (2 ** attempt)\n        jitter_amount = delay * self.jitter\n        return delay + random.uniform(-jitter_amount, jitter_amount)\n</code></pre></p> <p>Effort Breakdown: - Implementation: 3h - Backoff + jitter logic: 2h - Unit tests: 4h - Integration tests: 2h - Debugging async timing: 1-3h - Total: 12-14h (realistic)</p>"},{"location":"ACTION_PLAN/#t-015-rate-limit-handling-http-429","title":"T-015: Rate Limit Handling (HTTP 429)","text":"Field Value Milestone M3 Effort 6-8 hours Priority P0 Dependencies T-014 <p>Description: Handle HTTP 429 rate limit responses by parsing Retry-After header and sleeping before retry. Separate from exponential backoff logic.</p> <p>Implementation Notes: - Parse Retry-After header (seconds or HTTP date) - Sleep for specified duration - Log rate limit hit with cooldown time - Update HealthMonitor rate_limit_pct - Do NOT use exponential backoff for 429 (respect server instruction) - Single retry after cooldown (not multiple)</p> <p>Acceptance Criteria: - [ ] Detect HTTP 429 response - [ ] Parse Retry-After header (seconds format) - [ ] Sleep for Retry-After duration - [ ] Log rate limit event - [ ] Single retry after cooldown - [ ] If Retry-After missing, use default cooldown (60s) - [ ] Unit tests verify: header parsing, cooldown logic - [ ] Integration tests: mock 429 \u2192 success after cooldown</p> <p>Files to Create: - <code>src/data_loader/rate_limit.py</code> (or extend retry.py) - <code>tests/unit/test_rate_limit.py</code> - <code>tests/integration/test_rate_limit_integration.py</code></p> <p>Effort Breakdown: - Implementation: 2h - Retry-After parsing: 1h - Integration with retry handler: 1h - Unit tests: 2h - Integration tests: 1h - Debugging: 1-2h - Total: 8-9h (realistic)</p>"},{"location":"ACTION_PLAN/#t-016-dataloader-unified-interface","title":"T-016: DataLoader Unified Interface","text":"Field Value Milestone M3 Effort 10-14 hours Priority P0 Dependencies T-012, T-013, T-014 <p>Description: Implement main DataLoader class that orchestrates all components: providers, QoS router, circuit breaker, retry handler, cache. Provides unified interface: <code>get_fmp_data()</code>, <code>get_polygon_data()</code>, <code>get_fred_data()</code>.</p> <p>Implementation Notes: - Singleton or dependency-injected instance - Orchestration flow: check cache \u2192 check circuit \u2192 acquire QoS \u2192 call provider \u2192 handle retries \u2192 update cache - Provider routing: map provider name to provider instance - Health report aggregation - Operating mode enforcement (LIVE vs READ-ONLY) - Session management (long-lived aiohttp.ClientSession)</p> <p>Acceptance Criteria: - [ ] Methods: <code>get_fmp_data()</code>, <code>get_polygon_data()</code>, <code>get_fred_data()</code>, <code>get_api_health_report()</code>, <code>set_operating_mode()</code> - [ ] Orchestration: cache \u2192 circuit \u2192 QoS \u2192 provider \u2192 retry \u2192 cache - [ ] Cache hit: return immediately (no API call) - [ ] Circuit OPEN: raise error (no API call) - [ ] QoS: enforce concurrency limits - [ ] Retry: handle 5xx and timeouts - [ ] Rate limit: handle 429 - [ ] Health tracking: update counters per request - [ ] Operating modes: LIVE (normal), READ-ONLY (cache only) - [ ] Unit tests: orchestration flow, mode enforcement - [ ] Integration tests: full request cycle</p> <p>Files to Create: - <code>src/data_loader/loader.py</code> - <code>tests/unit/test_loader.py</code> - <code>tests/integration/test_loader_integration.py</code></p> <p>Implementation Example: <pre><code># src/data_loader/loader.py\nfrom typing import Dict, Any, Optional\nfrom .providers.fmp import FMPProvider\nfrom .providers.polygon import PolygonProvider\nfrom .providers.fred import FREDProvider\n\nclass DataLoader:\n    \"\"\"Unified interface for financial data retrieval.\"\"\"\n\n    def __init__(self, config, cache, health_monitor, circuit_breaker, qos_router, retry_handler):\n        self.config = config\n        self.cache = cache\n        self.health = health_monitor\n        self.circuit = circuit_breaker\n        self.qos = qos_router\n        self.retry = retry_handler\n\n        # Initialize providers\n        self.providers = {\n            'fmp': FMPProvider(config.fmp_key, http_client),\n            'polygon': PolygonProvider(config.polygon_key, http_client),\n            'fred': FREDProvider(config.fred_key, http_client),\n        }\n\n        self.mode = 'LIVE'\n\n    async def get_fmp_data(self, endpoint: str, **params) -&gt; Dict[str, Any]:\n        \"\"\"Fetch data from FMP provider.\"\"\"\n        return await self._fetch('fmp', endpoint, params)\n\n    async def _fetch(self, provider: str, endpoint: str, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Orchestrate data fetch with all resilience components.\"\"\"\n        cache_key = self.providers[provider].generate_cache_key(endpoint, params)\n\n        # 1. Check cache\n        cached = self.cache.get_cached(cache_key, provider)\n        if cached:\n            return cached\n\n        # 2. Check mode\n        if self.mode == 'READ_ONLY':\n            raise ReadOnlyError(f\"READ-ONLY mode: no API calls allowed\")\n\n        # 3. Check circuit breaker\n        self.circuit.check(provider)\n\n        # 4. Acquire QoS semaphore\n        async with self.qos.acquire(provider):\n            try:\n                # 5. Fetch with retry\n                self.health.record_request(provider)\n                data = await self.retry.execute(\n                    self.providers[provider].fetch,\n                    endpoint,\n                    **params\n                )\n\n                # 6. Update cache\n                self.cache.set_cache(cache_key, provider, data)\n\n                # 7. Record success\n                self.circuit.record_success(provider)\n                return data\n\n            except Exception as e:\n                self.health.record_error(provider, str(e))\n                self.circuit.record_failure(provider)\n                raise\n\n    def get_api_health_report(self) -&gt; Dict[str, Any]:\n        return self.health.get_health_report()\n\n    def set_operating_mode(self, mode: str):\n        if mode not in ('LIVE', 'READ_ONLY'):\n            raise ValueError(f\"Invalid mode: {mode}\")\n        self.mode = mode\n</code></pre></p> <p>Effort Breakdown: - Core orchestration: 4h - Provider integration: 2h - Mode enforcement: 1h - Session management: 1h - Unit tests: 4h - Integration tests: 3h - Debugging orchestration: 2-4h - Total: 17-19h (realistic)</p>"},{"location":"ACTION_PLAN/#t-017-operating-modes-liveread-only","title":"T-017: Operating Modes (LIVE/READ-ONLY)","text":"Field Value Milestone M3 Effort 4-6 hours Priority P0 Dependencies T-016 <p>Description: Implement operating mode switching: LIVE (normal API calls) and READ-ONLY (cache-only, no API calls). Enables offline analysis and testing.</p> <p>Implementation Notes: - Mode stored in DataLoader state - LIVE: normal operation (default) - READ-ONLY: cache hit returns data, cache miss raises error - set_operating_mode() method validates mode - Log mode changes - Unit tests verify mode enforcement</p> <p>Acceptance Criteria: - [ ] Modes: LIVE, READ_ONLY - [ ] LIVE mode: normal behavior (cache \u2192 API \u2192 cache) - [ ] READ_ONLY mode: cache hit \u2192 data, cache miss \u2192 ReadOnlyError - [ ] set_operating_mode() validates input - [ ] Log mode changes - [ ] Unit tests: mode switching, enforcement - [ ] E2E test: READ-ONLY prevents API calls</p> <p>Files to Create/Extend: - <code>src/data_loader/loader.py</code> (extend) - <code>tests/unit/test_loader.py</code> (extend) - <code>tests/e2e/test_readonly_mode.py</code></p> <p>Effort Breakdown: - Implementation: 1h - Error handling: 1h - Unit tests: 2h - E2E tests: 1h - Debugging: 1-2h - Total: 6-7h (realistic)</p>"},{"location":"ACTION_PLAN/#t-018-resilience-integration-tests","title":"T-018: Resilience Integration Tests","text":"Field Value Milestone M3 Effort 4-8 hours Priority P0 Dependencies T-012, T-013, T-014 <p>Description: Integration tests validating resilience components working together: circuit breaker with retry, QoS with circuit breaker, full orchestration cycle.</p> <p>Implementation Notes: - Mock HTTP responses to simulate failures - Verify circuit breaker opens after threshold - Verify QoS limits concurrent requests - Verify retry logic with backoff - Test full failure \u2192 recovery cycle - All tests must run in &lt;60s</p> <p>Acceptance Criteria: - [ ] Circuit breaker integration: errors \u2192 OPEN \u2192 HALF-OPEN \u2192 CLOSED - [ ] QoS integration: concurrent limit enforcement - [ ] Retry integration: 5xx \u2192 backoff \u2192 success - [ ] Full cycle: cache miss \u2192 circuit check \u2192 QoS \u2192 retry \u2192 cache write - [ ] All tests use mocked HTTP (aioresponses) - [ ] Tests run in &lt;60s</p> <p>Files to Create: - <code>tests/integration/test_resilience_integration.py</code></p> <p>Effort Breakdown: - Circuit breaker integration: 2h - QoS integration: 1h - Retry integration: 1h - Full cycle test: 2h - Debugging timing issues: 2-4h - Total: 8-10h (realistic)</p>"},{"location":"ACTION_PLAN/#t-019-unit-test-suite-completion-tier-1-90","title":"T-019: Unit Test Suite Completion (TIER 1 &gt;90%)","text":"Field Value Milestone M4 Effort 12-16 hours Priority P0 Dependencies All previous tasks <p>Description: Complete unit test suite to achieve &gt;90% coverage for TIER 1 components (Circuit Breaker, QoS Router, API Key Sanitization, Atomic Cache Writes) and &gt;80% overall coverage.</p> <p>Implementation Notes: - Focus on TIER 1 components first - Use pytest-cov to identify gaps - Write tests for edge cases and error paths - Mock external dependencies - Fast execution (&lt;20s for all unit tests)</p> <p>Acceptance Criteria: - [ ] &gt;90% coverage: Circuit Breaker, QoS Router, Logging Sanitization, Cache (atomic writes) - [ ] &gt;80% coverage: Providers, DataLoader, Retry Handler, Health Monitor - [ ] ~150 unit tests total - [ ] All tests passing - [ ] Execution time &lt;20s - [ ] Coverage report generated (HTML + terminal)</p> <p>Files to Create/Extend: - Extend all existing <code>tests/unit/test_*.py</code> files - Fill coverage gaps identified by pytest-cov</p> <p>Effort Breakdown: - Coverage analysis: 2h - Gap filling (TIER 1): 6h - Gap filling (TIER 2): 4h - Edge case tests: 3h - Debugging flaky tests: 2-4h - Total: 17-19h (realistic)</p>"},{"location":"ACTION_PLAN/#t-020-e2e-test-scenarios-10-critical-paths","title":"T-020: E2E Test Scenarios (10 critical paths)","text":"Field Value Milestone M4 Effort 8-12 hours Priority P0 Dependencies T-016 <p>Description: Implement 10 end-to-end test scenarios covering critical user workflows: cache miss \u2192 API \u2192 cache hit, circuit breaker full cycle, multi-provider parallel fetch, READ-ONLY mode.</p> <p>Implementation Notes: - Mock all HTTP responses (aioresponses) - Test full DataLoader orchestration - Verify state changes (cache, circuit breaker, health) - Each test validates complete workflow - Tests should run in &lt;30s total</p> <p>Acceptance Criteria: - [ ] 10 E2E tests covering critical scenarios (see TEST_STRATEGY.md Section 8.5) - [ ] TC-401: Cache miss \u2192 API \u2192 Cache hit - [ ] TC-402: READ-ONLY mode enforcement - [ ] TC-403: Parallel multi-provider fetch - [ ] TC-404: Health report aggregation - [ ] Circuit breaker full cycle: OPEN \u2192 HALF-OPEN \u2192 CLOSED - [ ] QoS limits concurrent requests - [ ] All tests mocked (no real API calls) - [ ] Execution time &lt;30s</p> <p>Files to Create: - <code>tests/e2e/test_happy_path.py</code> - <code>tests/e2e/test_readonly_mode.py</code> - <code>tests/e2e/test_parallel_fetch.py</code> - <code>tests/e2e/test_resilience.py</code></p> <p>Effort Breakdown: - Happy path tests: 3h - Circuit breaker cycle: 2h - Parallel fetch: 2h - READ-ONLY mode: 1h - Health report: 1h - Debugging orchestration: 3-5h - Total: 12-14h (realistic)</p>"},{"location":"ACTION_PLAN/#t-021-security-tests-api-key-sanitization","title":"T-021: Security Tests (API key sanitization)","text":"Field Value Milestone M4 Effort 4-6 hours Priority P0 Dependencies T-007 <p>Description: Implement security tests validating API key sanitization in all log output, error messages, and cache files. TIER 1 requirement (TC-101 through TC-106).</p> <p>Implementation Notes: - Test all log levels (DEBUG, INFO, WARNING, ERROR) - Verify keys redacted in: URLs, headers, error messages - Verify keys NOT in cache files - Verify .env in .gitignore - Use regex patterns to detect key leakage</p> <p>Acceptance Criteria: - [ ] TC-101: Keys not in error messages - [ ] TC-102: Keys not in success logs - [ ] TC-103: Keys not in cache files - [ ] TC-104: .env in .gitignore - [ ] TC-105: HTTPS enforcement - [ ] TC-106: TLS validation enabled - [ ] All security tests passing - [ ] 100% coverage of security test cases</p> <p>Files to Create: - <code>tests/unit/test_security.py</code> - <code>tests/integration/test_security_integration.py</code></p> <p>Effort Breakdown: - Log sanitization tests: 2h - Cache file tests: 1h - HTTPS/TLS tests: 1h - .gitignore verification: 0.5h - Debugging regex patterns: 1-2h - Total: 5.5-6.5h (realistic)</p>"},{"location":"ACTION_PLAN/#t-022-coverage-analysis-and-gap-filling","title":"T-022: Coverage Analysis and Gap Filling","text":"Field Value Milestone M4 Effort 4-6 hours Priority P0 Dependencies T-019, T-020 <p>Description: Analyze coverage reports, identify gaps, and write tests to fill gaps until &gt;80% overall and &gt;90% TIER 1 coverage achieved.</p> <p>Implementation Notes: - Run pytest-cov with HTML report - Review uncovered lines - Prioritize TIER 1 gaps - Write targeted tests for specific branches - Exclude legitimate uncoverable code (defensive errors, debug code)</p> <p>Acceptance Criteria: - [ ] Overall coverage &gt;80% - [ ] TIER 1 coverage &gt;90% - [ ] Coverage report generated (HTML) - [ ] Coverage gaps documented (excluded lines marked) - [ ] All new tests passing</p> <p>Files to Create/Extend: - Extend existing test files based on gaps</p> <p>Effort Breakdown: - Initial coverage analysis: 1h - Gap identification: 1h - Writing gap-filling tests: 3h - Re-analysis and verification: 1h - Documentation: 0.5-1h - Total: 6.5-7h (realistic)</p>"},{"location":"ACTION_PLAN/#t-023-cicd-pipeline-setup-github-actions","title":"T-023: CI/CD Pipeline Setup (GitHub Actions)","text":"Field Value Milestone M4 Effort 4-6 hours Priority P1 Dependencies T-022 <p>Description: Set up GitHub Actions workflow for automated testing: lint \u2192 type check \u2192 unit \u2192 integration \u2192 E2E \u2192 coverage \u2192 secret scan.</p> <p>Implementation Notes: - Run on: push to main, pull requests - Jobs: lint, type-check, test - Test job runs all test types - Upload coverage report to codecov or similar - Secret scanning with detect-secrets - Cache pip dependencies for speed - Target execution time: &lt;2 minutes</p> <p>Acceptance Criteria: - [ ] GitHub Actions workflow file created - [ ] Jobs: lint (ruff), type-check (mypy), test (pytest) - [ ] Test job runs: unit \u2192 integration \u2192 E2E - [ ] Coverage report uploaded - [ ] Secret scanning runs - [ ] Badge in README.md - [ ] Execution time &lt;2 minutes</p> <p>Files to Create: - <code>.github/workflows/ci.yml</code> - Update <code>README.md</code> with badge</p> <p>Effort Breakdown: - Workflow setup: 2h - Coverage upload: 1h - Secret scanning: 1h - Testing and debugging: 1-2h - Documentation: 0.5-1h - Total: 5.5-7h (realistic)</p>"},{"location":"ACTION_PLAN/#t-024-documentation-polish-readme-examples","title":"T-024: Documentation Polish (README, examples)","text":"Field Value Milestone M4 Effort 6-8 hours Priority P1 Dependencies All <p>Description: Polish documentation: comprehensive README.md with setup instructions, usage examples, architecture overview, and troubleshooting. Create example scripts demonstrating usage.</p> <p>Implementation Notes: - README sections: Overview, Features, Installation, Quickstart, Examples, Configuration, Architecture, Testing, Contributing - Example scripts: basic usage, multi-provider fetch, error handling, READ-ONLY mode - API reference (docstrings already written) - Troubleshooting common issues - Link to architecture docs</p> <p>Acceptance Criteria: - [ ] README.md complete with all sections - [ ] Installation instructions: &lt;5 minute setup - [ ] Quickstart example: fetch data from all 3 providers - [ ] Example scripts in <code>examples/</code> directory - [ ] Configuration documentation (.env variables) - [ ] Architecture diagram (ASCII or image) - [ ] Troubleshooting section - [ ] Links to detailed docs in <code>docs/</code></p> <p>Files to Create: - Update <code>README.md</code> - <code>examples/quickstart.py</code> - <code>examples/multi_provider.py</code> - <code>examples/error_handling.py</code> - <code>examples/readonly_mode.py</code></p> <p>Effort Breakdown: - README writing: 3h - Example scripts: 2h - Architecture diagram: 1h - Troubleshooting: 1h - Review and polish: 1-2h - Total: 8-9h (realistic)</p>"},{"location":"ACTION_PLAN/#t-025-pre-commit-hooks-secret-scanning","title":"T-025: Pre-commit Hooks (secret scanning)","text":"Field Value Milestone M4 Effort 2-3 hours Priority P1 Dependencies T-007 <p>Description: Set up pre-commit hooks to automatically scan for API keys and secrets before commits. Prevents accidental key leakage.</p> <p>Implementation Notes: - Use detect-secrets or similar tool - Configure to scan: .py, .md, .yml, .json - Exclude: tests/fixtures/ (contains mock data) - Baseline file for known false positives - Instructions in README for setup</p> <p>Acceptance Criteria: - [ ] Pre-commit configuration file created - [ ] detect-secrets hook configured - [ ] Baseline file for false positives - [ ] Hook blocks commits with secrets - [ ] README.md documents setup - [ ] Tested with actual secret in file</p> <p>Files to Create: - <code>.pre-commit-config.yaml</code> - <code>.secrets.baseline</code> - Update <code>README.md</code> with setup instructions</p> <p>Effort Breakdown: - Pre-commit setup: 1h - Baseline configuration: 0.5h - Testing: 0.5h - Documentation: 0.5h - Debugging: 0.5-1h - Total: 3-3.5h (realistic)</p>"},{"location":"ACTION_PLAN/#t-026-manual-smoke-test-with-real-apis","title":"T-026: Manual Smoke Test with Real APIs","text":"Field Value Milestone M4 Effort 2-3 hours Priority P1 Dependencies All <p>Description: Manual validation with real API calls to all 3 providers. Verify: authentication works, data fetched, cache written, circuit breaker recovers, health report accurate.</p> <p>Implementation Notes: - Use real API keys (.env file) - Test representative endpoints from each provider - Verify cache files created - Trigger circuit breaker (intentional errors) - Verify recovery cycle - Document any discrepancies from mocked tests</p> <p>Acceptance Criteria: - [ ] FMP: fetch profile, quote, historical_price (3 endpoints) - [ ] Polygon: fetch aggs_daily, market_snapshot (2 endpoints) - [ ] FRED: fetch CPIAUCSL, UNRATE, DGS10 (3 series) - [ ] Cache files created in correct paths - [ ] Circuit breaker opens after failures - [ ] Circuit breaker recovers - [ ] Health report shows accurate stats - [ ] No errors in logs (keys sanitized) - [ ] Manual test report documented</p> <p>Files to Create: - <code>scripts/manual_smoke_test.py</code> - <code>docs/MANUAL_TEST_REPORT.md</code></p> <p>Effort Breakdown: - Script creation: 1h - Test execution: 1h - Issue investigation: 0.5-1h - Report writing: 0.5h - Total: 3-3.5h (realistic)</p>"},{"location":"ACTION_PLAN/#4-technical-approach","title":"4. Technical Approach","text":"Component Approach Rationale Config Management python-dotenv + environment variables Standard practice; keeps secrets out of code; supports CI override HTTP Client aiohttp.ClientSession (long-lived) Production-ready async client; connection pooling; timeout control Cache Filesystem JSON with atomic writes (temp+rename) Simple, inspectable, no DB overhead; atomic writes prevent corruption QoS Router asyncio.Semaphore per provider Efficient async concurrency control; prevents rate limit violations Circuit Breaker Finite state machine with sliding window Industry-standard pattern; per-provider isolation; error rate threshold Retry Handler Exponential backoff with jitter Prevents thundering herd; respects server capacity; fast recovery Providers Plugin architecture (BaseDataProvider) Extensibility; separation of concerns; consistent interface Testing pytest + pytest-asyncio + aioresponses Industry standard; excellent async support; reliable HTTP mocking Logging Python logging with rotating file handler Standard library; API key sanitization via filter; no external deps CI/CD GitHub Actions Free for public repos; familiar workflow; good Python support"},{"location":"ACTION_PLAN/#5-file-structure-target","title":"5. File Structure (Target)","text":"<pre><code>nexus_core/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 ci.yml\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 .secrets.baseline\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 requirements-dev.txt\n\u251c\u2500\u2500 pyproject.toml (optional - for ruff/mypy config)\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 data_loader/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 loader.py              # DataLoader unified interface\n\u2502       \u251c\u2500\u2500 config.py              # Config Manager\n\u2502       \u251c\u2500\u2500 qos_router.py          # QoS Semaphore Router\n\u2502       \u251c\u2500\u2500 circuit_breaker.py     # Circuit Breaker Manager\n\u2502       \u251c\u2500\u2500 retry.py               # Retry &amp; Backoff Handler\n\u2502       \u251c\u2500\u2500 rate_limit.py          # Rate Limit Handler\n\u2502       \u251c\u2500\u2500 http_client.py         # HTTP Client Layer\n\u2502       \u251c\u2500\u2500 cache.py               # Cache Manager\n\u2502       \u251c\u2500\u2500 health.py              # Health Monitor\n\u2502       \u251c\u2500\u2500 logging_config.py      # Logging with sanitization\n\u2502       \u2514\u2500\u2500 providers/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 base.py            # BaseDataProvider\n\u2502           \u251c\u2500\u2500 fmp.py             # FMP Provider\n\u2502           \u251c\u2500\u2500 polygon.py         # Polygon Provider\n\u2502           \u2514\u2500\u2500 fred.py            # FRED Provider\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py                # Shared fixtures\n\u2502   \u251c\u2500\u2500 fixtures/                  # Mock API responses\n\u2502   \u2502   \u251c\u2500\u2500 fmp/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 profile_AAPL.json\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 quote_MSFT.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ... (13 endpoints)\n\u2502   \u2502   \u251c\u2500\u2500 polygon/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 aggs_daily_SPY.json\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ... (4 endpoints)\n\u2502   \u2502   \u2514\u2500\u2500 fred/\n\u2502   \u2502       \u251c\u2500\u2500 CPIAUCSL.json\n\u2502   \u2502       \u251c\u2500\u2500 UNRATE.json\n\u2502   \u2502       \u2514\u2500\u2500 ... (sample series)\n\u2502   \u251c\u2500\u2500 unit/                      # Unit tests (~150)\n\u2502   \u2502   \u251c\u2500\u2500 test_config_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 test_http_client.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cache_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 test_health_monitor.py\n\u2502   \u2502   \u251c\u2500\u2500 test_base_provider.py\n\u2502   \u2502   \u251c\u2500\u2500 test_fmp_provider.py\n\u2502   \u2502   \u251c\u2500\u2500 test_polygon_provider.py\n\u2502   \u2502   \u251c\u2500\u2500 test_fred_provider.py\n\u2502   \u2502   \u251c\u2500\u2500 test_qos_router.py\n\u2502   \u2502   \u251c\u2500\u2500 test_circuit_breaker.py\n\u2502   \u2502   \u251c\u2500\u2500 test_retry_handler.py\n\u2502   \u2502   \u251c\u2500\u2500 test_rate_limit.py\n\u2502   \u2502   \u251c\u2500\u2500 test_loader.py\n\u2502   \u2502   \u251c\u2500\u2500 test_logging_sanitization.py\n\u2502   \u2502   \u2514\u2500\u2500 test_security.py\n\u2502   \u251c\u2500\u2500 integration/               # Integration tests (~40)\n\u2502   \u2502   \u251c\u2500\u2500 test_fmp_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_polygon_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_fred_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cache_filesystem.py\n\u2502   \u2502   \u251c\u2500\u2500 test_circuit_breaker_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_resilience_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_loader_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_logging_integration.py\n\u2502   \u2502   \u2514\u2500\u2500 test_security_integration.py\n\u2502   \u2514\u2500\u2500 e2e/                       # E2E tests (~10)\n\u2502       \u251c\u2500\u2500 test_happy_path.py\n\u2502       \u251c\u2500\u2500 test_readonly_mode.py\n\u2502       \u251c\u2500\u2500 test_parallel_fetch.py\n\u2502       \u2514\u2500\u2500 test_resilience.py\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 quickstart.py\n\u2502   \u251c\u2500\u2500 multi_provider.py\n\u2502   \u251c\u2500\u2500 error_handling.py\n\u2502   \u2514\u2500\u2500 readonly_mode.py\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 manual_smoke_test.py\n\u2502   \u2514\u2500\u2500 record_fixtures.py (for updating test fixtures)\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 REQUIREMENTS.md\n\u2502   \u251c\u2500\u2500 ARCHITECTURE.md\n\u2502   \u251c\u2500\u2500 SECURITY.md\n\u2502   \u251c\u2500\u2500 TEST_STRATEGY.md\n\u2502   \u251c\u2500\u2500 DECISIONS.md\n\u2502   \u251c\u2500\u2500 ACTION_PLAN.md (this document)\n\u2502   \u2514\u2500\u2500 MANUAL_TEST_REPORT.md\n\u251c\u2500\u2500 data/                          # Cache storage (excluded from git)\n\u2502   \u251c\u2500\u2500 fmp_cache/\n\u2502   \u251c\u2500\u2500 polygon_cache/\n\u2502   \u2514\u2500\u2500 fred_cache/\n\u2514\u2500\u2500 logs/                          # Log files (excluded from git)\n    \u2514\u2500\u2500 nexus_core.log\n</code></pre>"},{"location":"ACTION_PLAN/#6-dependencies-to-install","title":"6. Dependencies to Install","text":"Package Version Purpose Type aiohttp &gt;=3.8.0 Async HTTP client Runtime python-dotenv &gt;=0.19.0 .env file loading Runtime pytest &gt;=7.0.0 Testing framework Dev pytest-cov &gt;=3.0.0 Coverage reporting Dev pytest-asyncio &gt;=0.21.0 Async test support Dev aioresponses &gt;=0.7.0 HTTP mocking Dev mypy &gt;=1.0.0 Type checking Dev ruff latest Linting Dev detect-secrets &gt;=1.4.0 Secret scanning Dev <p>Installation: <pre><code># Runtime dependencies\npip install aiohttp&gt;=3.8.0 python-dotenv&gt;=0.19.0\n\n# Development dependencies\npip install pytest&gt;=7.0.0 pytest-cov&gt;=3.0.0 pytest-asyncio&gt;=0.21.0 \\\n            aioresponses&gt;=0.7.0 mypy&gt;=1.0.0 ruff detect-secrets&gt;=1.4.0\n</code></pre></p>"},{"location":"ACTION_PLAN/#7-risk-mitigation","title":"7. Risk Mitigation","text":"Risk Probability Impact Mitigation Buffer Effort underestimation HIGH HIGH Applied 2-3x multipliers; built-in buffer per task +20% contingency API schema changes MEDIUM MEDIUM Schema validation with warnings; version fixtures; graceful degradation +4h for fixture updates Async timing bugs MEDIUM MEDIUM Comprehensive unit tests; integration tests with mocks; manual smoke test +8h debugging budget Coverage gaps MEDIUM LOW Dedicated gap-filling task (T-022); prioritize TIER 1 first +6h for stubborn gaps Testing complexity MEDIUM MEDIUM Mock all HTTP; use fixtures; test in isolation first +12h for flaky tests Scope creep LOW HIGH Strict adherence to DECISIONS.md; defer all \"nice to have\" features Milestone review gates Burnout MEDIUM CRITICAL Sustainable pace (8-10h/week); no hard deadlines; flexible schedule Built into timeline <p>Contingency Budget: +50 hours (20% of total) for unknowns, debugging, and iteration.</p>"},{"location":"ACTION_PLAN/#8-quality-checkpoints","title":"8. Quality Checkpoints","text":""},{"location":"ACTION_PLAN/#per-task","title":"Per Task","text":"<ul> <li>[ ] Code complete and working</li> <li>[ ] Unit tests written and passing</li> <li>[ ] Lint clean (ruff: 0 errors)</li> <li>[ ] Type check passing (mypy: 0 errors)</li> <li>[ ] Task acceptance criteria met</li> </ul>"},{"location":"ACTION_PLAN/#per-milestone","title":"Per Milestone","text":"<ul> <li>[ ] All milestone tasks complete</li> <li>[ ] Integration tests passing</li> <li>[ ] Milestone success criteria met</li> <li>[ ] Self-review complete</li> <li>[ ] Documentation updated</li> </ul>"},{"location":"ACTION_PLAN/#final-m4-complete","title":"Final (M4 Complete)","text":"<ul> <li>[ ] Overall coverage &gt;80%</li> <li>[ ] TIER 1 coverage &gt;90%</li> <li>[ ] All ~200 tests passing</li> <li>[ ] CI pipeline green</li> <li>[ ] All critical test cases (TC-001 to TC-106) passing</li> <li>[ ] No API keys in logs/cache/git</li> <li>[ ] Manual smoke test successful</li> <li>[ ] README.md complete</li> <li>[ ] Pre-commit hooks working</li> </ul>"},{"location":"ACTION_PLAN/#9-commands-for-claude-code","title":"9. Commands for Claude Code","text":""},{"location":"ACTION_PLAN/#project-setup-t-001","title":"Project Setup (T-001)","text":"<pre><code># Create directory structure\nmkdir -p src/data_loader/providers\nmkdir -p tests/{unit,integration,e2e,fixtures/{fmp,polygon,fred}}\nmkdir -p docs examples scripts data logs\n\n# Initialize git\ngit init\n\n# Create virtual environment\npython3.9 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install aiohttp&gt;=3.8.0 python-dotenv&gt;=0.19.0\npip install pytest&gt;=7.0.0 pytest-cov&gt;=3.0.0 pytest-asyncio&gt;=0.21.0 \\\n            aioresponses&gt;=0.7.0 mypy&gt;=1.0.0 ruff detect-secrets&gt;=1.4.0\n\n# Create requirements files\npip freeze &gt; requirements.txt\n\n# Verify installation\npython -c \"import aiohttp; import dotenv; print('OK')\"\npytest --version\n</code></pre>"},{"location":"ACTION_PLAN/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test type\npytest -m unit          # Unit tests only\npytest -m integration   # Integration tests only\npytest -m e2e          # E2E tests only\n\n# Run with coverage\npytest --cov=src --cov-report=html --cov-report=term-missing\n\n# Run fast tests only\npytest -m \"not slow\"\n\n# Run TIER 1 tests\npytest -m tier1\n\n# Run specific file\npytest tests/unit/test_circuit_breaker.py -v\n</code></pre>"},{"location":"ACTION_PLAN/#lint-and-type-check","title":"Lint and Type Check","text":"<pre><code># Lint\nruff check src/ tests/\n\n# Type check\nmypy src/\n\n# Fix auto-fixable lint issues\nruff check --fix src/ tests/\n</code></pre>"},{"location":"ACTION_PLAN/#coverage-analysis","title":"Coverage Analysis","text":"<pre><code># Generate HTML coverage report\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n\n# Check coverage threshold\npytest --cov=src --cov-fail-under=80\n\n# Coverage for specific component (TIER 1)\npytest --cov=src/data_loader/circuit_breaker.py --cov-report=term\n</code></pre>"},{"location":"ACTION_PLAN/#pre-commit-setup","title":"Pre-commit Setup","text":"<pre><code># Install pre-commit hooks\npre-commit install\n\n# Run hooks manually\npre-commit run --all-files\n\n# Update hooks\npre-commit autoupdate\n</code></pre>"},{"location":"ACTION_PLAN/#10-definition-of-done","title":"10. Definition of Done","text":"<p>A task is DONE when:</p> <ul> <li>[ ] Code complete and working</li> <li>[ ] All task acceptance criteria met</li> <li>[ ] Unit tests written and passing (per tier requirements)</li> <li>[ ] Lint passing (ruff: 0 errors)</li> <li>[ ] Type check passing (mypy: 0 errors)</li> <li>[ ] Integration tests passing (if applicable)</li> <li>[ ] Documentation updated (docstrings, README)</li> <li>[ ] Self-review complete (code quality check)</li> <li>[ ] No known bugs or regressions</li> </ul> <p>A milestone is DONE when:</p> <ul> <li>[ ] All milestone tasks DONE</li> <li>[ ] Milestone success criteria met</li> <li>[ ] Integration tests passing</li> <li>[ ] Coverage targets met (if M4)</li> <li>[ ] Documentation complete</li> </ul> <p>The project is DONE when:</p> <ul> <li>[ ] All 4 milestones complete</li> <li>[ ] Overall coverage &gt;80%, TIER 1 &gt;90%</li> <li>[ ] All ~200 tests passing (&lt;2 min execution)</li> <li>[ ] CI pipeline green</li> <li>[ ] All critical test cases passing (TC-001 to TC-106)</li> <li>[ ] Security tests passing (no API keys leaked)</li> <li>[ ] Manual smoke test successful (real APIs)</li> <li>[ ] README.md complete (&lt;5 min setup instructions)</li> <li>[ ] Pre-commit hooks working</li> <li>[ ] All 14 MUST requirements implemented</li> <li>[ ] No P0 bugs or issues</li> </ul>"},{"location":"ACTION_PLAN/#approval","title":"Approval","text":"Role Name Date Status Author Lead Agent 2026-01-31 Generated Owner Solo Developer Pending"},{"location":"ACTION_PLAN/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 Lead Agent Initial action plan with realistic effort estimates <p>Reality Check Applied:</p> <p>This plan reflects realistic effort estimates with 2-3x multipliers on initial estimates. Solo developer working 8-10 hours/week can complete in 5-7 weeks at sustainable pace. NO hard deadlines. Quality over speed.</p> <p>Key realism factors: - \"Add tests\" tasks are 8-16h (not \"2h\") - Complex components (Circuit Breaker, FMP Provider) get realistic time budgets - Debugging and iteration time explicitly included - 20% contingency buffer for unknowns - Focus on TIER 1 components first (&gt;90% coverage) - Accept TIER 3 at 60% (proportionate to risk)</p> <p>This is a research tool, not a production system. Build it right, not fast.</p>"},{"location":"ACTION_PLAN/#11-next-steps-post-completion","title":"11. Next Steps (Post-Completion)","text":"<p>A projekt alapvet\u0151 funkcionalit\u00e1sa k\u00e9sz (M1-M4 \u2705). A k\u00f6vetkez\u0151 opcion\u00e1lis fejleszt\u00e9sek \u00e9rhet\u0151k el:</p>"},{"location":"ACTION_PLAN/#111-dokumentacio-kiegeszitese","title":"11.1 Dokument\u00e1ci\u00f3 kieg\u00e9sz\u00edt\u00e9se","text":"<p>Priorit\u00e1s: Magas | Komplexit\u00e1s: K\u00f6zepes</p> <ul> <li>[ ] README.md friss\u00edt\u00e9se</li> <li>Telep\u00edt\u00e9si \u00fatmutat\u00f3</li> <li>Haszn\u00e1lati p\u00e9ld\u00e1k (basic, advanced)</li> <li>API referencia \u00f6sszefoglal\u00f3</li> <li>Troubleshooting szekci\u00f3</li> <li>[ ] API dokument\u00e1ci\u00f3 gener\u00e1l\u00e1sa</li> <li>Sphinx vagy mkdocs v\u00e1laszt\u00e1s</li> <li>Docstring-ek ellen\u0151rz\u00e9se</li> <li>Hosted docs (GitHub Pages / ReadTheDocs)</li> </ul>"},{"location":"ACTION_PLAN/#112-csomagolas-es-publikalas","title":"11.2 Csomagol\u00e1s \u00e9s publik\u00e1l\u00e1s","text":"<p>Priorit\u00e1s: Magas | Komplexit\u00e1s: K\u00f6zepes</p> <ul> <li>[ ] pyproject.toml kieg\u00e9sz\u00edt\u00e9se</li> <li>Build system (hatchling/setuptools)</li> <li>Entry points</li> <li>Dependencies specifik\u00e1ci\u00f3</li> <li>[ ] PyPI felt\u00f6lt\u00e9s el\u0151k\u00e9sz\u00edt\u00e9se</li> <li>Package name ellen\u0151rz\u00e9s</li> <li>Verzi\u00f3kezel\u00e9s strat\u00e9gia (semver)</li> <li>CHANGELOG.md l\u00e9trehoz\u00e1sa</li> <li>[ ] Distribution tesztel\u00e9s</li> <li>TestPyPI felt\u00f6lt\u00e9s</li> <li>Clean install teszt</li> </ul>"},{"location":"ACTION_PLAN/#113-integracios-tesztek-valos-api-kkal","title":"11.3 Integr\u00e1ci\u00f3s tesztek val\u00f3s API-kkal","text":"<p>Priorit\u00e1s: K\u00f6zepes | Komplexit\u00e1s: K\u00f6zepes</p> <ul> <li>[ ] FMP integr\u00e1ci\u00f3 tesztel\u00e9se (\u00e9les API kulccsal)</li> <li>[ ] Polygon integr\u00e1ci\u00f3 tesztel\u00e9se</li> <li>[ ] FRED integr\u00e1ci\u00f3 tesztel\u00e9se</li> <li>[ ] Cross-provider tesztek (p\u00e1rhuzamos lek\u00e9rdez\u00e9sek)</li> </ul>"},{"location":"ACTION_PLAN/#114-tovabbi-fejlesztesek","title":"11.4 Tov\u00e1bbi fejleszt\u00e9sek","text":"<p>Priorit\u00e1s: Alacsony | Komplexit\u00e1s: Magas</p> <ul> <li>[ ] \u00daj provider: Alpha Vantage</li> <li>[ ] \u00daj provider: Yahoo Finance</li> <li>[ ] Rate limiting finomhangol\u00e1s (adaptive)</li> <li>[ ] Webhook/callback t\u00e1mogat\u00e1s</li> </ul>"},{"location":"ACTION_PLAN/#115-production-hasznalat","title":"11.5 Production haszn\u00e1lat","text":"<p>St\u00e1tusz: \u2705 K\u00c9SZEN \u00c1LL</p> <p>A projekt production-ready: - 441 teszt passing - 92% code coverage - CI/CD pipeline akt\u00edv - Security tesztek (API key sanitization) - HTTPS enforced - Circuit breaker v\u00e9delem</p>"},{"location":"ACTION_PLAN/#revision-history_1","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 Lead Agent Initial action plan with realistic effort estimates 1.1 2026-01-31 Claude Updated status: M1-M4 COMPLETE, added Next Steps section"},{"location":"ARCHITECTURE/","title":"Architecture Design","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: Draft Based on: MoneyFlows Data Loader v2.7.0</p>"},{"location":"ARCHITECTURE/#1-overview","title":"1. Overview","text":""},{"location":"ARCHITECTURE/#11-purpose","title":"1.1 Purpose","text":"<p>OmniData Nexus Core is a modular asynchronous Python framework providing a unified Data Access Layer for aggregating financial and macroeconomic data from heterogeneous API sources (FMP Ultimate, Polygon, FRED). It abstracts away provider-specific complexity, implements production-grade resilience patterns (circuit breaker, exponential backoff, QoS semaphore routing), and provides intelligent caching with atomic writes to enable fast, reliable quantitative research for a solo developer.</p>"},{"location":"ARCHITECTURE/#12-architecture-goals","title":"1.2 Architecture Goals","text":"Priority Goal Rationale 1 Simplicity Solo dev environment - prioritize maintainability over abstraction; avoid over-engineering 2 Resilience Production-grade error handling (circuit breaker, backoff) to survive API failures without manual intervention 3 Performance Maximize throughput via async I/O and provider-specific concurrency limits (QoS semaphore routing) 4 Extensibility Plugin-style data sources allow easy addition of new APIs without core modifications 5 Transparency Clear separation of concerns - caching, circuit breaking, rate limiting as orthogonal layers"},{"location":"ARCHITECTURE/#13-architecture-style","title":"1.3 Architecture Style","text":"<p>Selected: Modular Monolith with Plugin Architecture</p> <p>Rationale: - Single developer: No operational overhead of distributed systems (no service mesh, no message brokers, no container orchestration) - Research workload: Request patterns are batch-oriented, not requiring microservice-style independent scaling - Shared infrastructure: Circuit breaker, caching, QoS router are common concerns - better centralized than duplicated - Deployment simplicity: Single Python process, no network latency between components - Fast iteration: Refactoring within a monolith is easier than coordinating across service boundaries</p> <p>Alternatives Considered:</p> Alternative Pros Cons Why Rejected Microservices (per provider) Independent deployment, language flexibility Operational complexity, network latency, distributed state management Overkill for solo dev; no need for independent scaling Serverless (AWS Lambda per endpoint) Auto-scaling, pay-per-use Cold start latency, vendor lock-in, difficult local testing Research workload doesn't match serverless burst pattern Pure Library (no framework) Maximum flexibility No standardization, repeated implementation of resilience patterns Would duplicate circuit breaker/cache logic across consumers"},{"location":"ARCHITECTURE/#2-system-context","title":"2. System Context","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        EXTERNAL SYSTEMS                              \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502  FMP Ultimate\u2502      \u2502  Polygon.io  \u2502      \u2502  FRED API    \u2502      \u2502\n\u2502  \u2502              \u2502      \u2502              \u2502      \u2502              \u2502      \u2502\n\u2502  \u2502 13 endpoints \u2502      \u2502 4 endpoints  \u2502      \u2502 32 series    \u2502      \u2502\n\u2502  \u2502 Rate: Plan   \u2502      \u2502 Rate: 5-10/m \u2502      \u2502 Rate: ~120/m \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502         \u2502                     \u2502                     \u2502              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                               \u2502                                    \u2502\n\u2502                     HTTPS/REST over Internet                        \u2502\n\u2502                               \u2502                                    \u2502\n\u2502                               \u25bc                                    \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502              \u2502   OmniData Nexus Core          \u2502                     \u2502\n\u2502              \u2502   (Python 3.9+ Framework)      \u2502                     \u2502\n\u2502              \u2502                                \u2502                     \u2502\n\u2502              \u2502  Unified DataLoader Interface  \u2502                     \u2502\n\u2502              \u2502  + Circuit Breaker             \u2502                     \u2502\n\u2502              \u2502  + QoS Semaphore Router        \u2502                     \u2502\n\u2502              \u2502  + Intelligent Cache           \u2502                     \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                               \u2502                                    \u2502\n\u2502                               \u25bc                                    \u2502\n\u2502                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502                \u2502  Quantitative Researcher  \u2502                        \u2502\n\u2502                \u2502  (Primary User)           \u2502                        \u2502\n\u2502                \u2502  - Jupyter Notebooks      \u2502                        \u2502\n\u2502                \u2502  - Python Scripts         \u2502                        \u2502\n\u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2502                                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#external-interfaces","title":"External Interfaces","text":"System Protocol Direction Purpose Rate Limit FMP Ultimate HTTPS REST (JSON) OUT Company fundamentals, insider trading, financials Plan-dependent (3 concurrent via QoS) Polygon.io HTTPS REST (JSON) OUT Market data, trades, options snapshots 5 req/min (free), 10 concurrent via QoS FRED HTTPS REST (JSON) OUT Macroeconomic indicators (32 series) ~120 req/min recommended, 1 concurrent via QoS"},{"location":"ARCHITECTURE/#3-component-architecture","title":"3. Component Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       OmniData Nexus Core                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502              DataLoader (Unified Interface)              \u2502         \u2502\n\u2502  \u2502  - get_fmp_data(session, endpoint, **params)            \u2502         \u2502\n\u2502  \u2502  - get_polygon_data(session, endpoint, **params)        \u2502         \u2502\n\u2502  \u2502  - get_fred_data(session, series_id, **params)          \u2502         \u2502\n\u2502  \u2502  - get_api_health_report() \u2192 dict                       \u2502         \u2502\n\u2502  \u2502  - set_operating_mode(mode: LIVE | READ_ONLY)           \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502               \u2502                              \u2502                        \u2502\n\u2502               \u25bc                              \u25bc                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n\u2502  \u2502  QoS Semaphore      \u2502        \u2502   Circuit Breaker       \u2502          \u2502\n\u2502  \u2502  Router             \u2502        \u2502   Manager               \u2502          \u2502\n\u2502  \u2502                     \u2502        \u2502                         \u2502          \u2502\n\u2502  \u2502  - FMP: max=3       \u2502        \u2502  - threshold: 20%       \u2502          \u2502\n\u2502  \u2502  - Polygon: max=10  \u2502        \u2502  - states: CLOSED/      \u2502          \u2502\n\u2502  \u2502  - FRED: max=1      \u2502        \u2502    OPEN/HALF-OPEN       \u2502          \u2502\n\u2502  \u2502  - acquire(provider)\u2502        \u2502  - per-provider state   \u2502          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n\u2502            \u2502                               \u2502                         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                        \u25bc                                             \u2502\n\u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502       \u2502      Provider Abstraction Layer        \u2502                     \u2502\n\u2502       \u2502  (Base class: BaseDataProvider)        \u2502                     \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502                        \u2502                                             \u2502\n\u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n\u2502        \u25bc               \u25bc               \u25bc                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n\u2502  \u2502   FMP    \u2502   \u2502 Polygon  \u2502   \u2502    FRED      \u2502                     \u2502\n\u2502  \u2502 Provider \u2502   \u2502 Provider \u2502   \u2502  Provider    \u2502                     \u2502\n\u2502  \u2502          \u2502   \u2502          \u2502   \u2502              \u2502                     \u2502\n\u2502  \u2502 - 13 EPs \u2502   \u2502 - 4 EPs  \u2502   \u2502 - 32 series  \u2502                     \u2502\n\u2502  \u2502 - auth() \u2502   \u2502 - auth() \u2502   \u2502 - auth()     \u2502                     \u2502\n\u2502  \u2502 - fetch()\u2502   \u2502 - fetch()\u2502   \u2502 - fetch()    \u2502                     \u2502\n\u2502  \u2502 - norm() \u2502   \u2502 - norm() \u2502   \u2502 - norm()     \u2502                     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n\u2502       \u2502              \u2502               \u2502                              \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                      \u25bc                                              \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502         \u2502  Retry &amp; Backoff Handler   \u2502                              \u2502\n\u2502         \u2502  - Exponential delay       \u2502                              \u2502\n\u2502         \u2502  - Jitter randomization    \u2502                              \u2502\n\u2502         \u2502  - Max retries: 3          \u2502                              \u2502\n\u2502         \u2502  - Retry only 5xx/timeout  \u2502                              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                      \u25bc                                              \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502         \u2502     HTTP Client Layer      \u2502                              \u2502\n\u2502         \u2502  (aiohttp.ClientSession)   \u2502                              \u2502\n\u2502         \u2502  - Connection pooling      \u2502                              \u2502\n\u2502         \u2502  - Timeout management      \u2502                              \u2502\n\u2502         \u2502  - API key sanitization    \u2502                              \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                      \u2502                                              \u2502\n\u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502         \u25bc                            \u25bc                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n\u2502  \u2502 Cache Manager\u2502            \u2502 Health Monitor   \u2502                   \u2502\n\u2502  \u2502              \u2502            \u2502                  \u2502                   \u2502\n\u2502  \u2502 - get_cached()\u2502            \u2502 - request_count  \u2502                   \u2502\n\u2502  \u2502 - set_cache()\u2502            \u2502 - error_count    \u2502                   \u2502\n\u2502  \u2502 - TTL: 7d    \u2502            \u2502 - error_rate     \u2502                   \u2502\n\u2502  \u2502 - atomic     \u2502            \u2502 - rate_limit_%   \u2502                   \u2502\n\u2502  \u2502   writes     \u2502            \u2502 - circuit_state  \u2502                   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u25bc                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                           \u2502\n\u2502  \u2502      Filesystem (JSON Cache)         \u2502                           \u2502\n\u2502  \u2502  data/                               \u2502                           \u2502\n\u2502  \u2502  \u251c\u2500\u2500 fmp_cache/{endpoint}/{date}/    \u2502                           \u2502\n\u2502  \u2502  \u2502   \u2514\u2500\u2500 {symbol}.json               \u2502                           \u2502\n\u2502  \u2502  \u251c\u2500\u2500 polygon_cache/{endpoint}/       \u2502                           \u2502\n\u2502  \u2502  \u2502   \u2514\u2500\u2500 {hash}.json                 \u2502                           \u2502\n\u2502  \u2502  \u2514\u2500\u2500 fred_cache/{series_id}/         \u2502                           \u2502\n\u2502  \u2502      \u2514\u2500\u2500 {date_range}.json           \u2502                           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                           \u2502\n\u2502                                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#component-descriptions","title":"Component Descriptions","text":"Component Responsibility Dependencies Location DataLoader Unified entry point for all data sources; orchestrates QoS, circuit breaker, provider selection All components <code>src/data_loader/loader.py</code> QoS Semaphore Router Enforces provider-specific concurrency limits; prevents rate limit violations Config Manager <code>src/data_loader/qos_router.py</code> Circuit Breaker Manager Monitors error rates; halts requests when threshold exceeded (&gt;20%); manages recovery Health Monitor <code>src/data_loader/circuit_breaker.py</code> BaseDataProvider Abstract base class defining provider interface (auth, fetch, normalize, cache_key) None <code>src/data_loader/providers/base.py</code> FMP Provider FMP-specific implementation; 13 endpoints (screener, profile, financials, etc.) BaseDataProvider, HTTP Client <code>src/data_loader/providers/fmp.py</code> Polygon Provider Polygon-specific implementation; 4 endpoints (aggs, trades, options, snapshot) BaseDataProvider, HTTP Client <code>src/data_loader/providers/polygon.py</code> FRED Provider FRED-specific implementation; 32 economic series BaseDataProvider, HTTP Client <code>src/data_loader/providers/fred.py</code> Retry &amp; Backoff Handler Implements exponential backoff with jitter; retries transient failures (5xx, timeouts) HTTP Client <code>src/data_loader/retry.py</code> HTTP Client Layer Async HTTP requests via aiohttp; connection pooling; timeout management aiohttp.ClientSession <code>src/data_loader/http_client.py</code> Cache Manager Read/write JSON cache; provider-specific paths; TTL management; atomic writes (temp+rename) Filesystem <code>src/data_loader/cache.py</code> Health Monitor Tracks request counts, error rates, rate limit consumption; generates health reports None <code>src/data_loader/health.py</code> Config Manager Loads API keys from environment; manages settings (TTLs, concurrency limits, timeouts) python-dotenv <code>src/data_loader/config.py</code>"},{"location":"ARCHITECTURE/#4-data-architecture","title":"4. Data Architecture","text":""},{"location":"ARCHITECTURE/#41-data-model","title":"4.1 Data Model","text":"<p>Domain Entities:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   DataRequest           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 provider: str           \u2502  (fmp | polygon | fred)\n\u2502 endpoint: str           \u2502  (profile, aggs_daily, CPIAUCSL)\n\u2502 params: dict            \u2502  (symbol, start_date, etc.)\n\u2502 cache_key: str          \u2502  MD5(provider+endpoint+params)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 1:1\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   DataResponse          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 data: dict | list       \u2502  Normalized response\n\u2502 source: str             \u2502  (cache | api)\n\u2502 timestamp: datetime     \u2502  Fetch time\n\u2502 provider: str           \u2502\n\u2502 error: str | None       \u2502  Error message if failed\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u2502 1:N\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CacheEntry            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 cache_key: str          \u2502  Unique identifier\n\u2502 data: dict              \u2502  Response payload\n\u2502 created_at: datetime    \u2502  Cache creation time\n\u2502 ttl_seconds: int        \u2502  Time to live (default 604800)\n\u2502 provider: str           \u2502\n\u2502 endpoint: str           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Health Monitoring Model:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   HealthReport          \u2502       \u2502   ProviderHealth        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524       \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 timestamp: datetime     \u2502\u2500\u2500\u25001:N\u2500\u2502 provider: str           \u2502\n\u2502 overall_status: str     \u2502       \u2502 total_requests: int     \u2502\n\u2502 providers: list[...]    \u2502       \u2502 errors: int             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502 error_rate: float       \u2502\n                                  \u2502 circuit_state: str      \u2502\n                                  \u2502 rate_limit_pct: float   \u2502\n                                  \u2502 last_error: str | None  \u2502\n                                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#42-storage-decisions","title":"4.2 Storage Decisions","text":"Data Type Storage Format Rationale API Responses Filesystem (JSON) <code>data/{provider}_cache/{path}/{file}.json</code> Simple, versioned, human-readable; no DB overhead Configuration Environment variables + <code>.env</code> file Key-value pairs Standard practice; keeps secrets out of code API Keys Environment variables <code>FMP_KEY</code>, <code>POLYGON_KEY</code>, <code>FRED_KEY</code> Security best practice; never in version control Logs Filesystem (rotating) <code>logs/nexus_core.log</code> Standard Python logging; rotation prevents disk fill Metrics (future) In-memory counters Python dicts Lightweight; no persistence needed for research workload <p>Cache Path Strategy:</p> Provider Path Pattern Example Rationale FMP <code>data/fmp_cache/{endpoint}/{date}/{symbol}.json</code> <code>data/fmp_cache/profile/2026-01-31/AAPL.json</code> Date-based expiration; easy manual inspection Polygon <code>data/polygon_cache/{endpoint}/{hash}.json</code> <code>data/polygon_cache/aggs_daily/a3f2e9d1b8c4.json</code> Hash handles complex query params (date ranges) FRED <code>data/fred_cache/{series_id}/{date_range}.json</code> <code>data/fred_cache/CPIAUCSL/2020-01-01_2026-01-31.json</code> Series-centric; date range in filename"},{"location":"ARCHITECTURE/#43-data-flow","title":"4.3 Data Flow","text":"<p>Happy Path (Cache Miss):</p> <pre><code>User Request\n     \u2502\n     \u25bc\n[DataLoader.get_fmp_data(\"profile\", symbol=\"AAPL\")]\n     \u2502\n     \u251c\u2500\u2500\u2500\u2500\u2500\u25b6 [Check Operating Mode] \u2500\u2500READONLY\u2500\u2500\u25b6 [Cache.get()] \u2500\u2500MISS\u2500\u2500\u25b6 [Raise Error]\n     \u2502                                   \u2502\n     \u2502                                  LIVE\n     \u25bc                                   \u2502\n[Cache.get(key)] \u2500\u2500HIT\u2500\u2500\u25b6 [Return cached data]\n     \u2502                                   \u2502\n    MISS                                 \u2502\n     \u2502                                   \u2502\n     \u25bc                                   \u2502\n[Circuit Breaker.check()] \u2500\u2500OPEN\u2500\u2500\u25b6 [Raise CircuitOpenError]\n     \u2502                                   \u2502\n   CLOSED                                \u2502\n     \u2502                                   \u2502\n     \u25bc                                   \u2502\n[QoS Semaphore.acquire(fmp)] \u2500\u2500max=3\u2500\u2500\u2500\u2500\u2524\n     \u2502                                   \u2502\n     \u25bc                                   \u2502\n[FMPProvider.fetch(endpoint, params)]   \u2502\n     \u2502                                   \u2502\n     \u251c\u2500\u2500\u25b6 [HTTP GET] \u2500\u2500429\u2500\u2500\u25b6 [Parse Retry-After] \u2500\u2500\u25b6 [Sleep] \u2500\u2500\u25b6 [Retry]\n     \u2502                 \u2502\n     \u2502                5xx\n     \u2502                 \u2502\n     \u251c\u2500\u2500\u25b6 [Retry Handler] \u2500\u2500\u25b6 [Exponential Backoff + Jitter] \u2500\u2500\u25b6 [Retry up to 3x]\n     \u2502                 \u2502\n     \u2502                200\n     \u25bc                 \u2502\n[Validate Schema]      \u2502\n     \u2502                 \u2502\n     \u25bc                 \u2502\n[Normalize Data]       \u2502\n     \u2502                 \u2502\n     \u25bc                 \u2502\n[Cache.set(key, data, ttl=7d)] \u2500\u2500atomic write\u2500\u2500\u25b6 [temp.json] \u2500\u2500rename\u2500\u2500\u25b6 [final.json]\n     \u2502                 \u2502\n     \u25bc                 \u2502\n[QoS Semaphore.release(fmp)]\n     \u2502\n     \u25bc\n[Return DataResponse]\n</code></pre> <p>Error Path (Circuit Breaker Opens):</p> <pre><code>Multiple Failures (&gt;20% error rate)\n     \u2502\n     \u25bc\n[Circuit Breaker] \u2500\u2500error_rate &gt; 0.2\u2500\u2500\u25b6 [State = OPEN]\n     \u2502\n     \u25bc\n[Subsequent Requests] \u2500\u2500\u25b6 [Immediate Failure: CircuitOpenError]\n     \u2502\n     \u25bc\n[After timeout period] \u2500\u2500\u25b6 [State = HALF-OPEN]\n     \u2502\n     \u25bc\n[Single test request] \u2500\u2500SUCCESS\u2500\u2500\u25b6 [State = CLOSED] \u2500\u2500\u25b6 [Normal operation]\n                       \u2502\n                     FAILURE\n                       \u2502\n                       \u25bc\n                  [State = OPEN]\n</code></pre>"},{"location":"ARCHITECTURE/#5-technology-stack","title":"5. Technology Stack","text":"Layer Technology Version Rationale Language Python 3.9+ Async/await improvements; widespread in quant research; asyncio stability Async HTTP aiohttp \u22653.8 Production-ready async HTTP client; connection pooling; timeout control Environment python-dotenv \u22650.19 Standard .env file loading; keeps secrets out of code Testing pytest \u22657.0 Industry standard; excellent async support via pytest-asyncio Coverage pytest-cov \u22653.0 Coverage reporting; integrated with pytest Type Hints typing (built-in) - Improved IDE support; runtime type validation possible Logging logging (built-in) - Standard library; rotating file handlers; no external deps JSON json (built-in) - Sufficient for cache; no serialization edge cases Hashing hashlib (built-in) - MD5 for cache keys (speed prioritized over crypto security)"},{"location":"ARCHITECTURE/#dependencies","title":"Dependencies","text":"Package Purpose License Risk Assessment aiohttp Async HTTP client Apache 2.0 Low - mature, widely used python-dotenv Environment variable loading BSD-3 Low - simple, no network calls pytest Testing framework MIT Low - dev dependency only pytest-cov Coverage reporting MIT Low - dev dependency only pytest-asyncio Async test support Apache 2.0 Low - dev dependency only <p>No database dependencies - Filesystem cache sufficient for research workload.</p>"},{"location":"ARCHITECTURE/#6-architecture-decisions","title":"6. Architecture Decisions","text":"ID Decision Context Consequences AD-001 Use Modular Monolith over Microservices Solo developer; research workload (batch-oriented, not user-facing); shared infrastructure (circuit breaker, cache, QoS) (+) Simple deployment, fast iteration, no network overhead(-) All components share failure domain AD-002 Filesystem JSON cache (not Redis/DB) Single user; dataset fits in memory; human-readable cache aids debugging (+) Zero external dependencies, simple backups, inspectable(-) No TTL auto-expiration, manual cleanup needed AD-003 Provider-specific concurrency limits (QoS Semaphore) Each API has different rate limits (FMP:3, Polygon:10, FRED:1) (+) Prevents rate limit violations, maximizes throughput(-) Requires tuning per API subscription tier AD-004 Circuit Breaker per provider (not global) API failures are independent (FMP down \u2260 Polygon down) (+) Isolates failures, allows partial service(-) More complex state management AD-005 Atomic cache writes (temp + rename) Concurrent reads during writes could read partial JSON (+) Guarantees consistency, prevents corruption(-) Extra filesystem operation per write AD-006 Plugin architecture (BaseDataProvider) Need to add new APIs (Alpha Vantage, IEX) without core changes (+) Extensibility, separation of concerns(-) Slight abstraction overhead AD-007 Exponential backoff with jitter (not linear) Prevent thundering herd on API recovery (+) Better distribution of retries, faster recovery(-) More complex retry logic AD-008 Operating modes (LIVE vs READ-ONLY) Support offline analysis; prevent accidental API calls during testing (+) Explicit control, enables airplane mode(-) Mode management adds complexity AD-009 Schema validation (warn, don't fail) API schemas evolve; prefer partial data over total failure (+) Robustness to API changes, research continuity(-) May mask breaking changes AD-010 Async/await (not threading) I/O-bound workload; Python GIL limits thread scaling (+) Efficient concurrency, single-threaded simplicity(-) All code must be async-compatible"},{"location":"ARCHITECTURE/#7-deployment","title":"7. Deployment","text":""},{"location":"ARCHITECTURE/#71-deployment-model","title":"7.1 Deployment Model","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Local Development Environment               \u2502\n\u2502                      (macOS / Linux)                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Python 3.9+ Virtual Environment                       \u2502 \u2502\n\u2502  \u2502  (venv / conda)                                        \u2502 \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n\u2502  \u2502                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  OmniData Nexus Core (src/)                      \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - DataLoader                                    \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Providers (FMP, Polygon, FRED)                \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Circuit Breaker, QoS Router, Cache            \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502                                                         \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502\n\u2502  \u2502  \u2502  Consumer Applications                           \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Jupyter Notebooks (analysis/)                 \u2502  \u2502 \u2502\n\u2502  \u2502  \u2502  - Python Scripts (scripts/)                     \u2502  \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502\n\u2502  \u2502                                                         \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Filesystem Storage                                    \u2502 \u2502\n\u2502  \u2502  - data/fmp_cache/                                     \u2502 \u2502\n\u2502  \u2502  - data/polygon_cache/                                 \u2502 \u2502\n\u2502  \u2502  - data/fred_cache/                                    \u2502 \u2502\n\u2502  \u2502  - logs/nexus_core.log                                 \u2502 \u2502\n\u2502  \u2502  - .env (API keys)                                     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u2502 HTTPS\n                         \u25bc\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  External APIs (Internet)     \u2502\n         \u2502  - FMP Ultimate               \u2502\n         \u2502  - Polygon.io                 \u2502\n         \u2502  - FRED                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ARCHITECTURE/#72-environments","title":"7.2 Environments","text":"Environment Purpose Configuration API Keys Development Local testing, research analysis <code>.env</code> file with real API keys Real (rate-limited tier) Testing Unit/integration tests <code>.env.test</code> with mock API keys Mock/test keys CI (future) Automated testing Environment variables, mock HTTP responses No real API calls <p>No Production Environment - Research tool runs locally only.</p>"},{"location":"ARCHITECTURE/#73-installation","title":"7.3 Installation","text":"<pre><code># 1. Clone repository\ngit clone &lt;repo_url&gt; nexus_core\ncd nexus_core\n\n# 2. Create virtual environment\npython3.9 -m venv venv\nsource venv/bin/activate\n\n# 3. Install dependencies\npip install -r requirements.txt\n\n# 4. Configure API keys\ncp .env.example .env\n# Edit .env with real API keys:\n# FMP_KEY=your_fmp_key\n# POLYGON_KEY=your_polygon_key\n# FRED_KEY=your_fred_key\n\n# 5. Create cache directories\nmkdir -p data/fmp_cache data/polygon_cache data/fred_cache logs\n\n# 6. Verify installation\npython -c \"from src.data_loader import DataLoader; print('OK')\"\n</code></pre> <p>Target: &lt;5 minutes to first successful fetch.</p>"},{"location":"ARCHITECTURE/#8-cross-cutting-concerns","title":"8. Cross-Cutting Concerns","text":"Concern Approach Implementation Error Handling Layered strategy: retry transient (5xx), circuit break persistent (&gt;20%), fail fast permanent (4xx) Try/except in each layer; custom exceptions (CircuitOpenError, CacheMissError, ValidationError) Logging Structured logging with provider/endpoint context; API key sanitization Python <code>logging</code> module; rotating file handler (10MB, 5 backups); log level configurable via env Configuration Environment variables for secrets; Python module for defaults <code>config.py</code> loads from <code>.env</code>; fallback defaults for non-secrets (TTL=7d, max_retries=3) Monitoring In-memory counters for requests/errors; health report API <code>HealthMonitor</code> class tracks per-provider metrics; exposed via <code>get_api_health_report()</code> Security API key sanitization in logs; no keys in cache files or errors Regex-based key redaction in log formatter; validate no keys in JSON before cache write Concurrency Async/await for I/O; semaphores for rate limiting <code>asyncio.Semaphore</code> per provider; single event loop per process Testing Unit tests (mocked HTTP), integration tests (real HTTP), coverage &gt;80% pytest with pytest-asyncio; <code>aioresponses</code> for HTTP mocking; separate test fixtures per provider Documentation Inline docstrings (Google style); architecture docs; API reference Sphinx-ready docstrings; <code>docs/</code> folder for ADRs and guides"},{"location":"ARCHITECTURE/#9-quality-attributes","title":"9. Quality Attributes","text":"Attribute Target How Achieved Measurement Performance Single API request &lt;2s p95 Async I/O, connection pooling, cache hits Timer wrapper on fetch methods Scalability 50+ concurrent requests (within QoS limits) asyncio.gather(), semaphore-controlled concurrency Benchmark script with varying concurrency Reliability &gt;95% success rate (excluding API outages) Circuit breaker, exponential backoff, retry logic success_count / total_count metric Maintainability &gt;80% test coverage; clear separation of concerns Modular design, plugin architecture, comprehensive tests pytest-cov report Extensibility Add new provider in &lt;4 hours BaseDataProvider abstract class with template Time-to-implement new source metric Usability &lt;5 minute setup; single-method API calls Minimal dependencies, clear examples, unified interface Timed installation by new user Security No API keys in logs/cache/git Sanitization filters, .gitignore, env var validation Automated grep in CI Availability Partial service during single API outage Per-provider circuit breaker, independent failure domains Health check during simulated outage"},{"location":"ARCHITECTURE/#10-risks-mitigations","title":"10. Risks &amp; Mitigations","text":"Risk Probability Impact Mitigation Owner API schema breaking change Medium High Schema validation with warnings; graceful degradation; version pinning in provider Developer Rate limit exhaustion Medium Medium QoS semaphore enforces limits; 429 handling; warning at 80% consumption Framework (automated) API service outage Medium Medium Circuit breaker halts requests; cache serves stale data; clear error messages Framework (automated) Cache corruption Low Medium Atomic writes (temp+rename); validate JSON before write; backup strategy Framework (automated) Dependency vulnerability Low Medium Minimal dependencies; regular <code>pip audit</code>; version pinning in requirements.txt Developer Thundering herd on recovery Low Low Exponential backoff with jitter; circuit breaker half-open state (single test request) Framework (automated) Memory exhaustion (large datasets) Low High Not mitigated in v1.0; future: streaming/pagination; document memory limits Developer (docs) API key leakage Low Critical .gitignore for .env; sanitization in logs; no keys in cache; pre-commit hook validation Developer + Framework"},{"location":"ARCHITECTURE/#11-future-considerations","title":"11. Future Considerations","text":"<p>Not in v1.0, but architectural support planned:</p> Feature Architectural Preparation Effort Estimate Plugin Discovery BaseDataProvider already abstract; add registry pattern 8h Metrics Export (Prometheus) HealthMonitor counters ready; add exposition endpoint 12h Request Batching Provider interface supports batch methods; add batch scheduler 16h Streaming/Pagination Add async iterator support to provider interface 20h Database Cache Backend Cache abstraction layer ready; implement SQLite/Postgres backend 24h Dark Pool Detection Polygon trades endpoint fetched; add filter logic 8h"},{"location":"ARCHITECTURE/#12-constraints-assumptions","title":"12. Constraints &amp; Assumptions","text":""},{"location":"ARCHITECTURE/#constraints","title":"Constraints","text":"<ul> <li>Python 3.9+ required (async/await, type hints)</li> <li>Single user - no multi-tenancy, no authentication</li> <li>Local execution - no distributed deployment</li> <li>Internet required - no offline mode beyond cache (READ-ONLY mode)</li> <li>Concurrency limits - FMP (3), Polygon (10), FRED (1) enforced</li> </ul>"},{"location":"ARCHITECTURE/#assumptions","title":"Assumptions","text":"ID Assumption Validation Risk if Wrong A-001 Datasets fit in memory (&lt;10GB per request) Monitor memory usage during testing Need streaming/pagination (20h effort) A-002 API schemas stable for 6+ months Version detection in provider Manual normalization updates needed A-003 Solo developer has valid API keys for all 3 providers Check at startup Graceful degradation to available providers A-004 Filesystem cache sufficient (no concurrency from multiple processes) Document limitation Need file locking or DB backend A-005 Research workload tolerates 7-day stale data User configurable TTL Reduce TTL or invalidate cache manually"},{"location":"ARCHITECTURE/#approval","title":"Approval","text":"Role Name Date Status Author Architect Agent 2026-01-31 \u2705 Generated Reference MoneyFlows Data Loader v2.7.0 - \u2705 Integrated Owner Solo Developer \u2610 Pending Review"},{"location":"ARCHITECTURE/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 Architect Agent Initial architecture design based on approved requirements <p>Architecture designed for simplicity, resilience, and extensibility. Built on proven patterns from MoneyFlows Data Loader v2.7.0.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to OmniData Nexus Core will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#110-2026-01-31","title":"[1.1.0] - 2026-01-31","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>[TC-001] FMP API: Migrated from <code>/api/v3/</code> to <code>/stable/</code> endpoints</li> <li>FMP deprecated the old API in August 2025</li> <li>Symbol parameter changed from path param to query param</li> <li>All 13 FMP endpoints updated</li> <li>All unit/integration/e2e tests updated with new URL patterns</li> <li>Impact: Medium (breaking change for existing URL mocks)</li> </ul>"},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>[SC-001] Endpoint Health diagnostic tool (<code>tools/diagnostics/endpoint_health.py</code>)</li> <li>Log parser for API error detection (429, 5xx)</li> <li>Heatmap generation by provider/endpoint</li> <li>CSV export for analysis</li> <li>Supports FMP, Polygon, FRED providers</li> <li>CLI and Python API usage</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Test isolation in <code>test_config.py</code>: Fixed <code>.env</code> file loading during tests</li> <li><code>clean_env</code> fixture now temporarily renames <code>.env</code> to prevent <code>load_dotenv()</code> interference</li> <li>Updated <code>.gitignore</code> to exclude <code>.claude/</code>, <code>data/</code>, <code>logs/</code> directories</li> </ul>"},{"location":"CHANGELOG/#100-2026-01-31","title":"[1.0.0] - 2026-01-31","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Initial release of OmniData Nexus Core</li> <li>Core Infrastructure</li> <li>Configuration manager with <code>.env</code> support</li> <li>HTTP client with aiohttp</li> <li>Filesystem-based JSON cache with TTL</li> <li>Health monitoring and metrics</li> <li>Data Providers</li> <li>FMP provider (13 endpoints)</li> <li>Polygon provider (4 endpoints)</li> <li>FRED provider (32+ series)</li> <li>Resilience Layer</li> <li>Circuit breaker with configurable thresholds</li> <li>Exponential backoff retry with jitter</li> <li>QoS semaphore for concurrency control</li> <li>Rate limit (429) handling</li> <li>DataLoader</li> <li>Unified interface for all providers</li> <li>LIVE and READ_ONLY operating modes</li> <li>Automatic caching</li> <li>Health report aggregation</li> <li>Testing</li> <li>441 tests (unit, integration, e2e)</li> <li>92% code coverage</li> <li>Security tests for API key sanitization</li> <li>CI/CD</li> <li>GitHub Actions workflow</li> <li>Pre-commit hooks configured</li> </ul>"},{"location":"DECISIONS/","title":"Project Decisions","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: Draft Synthesized by: Synthesizer Agent</p>"},{"location":"DECISIONS/#1-project-identity","title":"1. Project Identity","text":"Field Value Name OmniData Nexus Core Description Modular asynchronous Python framework providing unified Data Access Layer for financial and macroeconomic data aggregation Type Personal Research Tool Phase DEVELOPMENT Based On MoneyFlows Data Loader v2.7.0"},{"location":"DECISIONS/#2-team-context","title":"2. Team Context","text":"Factor Value Implications Team Size 1 (Solo Developer) Prioritize simplicity over abstraction; avoid over-engineering; single failure domain acceptable AI Assisted Yes CLAUDE.md important for context; documentation supports AI collaboration Timeline Research-driven No hard deadlines; quality over speed Sustainable Pace Flexible Personal tool allows adaptive scheduling; no external commitments <p>Key Implications: - Modular Monolith architecture (not microservices) - no operational overhead - Scale-appropriate security (not enterprise) - <code>.env</code> + <code>.gitignore</code> sufficient - Manual processes acceptable (dependency updates, key rotation) - ~200 tests target (not thousands)</p>"},{"location":"DECISIONS/#3-problem-solution","title":"3. Problem &amp; Solution","text":""},{"location":"DECISIONS/#problem-from-requirementsmd","title":"Problem (from REQUIREMENTS.md)","text":"<p>Quantitative finance research requires data from multiple specialized APIs (FMP Ultimate, Polygon.io, FRED), each with different authentication, response formats, rate limits, and availability patterns. There is no unified framework that:</p> <ul> <li>Provides a single interface for heterogeneous financial data sources</li> <li>Implements production-grade resilience (circuit breaker, exponential backoff)</li> <li>Manages concurrent requests with provider-specific limits</li> <li>Caches responses intelligently with atomic writes</li> <li>Monitors API health and rate limit consumption automatically</li> </ul>"},{"location":"DECISIONS/#solution-from-architecturemd","title":"Solution (from ARCHITECTURE.md)","text":"<p>A Modular Monolith with Plugin Architecture that provides:</p> <ul> <li>DataLoader - Unified entry point with <code>get_fmp_data()</code>, <code>get_polygon_data()</code>, <code>get_fred_data()</code> methods</li> <li>QoS Semaphore Router - Provider-specific concurrency limits (FMP:3, Polygon:10, FRED:1)</li> <li>Circuit Breaker Manager - Per-provider failure isolation with 20% error threshold</li> <li>Intelligent Cache - Filesystem JSON with atomic writes (temp + rename pattern)</li> <li>Health Monitor - Request/error tracking with per-provider status</li> </ul> <p>50 Endpoints Total: 13 FMP + 4 Polygon + 32 FRED series + 1 FRED base</p>"},{"location":"DECISIONS/#4-key-decisions","title":"4. Key Decisions","text":""},{"location":"DECISIONS/#41-technology-stack-from-architecturemd","title":"4.1 Technology Stack (from ARCHITECTURE.md)","text":"Layer Decision Rationale Language Python 3.9+ Async/await improvements; widespread in quant research; asyncio stability Async HTTP aiohttp (&gt;=3.8) Production-ready async client; connection pooling; timeout control Environment python-dotenv (&gt;=0.19) Standard .env file loading; keeps secrets out of code Testing pytest (&gt;=7.0) + pytest-cov (&gt;=3.0) + pytest-asyncio (&gt;=0.21) Industry standard; excellent async support HTTP Mocking aioresponses (&gt;=0.7) Mocks aiohttp.ClientSession for testing Type Checking mypy (&gt;=1.0) Static type analysis Linting ruff / flake8 Fast Python linter Caching Filesystem (JSON) Simple, human-readable, no DB overhead <p>No Database Dependencies - Filesystem cache sufficient for research workload.</p>"},{"location":"DECISIONS/#42-architecture-decisions","title":"4.2 Architecture Decisions","text":"ID Decision Source Consequences AD-001 Modular Monolith over Microservices ARCHITECTURE.md (+) Simple deployment, fast iteration, no network overhead; (-) All components share failure domain AD-002 Filesystem JSON cache (not Redis/DB) ARCHITECTURE.md (+) Zero external dependencies, inspectable; (-) No TTL auto-expiration, manual cleanup needed AD-003 Provider-specific concurrency limits (QoS Semaphore) ARCHITECTURE.md (+) Prevents rate limit violations, maximizes throughput; (-) Requires tuning per API subscription tier AD-004 Circuit Breaker per provider (not global) ARCHITECTURE.md (+) Isolates failures, allows partial service; (-) More complex state management AD-005 Atomic cache writes (temp + rename) ARCHITECTURE.md (+) Guarantees consistency, prevents corruption; (-) Extra filesystem operation per write AD-006 Plugin architecture (BaseDataProvider) ARCHITECTURE.md (+) Extensibility, separation of concerns; (-) Slight abstraction overhead AD-007 Exponential backoff with jitter (not linear) ARCHITECTURE.md (+) Prevents thundering herd, faster recovery; (-) More complex retry logic AD-008 Operating modes (LIVE vs READ-ONLY) ARCHITECTURE.md (+) Explicit control, enables airplane mode; (-) Mode management adds complexity AD-009 Schema validation (warn, don't fail) ARCHITECTURE.md (+) Robustness to API changes; (-) May mask breaking changes AD-010 Async/await (not threading) ARCHITECTURE.md (+) Efficient concurrency, single-threaded simplicity; (-) All code must be async-compatible"},{"location":"DECISIONS/#43-security-decisions-from-securitymd","title":"4.3 Security Decisions (from SECURITY.md)","text":"ID Decision Risk Level SD-001 API keys stored in <code>.env</code> file (plaintext) with chmod 600 + .gitignore LOW (appropriate for single-user local tool) SD-002 No encryption at rest for cache (public financial data) NEGLIGIBLE SD-003 No user authentication (single-user system) NONE (appropriate for use case) SD-004 Manual dependency updates (not automated) LOW (weekly pip audit schedule) SD-005 TLS 1.2+ enforced via aiohttp (HTTPS only) N/A (standard security) SD-006 API key sanitization in logs (regex-based redaction) N/A (required control) SD-007 Pre-commit hook for secret scanning N/A (required control) <p>Security Philosophy: Proportionate security for personal research tool. Focus on API key protection. NOT requiring OAuth, RBAC, SIEM, or enterprise-grade controls.</p>"},{"location":"DECISIONS/#44-quality-decisions-from-test_strategymd","title":"4.4 Quality Decisions (from TEST_STRATEGY.md)","text":"ID Decision Target QD-001 Overall line coverage &gt;80% QD-002 TIER 1 coverage (Circuit Breaker, QoS, Security, Cache Atomic) &gt;90% QD-003 TIER 2 coverage (DataLoader, Providers, Retry, Health) &gt;80% QD-004 TIER 3 coverage (Config, Cache read, HTTP wrapper) 60% QD-005 Test pass rate 100% QD-006 Test suite execution time &lt;2 minutes QD-007 Test pyramid ratio 75% unit / 20% integration / 5% E2E QD-008 Total test target ~200 tests QD-009 All external APIs mocked in CI Yes (no real API calls)"},{"location":"DECISIONS/#5-scope-boundaries","title":"5. Scope Boundaries","text":""},{"location":"DECISIONS/#in-scope-mvp","title":"In Scope (MVP)","text":"ID Feature Source Priority FR-001 Unified DataLoader Interface REQUIREMENTS.md MUST FR-002 FMP Ultimate Integration (13 endpoints) REQUIREMENTS.md MUST FR-003 Polygon Integration (4 endpoints) REQUIREMENTS.md MUST FR-004 FRED Integration (32 series) REQUIREMENTS.md MUST FR-005 QoS Semaphore Router REQUIREMENTS.md MUST FR-006 Intelligent Caching REQUIREMENTS.md MUST FR-007 Circuit Breaker REQUIREMENTS.md MUST FR-008 Exponential Backoff with Jitter REQUIREMENTS.md MUST FR-009 Rate Limit Handling REQUIREMENTS.md MUST FR-010 Health Check System REQUIREMENTS.md MUST FR-011 Schema-based Validation REQUIREMENTS.md MUST FR-012 Operating Modes (LIVE/READ-ONLY) REQUIREMENTS.md MUST FR-013 Configuration Management REQUIREMENTS.md MUST FR-014 API Key Sanitization REQUIREMENTS.md MUST FR-101 Cache TTL Configuration REQUIREMENTS.md SHOULD FR-102 Request Batching REQUIREMENTS.md SHOULD FR-103 Metrics Export REQUIREMENTS.md SHOULD FR-104 Graceful Degradation REQUIREMENTS.md SHOULD"},{"location":"DECISIONS/#explicitly-out-of-scope","title":"Explicitly Out of Scope","text":"Item Reason Revisit When GUI Dashboard Focus on programmatic API User requests UI Database Persistence Caller responsible for storage Multi-user requirement Data Analysis/Modeling Framework provides data, not analytics Never (separate concern) Multi-user Authentication Single-user research tool Multi-user requirement WebSocket Streaming REST endpoints sufficient for research Real-time requirement Cross-platform (Windows) POSIX assumed (macOS/Linux) User requests Windows support Enterprise Security (Vault, OAuth, RBAC) Overkill for personal tool Commercial deployment"},{"location":"DECISIONS/#deferred-post-mvp","title":"Deferred (Post-MVP)","text":"Item Effort Trigger FR-201: Plugin Discovery Registry 8h Adding 3+ new data sources FR-202: Dark Pool Detection 8h Research requires block trade analysis FR-203: Data Export (Parquet/HDF5) 12h Data persistence needs grow FR-204: CLI Tool 8h Manual query frequency increases FR-205: Async Iterator Support 20h Memory limits hit with large datasets Prometheus Metrics Export 12h Operational monitoring needed Database Cache Backend 24h Multi-process access required"},{"location":"DECISIONS/#6-known-issues-risks","title":"6. Known Issues &amp; Risks","text":""},{"location":"DECISIONS/#technical-risks","title":"Technical Risks","text":"Risk Probability Impact Mitigation API schema breaking change Medium High Schema validation with warnings; graceful degradation; version pinning in provider Rate limit exhaustion Medium Medium QoS semaphore enforces limits; 429 handling; warning at 80% consumption API service outage Medium Medium Circuit breaker halts requests; cache serves stale data; clear error messages Cache corruption Low Medium Atomic writes (temp+rename); validate JSON before write Dependency vulnerability Low Medium Minimal dependencies; regular pip audit; version pinning Thundering herd on recovery Low Low Exponential backoff with jitter; circuit breaker half-open state Memory exhaustion (large datasets) Low High NOT mitigated in v1.0; future: streaming/pagination; document memory limits API key leakage Low Critical .gitignore for .env; sanitization in logs; pre-commit hook validation"},{"location":"DECISIONS/#known-issues-accepted","title":"Known Issues (Accepted)","text":"Issue Location Why Accepted Review Date No TTL auto-expiration Cache Manager Filesystem cache simplicity; manual cleanup acceptable 2026-07-31 No multi-process cache access Cache Manager Single-user system; document limitation When multi-process needed 7-day stale data tolerance Cache TTL Research workload tolerates; user configurable User feedback Windows compatibility unknown POSIX assumption macOS/Linux primary environments User request"},{"location":"DECISIONS/#security-exceptions","title":"Security Exceptions","text":"Exception Reason Risk Approved By API Keys in Plaintext (.env) Single-user local system; encrypted secret store overkill LOW (file permissions + .gitignore) Solo Developer No Encryption at Rest (Cache) Public financial data; no confidentiality requirement NEGLIGIBLE Solo Developer No User Authentication Single-user system; no network exposure NONE Solo Developer Manual Dependency Updates Automated updates risk breaking changes LOW (weekly pip audit) Solo Developer"},{"location":"DECISIONS/#7-standards","title":"7. Standards","text":""},{"location":"DECISIONS/#coding-standards","title":"Coding Standards","text":"Aspect Standard Style ruff / flake8 (0 errors required) Type Hints mypy strict mode (0 type errors required) Docstrings Google style (Sphinx-ready) Naming snake_case (functions/variables), PascalCase (classes) Comments Required for complex logic; not for obvious code Max Line Length 120 characters (ruff default) Imports Sorted (isort compatible)"},{"location":"DECISIONS/#git-conventions","title":"Git Conventions","text":"Aspect Convention Branch naming <code>feature/FR-XXX-description</code>, <code>fix/issue-description</code>, <code>refactor/component-name</code> Commit messages Imperative mood: \"Add circuit breaker recovery logic\" (not \"Added...\") Commit scope Small, focused commits (one logical change per commit) Main branch <code>main</code> (protected, requires passing CI) Merge strategy Squash merge for feature branches"},{"location":"DECISIONS/#file-organization","title":"File Organization","text":"<pre><code>src/\n  data_loader/\n    __init__.py\n    loader.py              # DataLoader (unified interface)\n    qos_router.py          # QoS Semaphore Router\n    circuit_breaker.py     # Circuit Breaker Manager\n    retry.py               # Retry &amp; Backoff Handler\n    cache.py               # Cache Manager\n    health.py              # Health Monitor\n    config.py              # Config Manager\n    http_client.py         # HTTP Client Layer\n    providers/\n      __init__.py\n      base.py              # BaseDataProvider (abstract)\n      fmp.py               # FMP Provider\n      polygon.py           # Polygon Provider\n      fred.py              # FRED Provider\ntests/\n  conftest.py              # Shared fixtures\n  fixtures/                # Mock API responses\n  unit/                    # Unit tests (75%)\n  integration/             # Integration tests (20%)\n  e2e/                     # End-to-end tests (5%)\ndocs/\n  REQUIREMENTS.md\n  ARCHITECTURE.md\n  SECURITY.md\n  TEST_STRATEGY.md\n  DECISIONS.md             # This document\ndata/\n  fmp_cache/\n  polygon_cache/\n  fred_cache/\nlogs/\n  nexus_core.log\n</code></pre>"},{"location":"DECISIONS/#8-quality-gates","title":"8. Quality Gates","text":"Gate Threshold Blocking? Source Lint (ruff/flake8) 0 errors YES Standards Type Check (mypy) 0 type errors YES Standards Unit Tests 100% pass YES TEST_STRATEGY.md Integration Tests 100% pass YES TEST_STRATEGY.md E2E Tests 100% pass YES TEST_STRATEGY.md Overall Coverage &gt;=80% YES TEST_STRATEGY.md (NFR-006) TIER 1 Coverage &gt;=90% YES TEST_STRATEGY.md TIER 2 Coverage &gt;=80% NO (warning) TEST_STRATEGY.md Secret Scanning 0 API keys found YES SECURITY.md Test Execution Time &lt;2 minutes NO (warning) TEST_STRATEGY.md Security Vulnerabilities (pip audit) 0 HIGH/CRITICAL YES SECURITY.md <p>CI Pipeline Order: <pre><code>[Commit] -&gt; [Lint] -&gt; [Type Check] -&gt; [Unit] -&gt; [Integration] -&gt; [E2E] -&gt; [Coverage] -&gt; [Secret Scan] -&gt; [Pass]\n</code></pre></p>"},{"location":"DECISIONS/#9-excluded-paths","title":"9. Excluded Paths","text":"<p>Agents will NOT analyze these paths during ANALYSIS phase.</p> Path Reason <code>data/</code> Large cache files; not source code <code>data/fmp_cache/</code> Generated cache data <code>data/polygon_cache/</code> Generated cache data <code>data/fred_cache/</code> Generated cache data <code>__pycache__/</code> Python bytecode (generated) <code>.git/</code> Version control metadata <code>*.pyc</code> Python bytecode files <code>.env</code> Secret file (should not be analyzed) <code>.env.*</code> Environment variants <code>venv/</code> Virtual environment <code>*.egg-info/</code> Package metadata <code>htmlcov/</code> Coverage report output <code>logs/</code> Runtime logs <code>.pytest_cache/</code> Test cache <code>.mypy_cache/</code> Type checker cache <code>tests/fixtures/</code> Static test data (JSON mocks)"},{"location":"DECISIONS/#10-agent-instructions","title":"10. Agent Instructions","text":"<p>Special instructions for agents during ANALYSIS phase.</p> Agent Instruction security Focus on API key protection (SR-001 through SR-007). This is a personal tool - do NOT flag missing enterprise controls (Vault, OAuth, RBAC) as issues. Verify .gitignore includes .env. Check for key sanitization in logging. qa Verify coverage targets: &gt;80% overall, &gt;90% TIER 1. All tests must mock external APIs (no real HTTP calls in CI). Test pyramid: 75% unit / 20% integration / 5% E2E. architect Validate Modular Monolith approach. Check BaseDataProvider abstraction for extensibility. Verify circuit breaker is per-provider (not global). Confirm atomic write pattern in cache. analyst Verify all 50 endpoints documented: 13 FMP + 4 Polygon + 32 FRED series + 1 FRED base. Check NFRs are measurable. Validate MUST/SHOULD/COULD prioritization. dx Check setup time target (&lt;5 minutes). Verify installation instructions are complete. Validate .env.example exists. Check error messages are actionable. all Project phase is DEVELOPMENT (not LIVE). Scale-appropriate for solo developer. Avoid suggesting enterprise-grade solutions for personal tool."},{"location":"DECISIONS/#11-trade-offs-accepted","title":"11. Trade-offs Accepted","text":"Trade-off Our Choice Rationale Simplicity vs Scalability Simplicity Solo dev, personal tool; no multi-user, no distributed deployment Coverage vs Speed Balance TIER 1 = 90%, TIER 3 = 60%; risk-based prioritization Automation vs Control Control Manual dependency updates to prevent breaking changes Security Depth vs Overhead Proportionate .env + .gitignore, not Vault; appropriate for threat model Real-time vs Batch Batch Research workload; REST sufficient, no WebSocket Persistence vs Simplicity Simplicity Filesystem cache; no database overhead Cross-platform vs POSIX POSIX macOS/Linux primary; Windows deferred Stale Data vs Freshness 7-day TTL Research tolerates stale; configurable if needed Fail-fast vs Resilience Resilience Schema validation warns (not fails); partial data preferred Single Process vs Multi-process Single No file locking complexity; document limitation"},{"location":"DECISIONS/#12-document-references","title":"12. Document References","text":"Document Status Key Sections REQUIREMENTS.md Approved Section 4: Functional Requirements (14 MUST, 4 SHOULD); Section 5: NFRs; Appendix A: 50 Endpoints ARCHITECTURE.md Approved Section 3: Component Architecture; Section 6: Architecture Decisions (10 ADRs); Section 9: Quality Attributes SECURITY.md Approved Section 2: Threat Model; Section 4: Security Controls; Section 12: Risk Register (7 risks) TEST_STRATEGY.md Approved Section 3: Risk-Based Tiers; Section 6: Quality Gates; Section 8: Critical Test Cases"},{"location":"DECISIONS/#13-concurrency-limits-reference","title":"13. Concurrency Limits Reference","text":"<p>Quick reference for QoS Semaphore Router configuration.</p> Provider Concurrent Requests Rate Limit Source FMP 3 Plan-dependent REQUIREMENTS.md FR-005 Polygon 10 5 req/min (free tier) REQUIREMENTS.md FR-005 FRED 1 ~120 req/min recommended REQUIREMENTS.md FR-005"},{"location":"DECISIONS/#14-endpoint-summary","title":"14. Endpoint Summary","text":"<p>50 total endpoints across 3 providers.</p> Provider Count Endpoints FMP Ultimate 13 screener, profile, quote, historical_price, earnings_calendar, balance_sheet, income_statement, cash_flow, ratios, growth, key_metrics, insider_trading, institutional_ownership Polygon 4 aggs_daily, trades, options_snapshot, market_snapshot FRED 32 series + 1 base CPIAUCSL, CPILFESL, PCEPI, PCEPILFE, UNRATE, PAYEMS, CES0500000003, ECIWAG, CIVPART, GDPC1, GPDI, PNFI, Y033RC1Q027SBEA, Y006RC1Q027SBEA, EXPGS, IMPGS, NETEXP, HOUST, PERMIT, HSN1F, EXHOSLUSM495S, CSUSHPINSA, DGS10, FEDFUNDS, MORTGAGE30US, AWHMAN, DGORDER, ICSA, IC4WSA, UMCSENT, UMCSENTEXP, USEPUINDXD"},{"location":"DECISIONS/#15-success-criteria","title":"15. Success Criteria","text":"<p>Definition of \"Done\" for v1.0.</p> Criterion Target Measurement All MUST requirements implemented 14/14 Code review Overall test coverage &gt;80% pytest-cov TIER 1 coverage &gt;90% pytest-cov (filtered) CI pipeline passing Green GitHub Actions All critical test cases Pass TC-001 through TC-106 Setup time &lt;5 minutes Timed installation Single API request latency &lt;2s p95 Benchmark No API keys in logs/cache/git 0 found Secret scan Documentation complete All 5 docs Review"},{"location":"DECISIONS/#approval","title":"Approval","text":"Role Name Date Status Author Synthesizer Agent 2026-01-31 Generated Requirements Analyst Agent 2026-01-31 Approved Architecture Architect Agent 2026-01-31 Approved Security Security Agent 2026-01-31 Approved Quality QA Agent 2026-01-31 Approved Owner Solo Developer Pending"},{"location":"DECISIONS/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 Synthesizer Agent Initial synthesis from REQUIREMENTS.md, ARCHITECTURE.md, SECURITY.md, TEST_STRATEGY.md <p>Unified project decisions synthesized from approved documentation. This document serves as the single source of truth for project scope, standards, and trade-offs.</p>"},{"location":"REQUIREMENTS/","title":"Requirements Specification","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: Draft Based on: MoneyFlows Data Loader v2.7.0</p>"},{"location":"REQUIREMENTS/#1-executive-summary","title":"1. Executive Summary","text":"<p>OmniData Nexus Core is a modular, asynchronous Python framework serving as a centralized Data Access Layer for aggregating financial and macroeconomic data from multiple API sources. Built upon the proven MoneyFlows Data Loader architecture, it provides a unified interface for 50 endpoints across FMP Ultimate (13), Polygon (4), and FRED (33) APIs. The framework features intelligent caching, circuit breaker patterns, rate limit management via QoS Semaphore Router, and comprehensive health monitoring.</p>"},{"location":"REQUIREMENTS/#2-problem-statement","title":"2. Problem Statement","text":""},{"location":"REQUIREMENTS/#21-current-situation","title":"2.1 Current Situation","text":"<p>Quantitative finance research requires data from multiple specialized APIs: - FMP Ultimate: Fundamental data (financials, ratios, metrics, insider trading) - Polygon.io: Market data (daily aggregates, trades, options snapshots) - FRED: Macroeconomic indicators (32 key series across inflation, employment, growth)</p> <p>Each API has different authentication, response formats, rate limits, and availability patterns.</p>"},{"location":"REQUIREMENTS/#22-problem","title":"2.2 Problem","text":"<p>There is no unified framework that: - Provides a single interface for heterogeneous financial data sources - Implements production-grade resilience (circuit breaker, exponential backoff) - Manages concurrent requests with provider-specific limits - Caches responses intelligently with atomic writes - Monitors API health and rate limit consumption automatically</p>"},{"location":"REQUIREMENTS/#23-impact","title":"2.3 Impact","text":"<p>Without this framework: - Slow data acquisition (sequential fetching) - Brittle code prone to API-specific failures - Rate limit violations leading to throttling/bans - Inconsistent error handling across sources - Time spent on infrastructure instead of analysis</p>"},{"location":"REQUIREMENTS/#3-stakeholders","title":"3. Stakeholders","text":"Stakeholder Role Primary Need Quantitative Researcher Primary User Fast, reliable access to normalized financial data API Providers (FMP, Polygon, FRED) External Services Compliance with rate limits and ToS Future Contributors Developers Clear extension points for new data sources"},{"location":"REQUIREMENTS/#4-functional-requirements","title":"4. Functional Requirements","text":""},{"location":"REQUIREMENTS/#41-core-mvp-must","title":"4.1 Core (MVP) - MUST","text":"ID Feature Description Acceptance Criteria FR-001 Unified DataLoader Interface Central <code>DataLoader</code> class with <code>get_fmp_data()</code>, <code>get_polygon_data()</code>, <code>get_fred_data()</code> methods - Consistent method signatures- Returns normalized data structures- Supports all 50 endpoints FR-002 FMP Ultimate Integration (13 endpoints) Async client for all FMP endpoints - screener, profile, quote, historical_price- earnings_calendar, balance_sheet, income_statement- ratios, growth, key_metrics, cash_flow- insider_trading, institutional_ownership FR-003 Polygon Integration (4 endpoints) Async client for Polygon API - aggs_daily: <code>/v2/aggs/ticker/{symbol}/range/1/day/{start}/{end}</code>- trades: <code>/v3/trades/{symbol}</code>- options_snapshot: <code>/v3/snapshot/options/{symbol}</code>- market_snapshot: <code>/v2/snapshot/locale/us/markets/stocks/tickers</code> FR-004 FRED Integration (32 series) Async client for FRED API - Inflation: CPIAUCSL, CPILFESL, PCEPI, PCEPILFE- Labor: UNRATE, PAYEMS, CES0500000003, ECIWAG, CIVPART- Growth: GDPC1, GPDI, PNFI + equipment/R&amp;D- Housing: HOUST, PERMIT, HSN1F, CSUSHPINSA- Rates: DGS10, FEDFUNDS, MORTGAGE30US- LEI: AWHMAN, DGORDER, ICSA, IC4WSA- Sentiment: UMCSENT, UMCSENTEXP FR-005 QoS Semaphore Router Provider-specific concurrency limits - FMP: max 3 concurrent requests- Polygon: max 10 concurrent requests- FRED: max 1 concurrent request- Configurable via <code>api.concurrency.*</code> FR-006 Intelligent Caching Multi-strategy cache with atomic writes - FMP: <code>data/fmp_cache/{endpoint}/{date}/{symbol}.json</code>- Polygon: <code>data/polygon_cache/{endpoint}/{hash}.json</code>- TTL: 7 days default- Atomic writes (temp file + rename) FR-007 Circuit Breaker Automatic request halting on high error rate - Opens when error rate &gt; 20%- Half-open state for recovery testing- Closed state for normal operation- Per-provider circuit state FR-008 Exponential Backoff with Jitter Retry strategy for transient failures - Configurable max retries (default: 3)- Exponential delay (1s, 2s, 4s...)- Random jitter to prevent thundering herd- Only retry 5xx and timeouts FR-009 Rate Limit Handling HTTP 429 response management - Provider-specific cooldown times- Parse Retry-After headers- Warning at 80% consumption- Automatic request throttling FR-010 Health Check System API availability monitoring - <code>get_api_health_report()</code> method- Returns: total_requests, errors, error_rate, circuit_breaker state- Per-provider status (UP/DOWN/DEGRADED) FR-011 Schema-based Validation Response structure verification - Validate against expected schemas- Return <code>None</code> + warning on invalid- Log validation failures FR-012 Operating Modes LIVE vs READ-ONLY operation - LIVE: API as primary, write-through cache- READ-ONLY: Cache only, 0 API calls guaranteed FR-013 Configuration Management External config for credentials and settings - Environment variables: <code>FMP_KEY</code>, <code>POLYGON_KEY</code>, <code>FRED_KEY</code>- <code>.env</code> file support- <code>core.config_manager</code> integration FR-014 API Key Sanitization Security in logging - Automatic removal of API keys from logs- No keys in error messages- No keys in cache files"},{"location":"REQUIREMENTS/#42-important-should","title":"4.2 Important - SHOULD","text":"ID Feature Description Acceptance Criteria FR-101 Cache TTL Configuration Per-endpoint cache expiration - Configurable TTL per data type- Override defaults via config- Automatic cache invalidation FR-102 Request Batching Combine similar requests - Batch within time window- Respect API batch limits- Transparent to caller FR-103 Metrics Export Operational telemetry - Request count per source- Response times (p50, p95, p99)- Cache hit/miss rates FR-104 Graceful Degradation Partial success handling - Return available data on partial failure- Log failed sources- Clear error attribution"},{"location":"REQUIREMENTS/#43-nice-to-have-could","title":"4.3 Nice to Have - COULD","text":"ID Feature Description FR-201 Plugin Architecture Dynamic loading of new data source implementations FR-202 Dark Pool Detection Block trade identification (min_block_size: 10000, min_notional configurable) FR-203 Data Export Export to Parquet/HDF5 formats FR-204 CLI Tool Command-line interface for manual queries FR-205 Async Iterator Support Stream large datasets without full memory load"},{"location":"REQUIREMENTS/#44-out-of-scope-wont-this-version","title":"4.4 Out of Scope - WON'T (this version)","text":"Feature Reason GUI Dashboard Focus on programmatic API Database Persistence Caller responsible for storage Data Analysis/Modeling Framework provides data, not analytics Multi-user Authentication Single-user research tool WebSocket Streaming REST endpoints sufficient for research"},{"location":"REQUIREMENTS/#5-non-functional-requirements","title":"5. Non-Functional Requirements","text":"ID Category Requirement Target Measurement NFR-001 Performance Single API request latency &lt;2s p95 Timed fetch operations NFR-002 Performance Parallel fetch speedup &gt;3x for 10 concurrent vs sequential Benchmark comparison NFR-003 Reliability Successful request rate &gt;95% (excluding API outages) Success/total ratio NFR-004 Reliability Retry recovery rate &gt;80% of retryable failures Retry success tracking NFR-005 Reliability Circuit breaker threshold Opens at &gt;20% error rate Error rate monitoring NFR-006 Maintainability Code test coverage &gt;80% line coverage pytest-cov report NFR-007 Security API key protection No keys in code, logs, git, cache Audit + grep verification NFR-008 Scalability Concurrent requests 50+ simultaneous (within QoS limits) Load testing NFR-009 Usability Setup time &lt;5 minutes to first fetch Timed installation NFR-010 Compatibility Python version 3.9+ CI matrix testing NFR-011 Data Integrity Cache atomicity No partial/corrupt cache files Concurrent write tests"},{"location":"REQUIREMENTS/#6-user-stories","title":"6. User Stories","text":""},{"location":"REQUIREMENTS/#us-001-fetch-company-fundamentals","title":"US-001: Fetch Company Fundamentals","text":"<pre><code>AS A quantitative researcher\nI WANT to fetch financial fundamentals for a stock symbol\nSO THAT I can analyze company financial health\n\nACCEPTANCE CRITERIA:\n- [ ] Can call loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n- [ ] Can fetch balance_sheet, income_statement, cash_flow, ratios\n- [ ] Data normalized with consistent field names\n- [ ] Circuit breaker protects against API failures\n- [ ] Response cached for 7 days by default\n</code></pre>"},{"location":"REQUIREMENTS/#us-002-fetch-multi-source-data-in-parallel","title":"US-002: Fetch Multi-Source Data in Parallel","text":"<pre><code>AS A quantitative researcher\nI WANT to fetch FMP + Polygon + FRED data concurrently\nSO THAT I can correlate fundamental, market, and macro data efficiently\n\nACCEPTANCE CRITERIA:\n- [ ] All requests execute in parallel (respecting QoS limits)\n- [ ] Total time \u2248 slowest single request\n- [ ] Partial success handled (return what succeeds, log failures)\n- [ ] Results unified in consistent structure\n</code></pre>"},{"location":"REQUIREMENTS/#us-003-monitor-api-health","title":"US-003: Monitor API Health","text":"<pre><code>AS A quantitative researcher\nI WANT to check API health before running batch jobs\nSO THAT I can avoid wasted time on failing APIs\n\nACCEPTANCE CRITERIA:\n- [ ] Can call loader.get_api_health_report()\n- [ ] Returns per-provider: total_requests, errors, error_rate, circuit_breaker state\n- [ ] Circuit breaker state visible: CLOSED/OPEN/HALF-OPEN\n- [ ] Rate limit consumption visible\n</code></pre>"},{"location":"REQUIREMENTS/#us-004-work-offline-with-cache","title":"US-004: Work Offline with Cache","text":"<pre><code>AS A quantitative researcher\nI WANT to run analysis using only cached data\nSO THAT I can work without API calls (airplane mode, rate limit exhausted)\n\nACCEPTANCE CRITERIA:\n- [ ] Can set operating mode to READ-ONLY\n- [ ] Guarantees 0 API calls in READ-ONLY mode\n- [ ] Returns cached data if available\n- [ ] Clear error if cache miss in READ-ONLY mode\n</code></pre>"},{"location":"REQUIREMENTS/#us-005-handle-rate-limits-gracefully","title":"US-005: Handle Rate Limits Gracefully","text":"<pre><code>AS A quantitative researcher\nI WANT the system to manage rate limits automatically\nSO THAT I don't get blocked by API providers\n\nACCEPTANCE CRITERIA:\n- [ ] QoS Semaphore limits concurrent requests (FMP:3, Polygon:10, FRED:1)\n- [ ] HTTP 429 triggers provider-specific cooldown\n- [ ] Warning logged at 80% rate limit consumption\n- [ ] Request automatically delayed if limit would be exceeded\n</code></pre>"},{"location":"REQUIREMENTS/#us-006-fetch-fred-macroeconomic-series","title":"US-006: Fetch FRED Macroeconomic Series","text":"<pre><code>AS A quantitative researcher\nI WANT to fetch key macroeconomic indicators\nSO THAT I can incorporate macro factors in my models\n\nACCEPTANCE CRITERIA:\n- [ ] Can fetch all 32 FRED series (see Appendix A)\n- [ ] Inflation: CPIAUCSL, CPILFESL, PCEPI, PCEPILFE\n- [ ] Labor: UNRATE, PAYEMS, CIVPART\n- [ ] Rates: DGS10, FEDFUNDS, MORTGAGE30US\n- [ ] Housing: HOUST, CSUSHPINSA\n- [ ] Data normalized to consistent time series format\n</code></pre>"},{"location":"REQUIREMENTS/#us-007-extend-with-new-data-source","title":"US-007: Extend with New Data Source","text":"<pre><code>AS A quantitative researcher\nI WANT to add a new API (e.g., Alpha Vantage) easily\nSO THAT I can expand data coverage as needed\n\nACCEPTANCE CRITERIA:\n- [ ] Clear base class/interface to implement\n- [ ] New source inherits circuit breaker, caching, QoS\n- [ ] Documentation shows extension example\n- [ ] No core code modification required\n</code></pre>"},{"location":"REQUIREMENTS/#7-constraints","title":"7. Constraints","text":""},{"location":"REQUIREMENTS/#71-technical","title":"7.1 Technical","text":"<ul> <li>Python 3.9+ required (asyncio improvements)</li> <li>aiohttp for async HTTP</li> <li>Concurrency limits: FMP (3), Polygon (10), FRED (1)</li> <li>No API keys in version control or logs</li> <li>Atomic cache writes (temp + rename pattern)</li> </ul>"},{"location":"REQUIREMENTS/#72-business","title":"7.2 Business","text":"<ul> <li>API access limited by subscription tier (FMP Ultimate required)</li> <li>Solo developer project</li> <li>Research use only</li> </ul>"},{"location":"REQUIREMENTS/#73-api-specific","title":"7.3 API-Specific","text":"<ul> <li>FMP: Rate limits vary by plan</li> <li>Polygon: 5 req/min (free tier), higher on paid</li> <li>FRED: No official limit, recommend 120 req/min max</li> </ul>"},{"location":"REQUIREMENTS/#8-assumptions","title":"8. Assumptions","text":"ID Assumption Impact if Wrong A-001 Internet connectivity reliable Need offline queue mode A-002 API schemas remain stable Manual normalization updates needed A-003 User has valid API keys for all 3 sources Source unavailable; handle gracefully A-004 Data fits in memory Need streaming/pagination A-005 POSIX environment (macOS/Linux) Windows path adjustments needed"},{"location":"REQUIREMENTS/#9-dependencies","title":"9. Dependencies","text":"ID Dependency Type Version Risk D-001 aiohttp PyPI \u22653.8 Low D-002 asyncio Built-in - Low D-003 core.config_manager Internal - Low D-004 python-dotenv PyPI \u22650.19 Low D-005 FMP API External Service - Medium D-006 Polygon API External Service - Medium D-007 FRED API External Service - Low D-008 pytest / pytest-cov PyPI (dev) - Low"},{"location":"REQUIREMENTS/#10-glossary","title":"10. Glossary","text":"Term Definition DataLoader Central class providing unified access to all data sources Circuit Breaker Pattern that stops requests when error rate exceeds threshold (20%) QoS Semaphore Router Concurrency limiter enforcing per-provider request limits Exponential Backoff Retry strategy with increasing delays (1s, 2s, 4s...) + jitter TTL Time To Live - cache validity duration (default 7 days) Atomic Write File write pattern using temp file + rename to prevent corruption LIVE Mode Operating mode where API is primary source with write-through cache READ-ONLY Mode Operating mode serving only from cache (0 API calls) FMP Financial Modeling Prep - fundamental data provider Polygon Market data provider for equities, options, indices FRED Federal Reserve Economic Data - macroeconomic indicators"},{"location":"REQUIREMENTS/#appendix-a-complete-endpoint-reference","title":"Appendix A: Complete Endpoint Reference","text":""},{"location":"REQUIREMENTS/#a1-fmp-endpoints-13","title":"A.1 FMP Endpoints (13)","text":"Endpoint URL Pattern Description screener <code>/stable/company-screener</code> Stock screener profile <code>/stable/profile</code> Company profile quote <code>/stable/quote</code> Current price historical_price <code>/stable/historical-price-eod/full</code> Historical EOD prices earnings_calendar <code>/stable/earnings-calendar</code> Earnings dates balance_sheet <code>/stable/balance-sheet-statement</code> Balance sheet income_statement <code>/stable/income-statement</code> Income statement cash_flow <code>/stable/cash-flow-statement</code> Cash flow statement ratios <code>/stable/ratios</code> Financial ratios growth <code>/stable/financial-growth</code> Growth metrics key_metrics <code>/stable/key-metrics</code> Key metrics insider_trading <code>/stable/insider-trading/search</code> Insider transactions institutional_ownership <code>/stable/institutional-ownership/latest</code> Institutional holdings"},{"location":"REQUIREMENTS/#a2-polygon-endpoints-4","title":"A.2 Polygon Endpoints (4)","text":"Endpoint URL Pattern Description aggs_daily <code>/v2/aggs/ticker/{symbol}/range/1/day/{start}/{end}</code> Daily OHLCV trades <code>/v3/trades/{symbol}</code> Individual trades options_snapshot <code>/v3/snapshot/options/{symbol}</code> Options snapshot market_snapshot <code>/v2/snapshot/locale/us/markets/stocks/tickers</code> Full market snapshot"},{"location":"REQUIREMENTS/#a3-fred-series-32","title":"A.3 FRED Series (32)","text":"Category Series IDs Frequency Inflation CPIAUCSL, CPILFESL, PCEPI, PCEPILFE Monthly Labor Market UNRATE, PAYEMS, CES0500000003, ECIWAG, CIVPART Monthly/Quarterly Growth GDPC1, GPDI, PNFI, Y033RC1Q027SBEA, Y006RC1Q027SBEA Quarterly Trade EXPGS, IMPGS, NETEXP Quarterly Housing HOUST, PERMIT, HSN1F, EXHOSLUSM495S, CSUSHPINSA Monthly Interest Rates DGS10, FEDFUNDS, MORTGAGE30US Daily/Monthly/Weekly Leading Indicators AWHMAN, DGORDER, ICSA, IC4WSA Monthly/Weekly Sentiment UMCSENT, UMCSENTEXP Monthly Policy USEPUINDXD Monthly <p>Total: 50 endpoints (13 FMP + 4 Polygon + 1 FRED base + 32 FRED series)</p>"},{"location":"REQUIREMENTS/#approval","title":"Approval","text":"Role Name Date Status Author Analyst Agent 2026-01-31 \u2705 Generated Reference MoneyFlows Data Loader v2.7.0 - \u2705 Integrated Owner Solo Developer \u2610 Pending"},{"location":"REQUIREMENTS/#revision-history","title":"Revision History","text":"Version Date Author Changes 0.1 2026-01-31 Analyst Agent Initial draft 0.2 2026-01-31 Analyst Agent Integrated MoneyFlows Data Loader v2.7.0 specs <p>Based on MoneyFlows Data Loader v2.7.0 Technical Documentation</p>"},{"location":"SECURITY/","title":"Security Design","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: Draft Scope: Personal research tool (single-user, local deployment)</p>"},{"location":"SECURITY/#1-security-overview","title":"1. Security Overview","text":""},{"location":"SECURITY/#11-security-objectives","title":"1.1 Security Objectives","text":"Objective Priority Description Confidentiality HIGH Protect API keys (FMP, Polygon, FRED) from unauthorized disclosure Integrity MEDIUM Ensure data accuracy through schema validation and atomic cache writes Availability MEDIUM Maintain service continuity through circuit breaker and rate limit management <p>Key Security Principle: Proportionate security for a personal research tool. This is NOT an enterprise system requiring OAuth, RBAC, or network segmentation. Focus is on API key protection and safe interaction with external services.</p>"},{"location":"SECURITY/#12-security-scope","title":"1.2 Security Scope","text":"<p>In Scope: - API key protection (storage, transmission, logging) - Secure communication with external APIs (FMP, Polygon, FRED) - Cache integrity (atomic writes, no credential leakage) - Log sanitization (no secrets in logs or error messages) - Dependency security (vulnerability scanning)</p> <p>Out of Scope: - User authentication (single-user system) - Multi-tenancy/authorization - GDPR/HIPAA compliance (no PII collected) - Network segmentation (local-only deployment) - Database security (no database) - WebSocket security (REST only)</p>"},{"location":"SECURITY/#2-threat-model","title":"2. Threat Model","text":""},{"location":"SECURITY/#21-assets","title":"2.1 Assets","text":"Asset Sensitivity Location Protection FMP API Key CRITICAL <code>.env</code> file (local filesystem) File permissions (600), gitignored, environment variable Polygon API Key CRITICAL <code>.env</code> file (local filesystem) File permissions (600), gitignored, environment variable FRED API Key CRITICAL <code>.env</code> file (local filesystem) File permissions (600), gitignored, environment variable Cached API responses LOW <code>data/{provider}_cache/</code> (JSON files) Contains only public data, no credentials Source code LOW Git repository Public or private repo, no secrets embedded Log files LOW <code>logs/nexus_core.log</code> API keys sanitized before logging <p>Note: API keys are the ONLY critical assets. Cached financial data is public information with no inherent sensitivity.</p>"},{"location":"SECURITY/#22-threat-actors","title":"2.2 Threat Actors","text":"Actor Motivation Capability Likelihood Realistic for Personal Tool? Opportunistic attacker API key theft for resale Low (automated scanners) Medium Yes - if keys committed to public repo Malicious script/dependency Data exfiltration Medium (supply chain) Low Yes - compromised PyPI package Local malware Credential harvesting Medium (filesystem access) Low Yes - but outside project scope Insider threat N/A N/A N/A No - single-user system Nation-state actor N/A N/A N/A No - not a target profile <p>Realistic Threat Focus: Accidental API key exposure (git commit, log files) and dependency vulnerabilities. NOT nation-state attacks or sophisticated APTs.</p>"},{"location":"SECURITY/#23-attack-surface","title":"2.3 Attack Surface","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     ATTACK SURFACE                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  EXTERNAL                    INTERNAL                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 FMP API     \u2502            \u2502 .env file   \u2502                \u2502\n\u2502  \u2502 HTTPS       \u2502            \u2502 Risk: HIGH  \u2502                \u2502\n\u2502  \u2502 Risk: LOW   \u2502            \u2502 Mitigation: \u2502                \u2502\n\u2502  \u2502             \u2502            \u2502 - .gitignore\u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502 - chmod 600 \u2502                \u2502\n\u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 Polygon API \u2502            \u2502 Dependencies\u2502                \u2502\n\u2502  \u2502 HTTPS       \u2502            \u2502 Risk: MEDIUM\u2502                \u2502\n\u2502  \u2502 Risk: LOW   \u2502            \u2502 Mitigation: \u2502                \u2502\n\u2502  \u2502             \u2502            \u2502 - pip audit \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502 - pinned ver\u2502                \u2502\n\u2502                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502 FRED API    \u2502            \u2502 Log files   \u2502                \u2502\n\u2502  \u2502 HTTPS       \u2502            \u2502 Risk: MEDIUM\u2502                \u2502\n\u2502  \u2502 Risk: LOW   \u2502            \u2502 Mitigation: \u2502                \u2502\n\u2502  \u2502             \u2502            \u2502 - key sanit \u2502                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Entry Points: 1. External APIs (FMP, Polygon, FRED) - HTTPS endpoints (LOW risk - TLS protects in transit) 2. <code>.env</code> file - API key storage (HIGH risk - if committed to git or world-readable) 3. Dependencies - PyPI packages (MEDIUM risk - supply chain attack) 4. Log files - Error messages (MEDIUM risk - if keys logged) 5. Cache files - JSON storage (LOW risk - no credentials stored)</p>"},{"location":"SECURITY/#24-stride-analysis","title":"2.4 STRIDE Analysis","text":"Threat Applies? Attack Vector Mitigation Spoofing NO No authentication mechanism to spoof N/A - single-user, local system Tampering YES Attacker modifies cache files or code File permissions (POSIX), code integrity via git Repudiation NO No audit trail needed for single-user research N/A - no regulatory requirement Info Disclosure YES API keys exposed in git, logs, or cache <code>.gitignore</code>, log sanitization, no keys in cache Denial of Service YES Rate limit exhaustion by runaway script QoS Semaphore Router (FR-005), circuit breaker (FR-007) Elevation NO No privilege levels to elevate N/A - single-user system <p>Security Focus: Information Disclosure (API keys) and Denial of Service (rate limits).</p>"},{"location":"SECURITY/#3-security-architecture","title":"3. Security Architecture","text":""},{"location":"SECURITY/#31-authentication","title":"3.1 Authentication","text":"<p>To External APIs: - Method: API key authentication via query parameters or HTTP headers (provider-specific) - Storage: Environment variables (<code>FMP_KEY</code>, <code>POLYGON_KEY</code>, <code>FRED_KEY</code>) loaded from <code>.env</code> file - Transmission: HTTPS only (TLS 1.2+) - enforced by <code>aiohttp</code> default configuration - Rotation: Manual - user responsible for key rotation per API provider policy</p> <p>No User Authentication: Single-user system running locally - no login mechanism required.</p>"},{"location":"SECURITY/#32-authorization","title":"3.2 Authorization","text":"<p>N/A - No multi-user access, no role-based permissions. The user running the Python process has full access to all features.</p> <p>API Provider Authorization: Governed by API subscription tier (e.g., FMP Ultimate plan) - handled externally by providers.</p>"},{"location":"SECURITY/#33-cryptography","title":"3.3 Cryptography","text":"Purpose Algorithm/Protocol Implementation Notes Data in transit TLS 1.2/1.3 <code>aiohttp</code> default (system SSL library) All API communication encrypted Data at rest None (unencrypted) Filesystem JSON cache Public financial data, no encryption needed API key storage None (plaintext in .env) <code>python-dotenv</code> Appropriate for local single-user; file permissions protect Cache key hashing MD5 <code>hashlib.md5()</code> Speed prioritized over cryptographic security (collision resistance not critical) <p>Rationale for No Encryption at Rest: - Cached data is public financial information (no confidentiality requirement) - API keys in <code>.env</code> protected by filesystem permissions (600) and <code>.gitignore</code> - Full disk encryption (macOS FileVault, Linux LUKS) available at OS level if needed</p>"},{"location":"SECURITY/#34-network-security","title":"3.4 Network Security","text":"<p>Architecture: Local Python process \u2192 Internet \u2192 External APIs</p> <p>Controls: - TLS enforcement: All HTTP clients configured to use HTTPS (no HTTP fallback) - Certificate validation: <code>aiohttp</code> verifies server certificates by default - No listening ports: Framework makes outbound requests only (no server component) - Firewall: Relies on OS firewall for outbound filtering (user-managed)</p> <p>Network Diagram:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Local Python Process\u2502\n\u2502  (OmniData Nexus)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502 Outbound HTTPS only\n           \u2502 (TLS 1.2+)\n           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Internet            \u2502\n\u2502  (Provider APIs)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>No Internal Network: Single process, no service-to-service communication.</p>"},{"location":"SECURITY/#4-security-controls","title":"4. Security Controls","text":""},{"location":"SECURITY/#41-preventive-controls","title":"4.1 Preventive Controls","text":"Control Implementation Priority Component API Key Protection <code>.env</code> file with <code>.gitignore</code>; never hardcoded CRITICAL Config Manager HTTPS Enforcement <code>aiohttp</code> default TLS; reject HTTP URLs HIGH HTTP Client Layer Input Validation Schema validation for API responses (FR-011) MEDIUM Providers Rate Limiting QoS Semaphore Router enforces concurrency limits HIGH QoS Router Atomic Cache Writes Temp file + rename pattern prevents corruption MEDIUM Cache Manager Dependency Pinning <code>requirements.txt</code> with exact versions MEDIUM Build System File Permissions <code>.env</code> should be <code>chmod 600</code> (user-only) HIGH Setup Documentation"},{"location":"SECURITY/#42-detective-controls","title":"4.2 Detective Controls","text":"Control Implementation Alerting API Key Sanitization in Logs Regex-based redaction in log formatter (FR-014) Warning logged if key pattern detected Rate Limit Monitoring Warning at 80% consumption (FR-009) Log entry (no automated alert) Circuit Breaker Telemetry Health report shows error rates (FR-010) Open state triggers log warning Dependency Vulnerability Scan Manual <code>pip audit</code> (developer responsibility) Console output during CI Git Pre-commit Hook Check for API key patterns before commit Reject commit if pattern found <p>Monitoring Gaps (Acceptable for Personal Tool): - No real-time SIEM alerts (log review is manual) - No automated incident response - No intrusion detection system</p>"},{"location":"SECURITY/#43-corrective-controls","title":"4.3 Corrective Controls","text":"Threat Response Automation API Key Leaked to Git Immediate revoke via provider dashboard; rotate keys Manual (user detects via GitHub scanning or manual review) Rate Limit Exhausted Circuit breaker halts requests; wait for cooldown Automatic (circuit breaker pattern) Dependency Vulnerability Update package; test compatibility; redeploy Manual (user runs <code>pip install -U</code>) Cache Corruption Delete corrupted file; refetch from API Automatic (validation fails, cache miss triggers refetch)"},{"location":"SECURITY/#5-data-protection","title":"5. Data Protection","text":""},{"location":"SECURITY/#51-data-classification","title":"5.1 Data Classification","text":"Data Type Classification Rationale Handling API Keys CRITICAL - Secret Unauthorized use leads to quota exhaustion or financial cost <code>.env</code> file, chmod 600, never logged Cached API Responses PUBLIC Financial data available to any subscriber Filesystem JSON, no encryption Log Files INTERNAL May contain debugging info but no secrets Sanitized output, local storage only Configuration (non-secrets) PUBLIC TTLs, concurrency limits, endpoints Version controlled in <code>config.py</code> Source Code PUBLIC Framework logic (can be open-sourced) Git repository (public or private)"},{"location":"SECURITY/#52-secrets-management","title":"5.2 Secrets Management","text":"Secret Type Storage Access Rotation Backup FMP API Key <code>.env</code> file (gitignored) Environment variable <code>FMP_KEY</code> Manual via FMP dashboard User responsibility (password manager) Polygon API Key <code>.env</code> file (gitignored) Environment variable <code>POLYGON_KEY</code> Manual via Polygon dashboard User responsibility (password manager) FRED API Key <code>.env</code> file (gitignored) Environment variable <code>FRED_KEY</code> Manual via FRED website User responsibility (password manager) <p>Secret Lifecycle:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Acquisition: User obtains keys from provider dashboards  \u2502\n\u2502                                                             \u2502\n\u2502 2. Storage: User creates .env file (chmod 600)             \u2502\n\u2502    Example:                                                 \u2502\n\u2502    FMP_KEY=abc123...                                        \u2502\n\u2502    POLYGON_KEY=xyz789...                                    \u2502\n\u2502    FRED_KEY=def456...                                       \u2502\n\u2502                                                             \u2502\n\u2502 3. Loading: python-dotenv reads .env \u2192 os.environ          \u2502\n\u2502                                                             \u2502\n\u2502 4. Usage: Config Manager retrieves via os.getenv()         \u2502\n\u2502                                                             \u2502\n\u2502 5. Transmission: Included in HTTPS requests (TLS encrypted)\u2502\n\u2502                                                             \u2502\n\u2502 6. Rotation: User manually updates .env after rotating     \u2502\n\u2502             keys in provider dashboards                    \u2502\n\u2502                                                             \u2502\n\u2502 7. Revocation: Delete from .env, revoke in provider portal \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>No Vault/KMS: For a single-user local tool, <code>.env</code> + <code>.gitignore</code> + file permissions is proportionate. Enterprise secret management (HashiCorp Vault, AWS Secrets Manager) would be overkill.</p>"},{"location":"SECURITY/#53-data-retention","title":"5.3 Data Retention","text":"Data Type Retention Location Deletion Cached responses 7 days (configurable TTL) <code>data/{provider}_cache/</code> Manual cleanup or TTL-based script Log files 10MB max (5 rotating backups) <code>logs/nexus_core.log</code> Automatic rotation via logging handler API keys Indefinite (until user rotates) <code>.env</code> file Manual deletion"},{"location":"SECURITY/#6-compliance","title":"6. Compliance","text":""},{"location":"SECURITY/#61-api-provider-terms-of-service","title":"6.1 API Provider Terms of Service","text":"<p>This is the ONLY compliance requirement for a personal research tool.</p> Provider Key ToS Requirements Compliance Mechanism FMP Ultimate - Respect rate limits- No redistribution of data- Proper attribution - QoS Semaphore (3 concurrent)- Cache for personal use only- Attribution in documentation Polygon.io - Respect rate limits (5 req/min free tier)- No resale of data- Acceptable use policy - QoS Semaphore (10 concurrent)- Personal research only- No commercial distribution FRED - Public domain data- Attribution required- Recommended 120 req/min limit - QoS Semaphore (1 concurrent)- Citation: \"Source: Federal Reserve Economic Data\"- Conservative rate limiting <p>Verification: User is responsible for reviewing and adhering to provider ToS. Framework provides technical controls (rate limiting) to prevent violations.</p>"},{"location":"SECURITY/#62-standards-not-applicable","title":"6.2 Standards NOT Applicable","text":"Standard Reason Not Applicable GDPR No personal data collected (financial data is public) HIPAA No health information PCI-DSS No payment processing SOC 2 No service offering (internal research tool) ISO 27001 Personal tool, not enterprise"},{"location":"SECURITY/#7-vulnerability-management","title":"7. Vulnerability Management","text":""},{"location":"SECURITY/#71-dependency-security","title":"7.1 Dependency Security","text":"<p>Process:</p> <pre><code># 1. Regular scanning (recommended: weekly)\npip audit\n\n# 2. Review output for HIGH/CRITICAL vulnerabilities\n# Example output:\n# Found 2 known vulnerabilities in 1 package\n# aiohttp 3.8.0 -&gt; CVE-2023-XXXXX (HIGH)\n\n# 3. Update affected packages\npip install --upgrade aiohttp\n\n# 4. Test compatibility\npytest tests/\n\n# 5. Update requirements.txt\npip freeze &gt; requirements.txt\n</code></pre> <p>Vulnerability Severity Response:</p> Severity Response Time Action CRITICAL Within 24 hours Immediate update and testing HIGH Within 1 week Scheduled update in next maintenance window MEDIUM Within 1 month Evaluate during regular dependency review LOW Next major release Document and defer unless easy fix"},{"location":"SECURITY/#72-code-security","title":"7.2 Code Security","text":"<p>Practices:</p> Practice Implementation Frequency Static Analysis Linters (pylint, flake8, mypy) On every commit (pre-commit hook) Secret Scanning <code>git grep -E \"(FMP_KEY\\|POLYGON_KEY\\|FRED_KEY)\" -- ':(exclude).env'</code> Pre-commit hook Dependency Audit <code>pip audit</code> Weekly (manual) Code Review Self-review before commit (solo developer) Every change <p>Pre-commit Hook Example:</p> <pre><code>#!/bin/bash\n# .git/hooks/pre-commit\n\n# Check for API keys in committed files (excluding .env)\nif git grep -E \"(FMP_KEY|POLYGON_KEY|FRED_KEY|apikey|api_key)\" HEAD -- ':(exclude).env' ':(exclude)*.md'; then\n    echo \"ERROR: Potential API key found in commit\"\n    exit 1\nfi\n\n# Run secret scanning tool (optional)\n# pip install detect-secrets\n# detect-secrets scan --baseline .secrets.baseline\n</code></pre>"},{"location":"SECURITY/#73-api-security","title":"7.3 API Security","text":"<p>Security Measures:</p> Concern Mitigation Verification Key Rotation Manual rotation every 90 days (best practice) Calendar reminder Key Leakage Detection GitHub secret scanning (if public repo) Automatic (GitHub feature) Rate Limit Abuse QoS Semaphore enforces provider limits Health monitor tracks usage Man-in-the-Middle TLS certificate validation <code>aiohttp</code> default behavior"},{"location":"SECURITY/#8-security-exceptions","title":"8. Security Exceptions","text":"Exception Rationale Risk Acceptance Mitigation API Keys in Plaintext (.env) Single-user local system; encrypted secret store overkill LOW - file permissions (600) + <code>.gitignore</code> sufficient Recommend OS-level full disk encryption No Encryption at Rest (Cache) Public financial data; no confidentiality requirement NEGLIGIBLE - data is already public None needed No Authentication Single-user system; no network exposure NONE - appropriate for use case N/A Manual Dependency Updates Automated updates risk breaking changes LOW - research tool can tolerate brief downtime Weekly <code>pip audit</code> schedule <p>Approval: Solo developer accepts above exceptions as appropriate for personal research tool.</p>"},{"location":"SECURITY/#9-security-testing","title":"9. Security Testing","text":""},{"location":"SECURITY/#91-test-plan","title":"9.1 Test Plan","text":"Test Type Scope Frequency Tooling Secret Scanning Git history, code, logs Every commit (pre-commit hook) <code>git grep</code>, <code>detect-secrets</code> Dependency Audit PyPI packages Weekly <code>pip audit</code> TLS Verification HTTPS connections Every release Manual inspection (<code>openssl s_client</code>) Log Sanitization API key redaction Unit tests pytest (test log output) Cache Integrity Atomic write validation Unit tests pytest (concurrent writes)"},{"location":"SECURITY/#92-security-test-cases","title":"9.2 Security Test Cases","text":"<p>Test 1: API Key Sanitization in Logs</p> <pre><code>def test_api_key_not_logged():\n    \"\"\"Verify API keys are redacted in log output.\"\"\"\n    with patch('sys.stdout', new=StringIO()) as fake_out:\n        logger.error(f\"Request failed: {os.getenv('FMP_KEY')}\")\n        log_output = fake_out.getvalue()\n        assert \"FMP_KEY\" not in log_output\n        assert \"***REDACTED***\" in log_output\n</code></pre> <p>Test 2: .env File in .gitignore</p> <pre><code>def test_env_file_gitignored():\n    \"\"\"Verify .env is in .gitignore.\"\"\"\n    with open('.gitignore', 'r') as f:\n        gitignore_content = f.read()\n        assert '.env' in gitignore_content\n</code></pre> <p>Test 3: HTTPS Enforcement</p> <pre><code>def test_https_only():\n    \"\"\"Verify HTTP URLs are rejected.\"\"\"\n    with pytest.raises(ValueError):\n        DataLoader.validate_url(\"http://api.example.com\")\n</code></pre>"},{"location":"SECURITY/#93-penetration-testing","title":"9.3 Penetration Testing","text":"<p>NOT REQUIRED for a personal research tool. No external attack surface beyond API providers (which are out of scope).</p> <p>If desired for learning purposes: - Scope: Local filesystem security (file permissions, git history) - Tools: <code>trufflehog</code> (secret scanning), <code>bandit</code> (Python security linting) - Frequency: One-time during initial development</p>"},{"location":"SECURITY/#10-incident-response","title":"10. Incident Response","text":""},{"location":"SECURITY/#101-api-key-compromise","title":"10.1 API Key Compromise","text":"<p>Detection: - GitHub alerts (if public repo) - Unusual API usage patterns (manual review of provider dashboards) - Third-party notification</p> <p>Response (1-hour playbook):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 INCIDENT: API Key Compromised                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 T+0:00  DETECT   Receive alert or notice unusual usage     \u2502\n\u2502                                                             \u2502\n\u2502 T+0:05  CONTAIN  Revoke compromised key in provider portal \u2502\n\u2502                  FMP: https://site.financialmodelingprep.com\u2502\n\u2502                  Polygon: https://polygon.io/dashboard/keys \u2502\n\u2502                  FRED: https://fred.stlouisfed.org/api      \u2502\n\u2502                                                             \u2502\n\u2502 T+0:10  ROTATE   Generate new API key                      \u2502\n\u2502                                                             \u2502\n\u2502 T+0:15  UPDATE   Update .env with new key                  \u2502\n\u2502                                                             \u2502\n\u2502 T+0:20  TEST     Verify new key works:                     \u2502\n\u2502                  python -c \"from src import DataLoader; ...\" \u2502\n\u2502                                                             \u2502\n\u2502 T+0:30  AUDIT    Check git history for leaked key:        \u2502\n\u2502                  git log -p -S \"OLD_KEY_VALUE\"             \u2502\n\u2502                                                             \u2502\n\u2502 T+0:45  CLEAN    If key in git history:                    \u2502\n\u2502                  - git filter-branch (nuclear option)      \u2502\n\u2502                  - OR accept history contamination         \u2502\n\u2502                  - Force push to remote (if private repo)  \u2502\n\u2502                                                             \u2502\n\u2502 T+1:00  LEARN    Document how leak occurred; update process\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Cost Impact: FMP/Polygon keys may incur charges if abused. FRED is free (no financial impact).</p>"},{"location":"SECURITY/#102-dependency-vulnerability","title":"10.2 Dependency Vulnerability","text":"<p>Response (24-hour playbook):</p> <ol> <li>Assess: Check CVE severity and exploitability</li> <li>Test: Determine if vulnerability affects this project</li> <li>Update: Upgrade to patched version (<code>pip install --upgrade</code>)</li> <li>Verify: Run test suite (<code>pytest</code>)</li> <li>Deploy: Update <code>requirements.txt</code> and document change</li> </ol>"},{"location":"SECURITY/#11-security-awareness","title":"11. Security Awareness","text":""},{"location":"SECURITY/#111-secure-development-practices","title":"11.1 Secure Development Practices","text":"<p>For Solo Developer:</p> Practice Description Frequency Never commit .env Always verify <code>.env</code> in <code>.gitignore</code> before <code>git add</code> Every commit Review logs before sharing Ensure no API keys in logs before posting to forums Before sharing Use HTTPS for examples Never hardcode HTTP URLs in documentation During writing Rotate keys periodically Change API keys every 90 days (best practice) Quarterly Review dependencies Run <code>pip audit</code> before major releases Weekly"},{"location":"SECURITY/#112-secure-configuration-checklist","title":"11.2 Secure Configuration Checklist","text":"<p>Initial Setup:</p> <pre><code># 1. Create .env file with restricted permissions\ntouch .env\nchmod 600 .env\n\n# 2. Verify .env is gitignored\ngrep -q \"^\\.env$\" .gitignore || echo \".env\" &gt;&gt; .gitignore\n\n# 3. Add API keys to .env (NOT to version control)\necho \"FMP_KEY=your_key_here\" &gt;&gt; .env\necho \"POLYGON_KEY=your_key_here\" &gt;&gt; .env\necho \"FRED_KEY=your_key_here\" &gt;&gt; .env\n\n# 4. Verify keys load correctly\npython -c \"import os; from dotenv import load_dotenv; load_dotenv(); print('FMP:', bool(os.getenv('FMP_KEY')))\"\n\n# 5. Install pre-commit hook for secret scanning\ncp scripts/pre-commit.sh .git/hooks/pre-commit\nchmod +x .git/hooks/pre-commit\n</code></pre>"},{"location":"SECURITY/#12-risk-register","title":"12. Risk Register","text":"ID Risk Likelihood Impact Risk Level Treatment SR-001 API key committed to git LOW (with .gitignore) CRITICAL MEDIUM <code>.gitignore</code> + pre-commit hook SR-002 API key in log files LOW (with sanitization) HIGH LOW Regex-based log sanitization (FR-014) SR-003 Dependency vulnerability MEDIUM (supply chain) MEDIUM MEDIUM Weekly <code>pip audit</code> + pinned versions SR-004 Rate limit abuse (cost) LOW (with QoS controls) MEDIUM LOW QoS Semaphore Router (FR-005) SR-005 Cache file tampering LOW (local system) LOW NEGLIGIBLE POSIX file permissions SR-006 MITM attack on APIs LOW (TLS enforced) MEDIUM LOW TLS 1.2+ via <code>aiohttp</code> SR-007 .env file world-readable MEDIUM (user error) CRITICAL MEDIUM Document <code>chmod 600</code> in setup <p>Overall Risk Posture: LOW - Appropriate for a personal research tool with no external users or sensitive data (beyond API keys).</p>"},{"location":"SECURITY/#13-security-metrics","title":"13. Security Metrics","text":"Metric Target Measurement Frequency API keys in git history 0 <code>git log -p -S \"FMP_KEY\\|POLYGON_KEY\\|FRED_KEY\"</code> Monthly API keys in logs 0 <code>grep -r \"FMP_KEY\\|POLYGON_KEY\\|FRED_KEY\" logs/</code> Weekly Critical vulnerabilities 0 <code>pip audit --severity HIGH</code> Weekly .env file permissions 600 (user-only) <code>ls -la .env</code> Setup verification HTTPS enforcement 100% Code review (all HTTP URLs rejected) Per commit"},{"location":"SECURITY/#14-what-this-security-design-doesnt-cover-and-why-thats-ok","title":"14. What This Security Design Doesn't Cover (And Why That's OK)","text":"Enterprise Control Why Skipped Alternative HashiCorp Vault Overkill for single-user local tool <code>.env</code> file with OS permissions OAuth 2.0 No user authentication needed N/A (single user) RBAC No multi-user access N/A (single user) WAF No web application N/A (CLI framework) SIEM No security operations team Manual log review DLP No sensitive data (public financials) N/A (data is public) Zero Trust No network perimeter OS firewall sufficient Automated Patching Breaking changes risk research continuity Manual updates with testing <p>Design Philosophy: Security controls should be proportionate to the threat model and risk tolerance. Personal research tools do not require enterprise-grade security infrastructure.</p>"},{"location":"SECURITY/#15-security-review-schedule","title":"15. Security Review Schedule","text":"Activity Frequency Owner Next Review Dependency audit (<code>pip audit</code>) Weekly Developer [Ongoing] API key rotation 90 days Developer [90 days from setup] .gitignore verification Per commit Pre-commit hook [Automated] Log sanitization testing Per release Unit tests [Automated] Security design review Annually Developer 2027-01-31"},{"location":"SECURITY/#approval","title":"Approval","text":"Role Name Date Status Author Security Agent 2026-01-31 \u2705 Generated Security Review Solo Developer \u2610 Pending Review Risk Acceptance Solo Developer \u2610 Pending Approval"},{"location":"SECURITY/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 Security Agent Initial security design for OmniData Nexus Core <p>Security Contact: For vulnerability reports, contact the project maintainer via GitHub Issues.</p> <p>Disclosure Policy: Personal tool - no formal vulnerability disclosure program. Responsible disclosure appreciated.</p> <p>Security design scaled appropriately for a personal research tool. Not all enterprise controls apply.</p>"},{"location":"TEST_STRATEGY/","title":"Test Strategy","text":"<p>Project: OmniData Nexus Core Version: 1.0 Date: 2026-01-31 Status: Draft Phase: DEVELOPMENT (not yet live)</p>"},{"location":"TEST_STRATEGY/#1-overview","title":"1. Overview","text":""},{"location":"TEST_STRATEGY/#11-scope","title":"1.1 Scope","text":"<p>In Scope: - Unit tests for all core components (DataLoader, QoS Router, Circuit Breaker, Providers, Cache Manager) - Integration tests for provider API interactions (mocked) - End-to-end tests for critical user workflows - Security tests for API key sanitization and HTTPS enforcement - Resilience tests for circuit breaker, retry logic, and rate limiting</p> <p>Out of Scope: - Real API calls in CI/CD (all external API interactions will be mocked) - Performance/load testing beyond basic concurrency validation (personal tool, not production service) - UI testing (no graphical interface) - Multi-user/concurrent process testing (single-user system) - Network security penetration testing (local-only deployment)</p>"},{"location":"TEST_STRATEGY/#12-quality-objectives","title":"1.2 Quality Objectives","text":"Objective Target Measurement Overall line coverage &gt;80% pytest-cov TIER 1 coverage &gt;90% pytest-cov (filtered by component) TIER 2 coverage &gt;80% pytest-cov (filtered by component) Test pass rate 100% CI pipeline Test suite execution time &lt;2 minutes CI pipeline Security test coverage 100% critical cases Manual verification"},{"location":"TEST_STRATEGY/#2-test-pyramid","title":"2. Test Pyramid","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                   \u2571   E2E    \u2572        \u2190 5%: Critical user workflows\n                  \u2571   (~10)    \u2572          (cache miss\u2192API\u2192cache write)\n                 \u2571\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2572\n                \u2571   Integration   \u2572    \u2190 20%: Provider mocking, retries\n               \u2571      (~40)       \u2572      (aioresponses for HTTP mocking)\n              \u2571\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2572\n             \u2571        Unit           \u2572  \u2190 75%: Isolated component logic\n            \u2571       (~150)            \u2572    (pure functions, state machines)\n           \u2571\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2572\n</code></pre> <p>Rationale for Pyramid: - Unit (75%): Fast feedback loop; isolates bugs to specific components; enables TDD workflow - Integration (20%): Validates provider-specific behavior (HTTP mocking); verifies retry/backoff logic - E2E (5%): Ensures critical paths work end-to-end; validates component orchestration</p> <p>Scale-Appropriate for Solo Developer: - Target ~200 total tests (not thousands) - Focus on high-risk areas (Circuit Breaker, Cache integrity, API key sanitization) - Accept manual testing for low-risk utilities</p>"},{"location":"TEST_STRATEGY/#3-risk-based-tiers","title":"3. Risk-Based Tiers","text":"Tier Components Coverage Target Rationale TIER 1 Circuit Breaker, QoS Semaphore Router, API Key Sanitization, Atomic Cache Writes 90%+ High risk: System resilience, security, data integrity TIER 2 DataLoader, Providers (FMP/Polygon/FRED), Retry Handler, Health Monitor 80% Medium risk: Core business logic, user-facing API TIER 3 Config Manager, Cache Manager (non-atomic methods), Logging utilities 60% Low risk: Wrappers, simple utilities TIER 4 Example scripts, one-off tools Manual testing OK Minimal risk: Not part of core framework"},{"location":"TEST_STRATEGY/#component-classification","title":"Component Classification","text":"Component Tier Rationale Priority Tests Circuit Breaker Manager 1 Prevents cascading failures; critical resilience pattern - State transitions (CLOSED\u2192OPEN\u2192HALF-OPEN)- Error rate threshold (20%)- Recovery logic QoS Semaphore Router 1 Prevents rate limit violations; quota management - Concurrency limits (FMP:3, Polygon:10, FRED:1)- Acquire/release semantics- Timeout handling API Key Sanitization 1 Security: Prevents credential leakage - Keys not in logs (regex test)- Keys not in error messages- Keys not in cache files Cache Atomic Writes 1 Data integrity: Prevents corruption - Concurrent writes (no partial JSON)- Temp file + rename pattern- Rollback on error DataLoader 2 Unified interface; orchestrates all components - Mode switching (LIVE/READ-ONLY)- Provider routing- Health report aggregation FMP Provider 2 13 endpoints; most complex provider - Endpoint coverage (screener, profile, financials)- Normalization logic- Cache key generation Polygon Provider 2 4 endpoints; options complexity - aggs_daily, trades, options_snapshot- Hash-based cache keys- Response validation FRED Provider 2 32 series; time series data - Series fetch (CPIAUCSL, UNRATE, DGS10)- Date range handling- Data normalization Retry Handler 2 Exponential backoff + jitter - Retry only 5xx/timeout- Max retries (3)- Jitter randomization Health Monitor 2 Metrics tracking - Request/error counters- Error rate calculation- Per-provider stats Config Manager 3 Environment variable loading - .env file parsing- Default values- Validation Cache Manager (read) 3 Simple file I/O - get_cached() logic- TTL expiration check- Missing file handling HTTP Client Layer 3 Thin wrapper over aiohttp - Timeout configuration- Connection pooling- TLS enforcement"},{"location":"TEST_STRATEGY/#4-test-types","title":"4. Test Types","text":""},{"location":"TEST_STRATEGY/#41-unit-tests","title":"4.1 Unit Tests","text":"Target Framework Mocking Approach Example Circuit Breaker state machine pytest Mock time.time() for threshold testing <code>test_circuit_breaker_opens_at_20_percent_error_rate()</code> QoS Semaphore limits pytest-asyncio No mocking (test semaphore directly) <code>test_qos_router_enforces_fmp_limit_3()</code> Provider normalization pytest Mock HTTP responses (aioresponses) <code>test_fmp_profile_normalizes_response()</code> Cache key generation pytest No mocking (pure function) <code>test_cache_key_md5_hash_consistency()</code> Retry backoff calculation pytest Mock random.uniform() for jitter <code>test_exponential_backoff_with_jitter()</code> <p>Isolation Strategy: - Mock external dependencies (<code>aiohttp.ClientSession</code>, filesystem, time) - Use dependency injection where possible (pass session as parameter) - Test pure functions first (cache_key generation, normalization logic)</p>"},{"location":"TEST_STRATEGY/#42-integration-tests","title":"4.2 Integration Tests","text":"Integration Point Approach Environment Example FMP API (mocked) aioresponses library Mock HTTP server <code>test_fmp_provider_handles_429_rate_limit()</code> Polygon API (mocked) aioresponses library Mock HTTP server <code>test_polygon_retry_on_503_server_error()</code> FRED API (mocked) aioresponses library Mock HTTP server <code>test_fred_circuit_breaker_opens_after_failures()</code> Filesystem cache tempfile.TemporaryDirectory Isolated temp dir <code>test_cache_atomic_write_concurrent_safety()</code> Health monitor integration pytest-asyncio In-memory counters <code>test_health_report_aggregates_all_providers()</code> <p>Mocking Rationale: - No real API calls in CI (cost, rate limits, reliability) - Faster test execution (&lt;2 min target) - Deterministic test results (no flaky tests from network issues)</p>"},{"location":"TEST_STRATEGY/#43-e2e-tests","title":"4.3 E2E Tests","text":"Scenario Priority Automation Components Involved Expected Outcome Happy path: Cache miss \u2192 API \u2192 Cache hit HIGH Yes DataLoader, Provider, Cache, HTTP Client - 1st call: API fetch + cache write- 2nd call: Cache hit (no API) Circuit breaker opens after failures HIGH Yes Circuit Breaker, Provider, Health Monitor - 3 failures \u2192 circuit OPEN- Subsequent calls fail immediately READ-ONLY mode (no API calls) HIGH Yes DataLoader, Cache - Cache hit: success- Cache miss: error (no API call) QoS limits concurrent requests MEDIUM Yes QoS Router, Provider - FMP: max 3 concurrent (10 attempted)- Verify semaphore enforcement Rate limit handling (HTTP 429) MEDIUM Yes Retry Handler, Provider - Mock 429 response- Parse Retry-After header- Exponential backoff Parallel multi-provider fetch MEDIUM Yes DataLoader, All Providers - Fetch FMP + Polygon + FRED concurrently- Verify total time \u2248 slowest request Graceful degradation (1 provider down) LOW Manual DataLoader, Circuit Breaker - FMP down (circuit OPEN)- Polygon/FRED still work Cache TTL expiration LOW Manual Cache Manager - Cache entry older than TTL- Refetch from API <p>E2E Test Environment: - Mocked HTTP responses (aioresponses) - Temporary filesystem cache - Isolated asyncio event loop per test</p>"},{"location":"TEST_STRATEGY/#5-test-environment","title":"5. Test Environment","text":"Environment Purpose API Behavior Data Source Config Local Development Manual testing, debugging Mocked (aioresponses) OR real API (opt-in via .env.local) Test fixtures <code>.env.test</code> (mock keys) CI/CD (GitHub Actions) Automated testing Mocked only (no real API calls) Test fixtures Environment variables (mock keys) Manual Testing Exploratory testing, one-off scenarios Real API calls (user's keys) Real API responses <code>.env</code> (real keys) <p>Test Data Strategy:</p> Data Type Source Example FMP responses JSON fixtures in <code>tests/fixtures/fmp/</code> <code>profile_AAPL.json</code>, <code>balance_sheet_MSFT.json</code> Polygon responses JSON fixtures in <code>tests/fixtures/polygon/</code> <code>aggs_daily_SPY.json</code>, <code>options_snapshot_AAPL.json</code> FRED responses JSON fixtures in <code>tests/fixtures/fred/</code> <code>CPIAUCSL.json</code>, <code>UNRATE.json</code> Error responses Inline JSON in tests <code>{\"error\": \"Invalid API key\"}</code> <p>Fixture Management: - Record real API responses once (via manual script) - Store as JSON files in version control - Update fixtures when API schemas change</p>"},{"location":"TEST_STRATEGY/#6-quality-gates","title":"6. Quality Gates","text":""},{"location":"TEST_STRATEGY/#cicd-pipeline","title":"CI/CD Pipeline","text":"<pre><code>[Commit] \u2192 [Lint] \u2192 [Type Check] \u2192 [Unit] \u2192 [Integration] \u2192 [E2E] \u2192 [Coverage] \u2192 [Security] \u2192 [Pass]\n              \u2502          \u2502            \u2502           \u2502             \u2502          \u2502            \u2502\n              \u25bc          \u25bc            \u25bc           \u25bc             \u25bc          \u25bc            \u25bc\n          ruff/flake8   mypy      pytest -m    pytest -m    pytest -m   pytest-cov    Secret scan\n          0 errors    0 errors     unit      integration     e2e       &gt;80% overall     0 leaks\n                                 100% pass    100% pass     100% pass   &gt;90% TIER 1\n</code></pre>"},{"location":"TEST_STRATEGY/#gate-definitions","title":"Gate Definitions","text":"Gate Tool Threshold Blocking? Rationale Code Style (Lint) ruff / flake8 0 errors YES Enforces consistency; prevents common bugs Type Checking mypy 0 type errors YES Catches type mismatches before runtime Unit Tests pytest -m unit 100% pass YES Core logic validation; fast feedback Integration Tests pytest -m integration 100% pass YES Provider interactions; resilience patterns E2E Tests pytest -m e2e 100% pass YES Critical user workflows Overall Coverage pytest-cov \u226580% YES Per NFR-006 requirement TIER 1 Coverage pytest-cov (filtered) \u226590% YES High-risk components TIER 2 Coverage pytest-cov (filtered) \u226580% NO (warning) Aspirational, not blocking Secret Scanning git grep / detect-secrets 0 API keys found YES Security: Prevent credential leakage <p>Coverage Exclusions: - <code>__init__.py</code> files (no logic) - Debug/print statements (tagged with <code># pragma: no cover</code>) - Defensive error handling for impossible states - Deprecated code (marked for removal)</p>"},{"location":"TEST_STRATEGY/#7-automation-scope","title":"7. Automation Scope","text":"Test Category Automate? Rationale CI Execution Time Unit tests YES Fast (&lt;30s), deterministic, high value ~20s Integration tests (mocked) YES Medium speed (~1min), validates resilience ~60s E2E tests (critical paths) YES Slower (~30s), but essential workflows ~30s E2E edge cases PARTIAL Diminishing returns; manual exploratory testing N/A Security tests (key sanitization) YES Critical security validation ~5s Dependency audit YES Weekly scheduled run (not per commit) ~10s Real API smoke tests NO Cost, rate limits; manual validation on release N/A Performance benchmarks NO Personal tool; manual ad-hoc benchmarking N/A Exploratory testing NO Human judgment for edge cases N/A <p>Total CI Time Budget: &lt;2 minutes (enables fast iteration)</p>"},{"location":"TEST_STRATEGY/#8-critical-test-cases","title":"8. Critical Test Cases","text":""},{"location":"TEST_STRATEGY/#81-resilience-tests-tier-1","title":"8.1 Resilience Tests (TIER 1)","text":"ID Scenario Type Priority Acceptance Criteria TC-001 Circuit breaker opens at &gt;20% error rate Unit MUST - Track 10 requests, 3 errors (30%)- Circuit state = OPEN- Subsequent calls raise CircuitOpenError TC-002 Circuit breaker recovery (HALF-OPEN \u2192 CLOSED) Integration MUST - Circuit OPEN \u2192 wait timeout \u2192 HALF-OPEN- Single test request succeeds \u2192 CLOSED- Normal operation resumes TC-003 Circuit breaker stays OPEN on recovery failure Integration MUST - Circuit HALF-OPEN \u2192 test request fails \u2192 OPEN- Reset timeout counter TC-004 QoS router enforces FMP concurrency (max 3) Unit MUST - Launch 10 concurrent FMP requests- Only 3 execute simultaneously- 7 wait in queue TC-005 QoS router enforces Polygon concurrency (max 10) Unit MUST - Launch 20 concurrent Polygon requests- Only 10 execute simultaneously TC-006 QoS router enforces FRED concurrency (max 1) Unit MUST - Launch 5 concurrent FRED requests- Serialized execution (1 at a time) TC-007 Exponential backoff with jitter Unit MUST - 1st retry: ~1s \u00b1 jitter- 2nd retry: ~2s \u00b1 jitter- 3rd retry: ~4s \u00b1 jitter- Max 3 retries TC-008 Retry only on 5xx/timeout (not 4xx) Integration MUST - 400 Bad Request: no retry- 429 Rate Limit: cooldown (not retry)- 503 Service Unavailable: retry up to 3x"},{"location":"TEST_STRATEGY/#82-security-tests-tier-1","title":"8.2 Security Tests (TIER 1)","text":"ID Scenario Type Priority Acceptance Criteria TC-101 API keys not logged in error messages Unit MUST - Trigger error with <code>FMP_KEY</code> in URL- Log output contains <code>***REDACTED***</code>- Regex match fails for actual key TC-102 API keys not logged in success messages Unit MUST - Successful API call- Log output contains endpoint/symbol- No API key in log TC-103 API keys not in cache files Integration MUST - Fetch data with API key in URL- Inspect cached JSON file- Verify no key present TC-104 .env file in .gitignore Unit MUST - Parse .gitignore file- Assert <code>.env</code> entry exists TC-105 HTTPS enforcement (reject HTTP) Unit MUST - Attempt HTTP URL: <code>http://api.example.com</code>- Raises ValueError TC-106 TLS certificate validation enabled Integration SHOULD - Mock invalid certificate- Verify aiohttp raises SSLError"},{"location":"TEST_STRATEGY/#83-cache-integrity-tests-tier-1","title":"8.3 Cache Integrity Tests (TIER 1)","text":"ID Scenario Type Priority Acceptance Criteria TC-201 Atomic write (temp + rename pattern) Unit MUST - Call cache.set()- Verify temp file created first- Verify rename to final path- No partial file remains TC-202 Concurrent writes (no corruption) Integration MUST - 10 threads write to same cache key- All writes succeed- Final file is valid JSON (not partial) TC-203 Rollback on write error Unit MUST - Mock filesystem error during write- Verify temp file deleted- Original cache (if any) unchanged TC-204 Cache miss returns None Unit MUST - Request non-existent cache key- Returns None (not error) TC-205 Cache TTL expiration Integration SHOULD - Cache entry older than TTL- get_cached() returns None- Triggers API refetch"},{"location":"TEST_STRATEGY/#84-provider-tests-tier-2","title":"8.4 Provider Tests (TIER 2)","text":"ID Scenario Type Priority Acceptance Criteria TC-301 FMP profile endpoint normalization Unit MUST - Mock FMP profile response- Verify normalized fields (symbol, companyName, sector)- Handle missing fields gracefully TC-302 Polygon aggs_daily date range Integration MUST - Request SPY data for 2024-01-01 to 2024-12-31- Verify all days present- Handle weekends/holidays TC-303 FRED series fetch (CPIAUCSL) Integration MUST - Request CPIAUCSL time series- Verify dates + values- Handle FRED-specific date format TC-304 FMP handles HTTP 429 rate limit Integration SHOULD - Mock 429 response with Retry-After: 60- Verify 60s cooldown- Retry after cooldown"},{"location":"TEST_STRATEGY/#85-end-to-end-tests-tier-2","title":"8.5 End-to-End Tests (TIER 2)","text":"ID Scenario Type Priority Acceptance Criteria TC-401 Happy path: Cache miss \u2192 API \u2192 Cache hit E2E MUST - 1st call: Cache miss \u2192 API fetch \u2192 Cache write- 2nd call: Cache hit (no API call)- Verify same data returned TC-402 READ-ONLY mode (no API calls) E2E MUST - Set mode = READ_ONLY- Cache hit: returns data- Cache miss: raises ReadOnlyError (no API call) TC-403 Parallel multi-provider fetch E2E SHOULD - Fetch FMP profile + Polygon aggs + FRED CPIAUCSL- All 3 return data- Total time \u2248 slowest single request TC-404 Health report aggregation E2E SHOULD - Execute 10 FMP calls (8 success, 2 fail)- get_api_health_report()- Verify: total=10, errors=2, error_rate=0.2, circuit=CLOSED"},{"location":"TEST_STRATEGY/#9-tools","title":"9. Tools","text":"Purpose Tool Version Notes Test Framework pytest \u22657.0 Industry standard; excellent async support Async Testing pytest-asyncio \u22650.21 Enables <code>@pytest.mark.asyncio</code> decorator Coverage Reporting pytest-cov \u22653.0 Integrated with pytest; generates HTML reports HTTP Mocking aioresponses \u22650.7 Mocks <code>aiohttp.ClientSession</code> responses Temporary Files tempfile (built-in) - Isolated cache directories for tests Fixtures pytest fixtures - Reusable test setup (sessions, mock data) Parametrization pytest.mark.parametrize - Test multiple inputs efficiently Secret Scanning detect-secrets \u22651.4 Pre-commit hook for API key detection Type Checking mypy \u22651.0 Static type analysis Linting ruff / flake8 Latest Fast Python linter CI/CD GitHub Actions - Automated test execution on push"},{"location":"TEST_STRATEGY/#test-execution-commands","title":"Test Execution Commands","text":"<pre><code># Run all tests\npytest\n\n# Run unit tests only\npytest -m unit\n\n# Run integration tests only\npytest -m integration\n\n# Run E2E tests only\npytest -m e2e\n\n# Run with coverage report\npytest --cov=src --cov-report=html --cov-report=term-missing\n\n# Run TIER 1 tests (critical components)\npytest -m tier1\n\n# Run fast tests only (exclude slow integration)\npytest -m \"not slow\"\n\n# Run specific test file\npytest tests/test_circuit_breaker.py\n\n# Run with verbose output\npytest -v\n\n# Run with detailed failure output\npytest -vv\n\n# Parallel execution (optional, for speed)\npytest -n auto  # Requires pytest-xdist\n</code></pre>"},{"location":"TEST_STRATEGY/#coverage-reporting","title":"Coverage Reporting","text":"<pre><code># Generate HTML coverage report\npytest --cov=src --cov-report=html\nopen htmlcov/index.html\n\n# Terminal coverage summary\npytest --cov=src --cov-report=term-missing\n\n# Coverage by component (TIER 1)\npytest --cov=src/data_loader/circuit_breaker.py --cov-report=term\n\n# Fail if coverage below threshold\npytest --cov=src --cov-fail-under=80\n</code></pre>"},{"location":"TEST_STRATEGY/#10-test-pyramid-breakdown","title":"10. Test Pyramid Breakdown","text":""},{"location":"TEST_STRATEGY/#unit-tests-150-tests-20s-execution","title":"Unit Tests (~150 tests, ~20s execution)","text":"Component Test Count Key Scenarios Circuit Breaker 15 State transitions, threshold calculations, timeout reset QoS Semaphore Router 12 Concurrency limits per provider, acquire/release, timeout Retry Handler 10 Exponential backoff, jitter, max retries, 5xx only Cache Manager 20 Atomic writes, TTL expiration, cache key generation, file I/O Config Manager 8 .env parsing, defaults, validation, missing keys FMP Provider (normalization) 25 All 13 endpoints, field mapping, missing data handling Polygon Provider 15 4 endpoints, hash-based cache keys, date ranges FRED Provider 20 32 series (sample), time series parsing, date formats Health Monitor 10 Counter increments, error rate calculation, reset API Key Sanitization 10 Regex redaction, log output, error messages DataLoader (routing) 5 Provider selection, mode switching, health aggregation"},{"location":"TEST_STRATEGY/#integration-tests-40-tests-60s-execution","title":"Integration Tests (~40 tests, ~60s execution)","text":"Integration Point Test Count Key Scenarios FMP API (mocked) 10 429 handling, 5xx retry, circuit breaker, cache integration Polygon API (mocked) 8 Timeout retry, malformed JSON, cache integration FRED API (mocked) 8 Series fetch, date range, circuit breaker Cache + Filesystem 8 Concurrent writes, temp file cleanup, permissions Circuit Breaker + Provider 6 Error accumulation, recovery, state persistence"},{"location":"TEST_STRATEGY/#e2e-tests-10-tests-30s-execution","title":"E2E Tests (~10 tests, ~30s execution)","text":"Workflow Test Count Key Scenarios Cache miss \u2192 API \u2192 Cache hit 3 FMP, Polygon, FRED happy paths READ-ONLY mode 2 Cache hit success, cache miss error Multi-provider parallel 2 All providers succeed, partial failure Circuit breaker full cycle 2 Open \u2192 Half-Open \u2192 Closed QoS limits enforcement 1 Concurrent request throttling"},{"location":"TEST_STRATEGY/#11-what-this-test-strategy-doesnt-cover-and-why-thats-ok","title":"11. What This Test Strategy Doesn't Cover (And Why That's OK)","text":""},{"location":"TEST_STRATEGY/#excluded-from-testing-proportionate-to-solo-developer-tool","title":"Excluded from Testing (Proportionate to Solo Developer Tool)","text":"Test Type Why Excluded Alternative Real API integration tests Cost, rate limits, reliability Mock all API calls; manual smoke test on release Load/stress testing Personal tool, not production service Manual ad-hoc benchmarking if needed UI/UX testing No graphical interface N/A Multi-user concurrency Single-user system Document limitation Security penetration testing No external attack surface Pre-commit secret scanning + manual review Database testing No database (filesystem cache) N/A Cross-platform testing macOS/Linux only (POSIX assumed) Document Windows compatibility as out of scope Internationalization English only N/A Accessibility No UI N/A"},{"location":"TEST_STRATEGY/#12-test-organization","title":"12. Test Organization","text":""},{"location":"TEST_STRATEGY/#directory-structure","title":"Directory Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py                 # Shared fixtures (mock sessions, temp cache dirs)\n\u251c\u2500\u2500 fixtures/                   # Mock API responses\n\u2502   \u251c\u2500\u2500 fmp/\n\u2502   \u2502   \u251c\u2500\u2500 profile_AAPL.json\n\u2502   \u2502   \u251c\u2500\u2500 balance_sheet_MSFT.json\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 polygon/\n\u2502   \u2502   \u251c\u2500\u2500 aggs_daily_SPY.json\n\u2502   \u2502   \u2514\u2500\u2500 options_snapshot_AAPL.json\n\u2502   \u2514\u2500\u2500 fred/\n\u2502       \u251c\u2500\u2500 CPIAUCSL.json\n\u2502       \u251c\u2500\u2500 UNRATE.json\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 unit/                       # Unit tests (75%)\n\u2502   \u251c\u2500\u2500 test_circuit_breaker.py\n\u2502   \u251c\u2500\u2500 test_qos_router.py\n\u2502   \u251c\u2500\u2500 test_retry_handler.py\n\u2502   \u251c\u2500\u2500 test_cache_manager.py\n\u2502   \u251c\u2500\u2500 test_config_manager.py\n\u2502   \u251c\u2500\u2500 test_fmp_provider.py\n\u2502   \u251c\u2500\u2500 test_polygon_provider.py\n\u2502   \u251c\u2500\u2500 test_fred_provider.py\n\u2502   \u251c\u2500\u2500 test_health_monitor.py\n\u2502   \u2514\u2500\u2500 test_sanitization.py\n\u251c\u2500\u2500 integration/                # Integration tests (20%)\n\u2502   \u251c\u2500\u2500 test_fmp_integration.py\n\u2502   \u251c\u2500\u2500 test_polygon_integration.py\n\u2502   \u251c\u2500\u2500 test_fred_integration.py\n\u2502   \u251c\u2500\u2500 test_cache_filesystem.py\n\u2502   \u2514\u2500\u2500 test_circuit_breaker_integration.py\n\u2514\u2500\u2500 e2e/                        # End-to-end tests (5%)\n    \u251c\u2500\u2500 test_happy_path.py\n    \u251c\u2500\u2500 test_readonly_mode.py\n    \u251c\u2500\u2500 test_parallel_fetch.py\n    \u2514\u2500\u2500 test_resilience.py\n</code></pre>"},{"location":"TEST_STRATEGY/#pytest-markers","title":"Pytest Markers","text":"<pre><code># In conftest.py\ndef pytest_configure(config):\n    config.addinivalue_line(\"markers\", \"unit: Unit tests (fast, isolated)\")\n    config.addinivalue_line(\"markers\", \"integration: Integration tests (mocked HTTP)\")\n    config.addinivalue_line(\"markers\", \"e2e: End-to-end tests (full workflows)\")\n    config.addinivalue_line(\"markers\", \"tier1: TIER 1 components (&gt;90% coverage required)\")\n    config.addinivalue_line(\"markers\", \"tier2: TIER 2 components (&gt;80% coverage required)\")\n    config.addinivalue_line(\"markers\", \"slow: Slow tests (&gt;1s execution)\")\n    config.addinivalue_line(\"markers\", \"security: Security-critical tests\")\n</code></pre>"},{"location":"TEST_STRATEGY/#13-test-maintenance-strategy","title":"13. Test Maintenance Strategy","text":""},{"location":"TEST_STRATEGY/#when-to-update-tests","title":"When to Update Tests","text":"Trigger Test Update Required Example API schema change Update fixtures + normalization tests FMP adds new field to profile response New endpoint added Add provider tests FRED adds new series Requirement change Update acceptance criteria Circuit breaker threshold changes to 30% Bug fix Add regression test Cache corruption bug \u2192 add test for that scenario Refactoring Update mocks/fixtures if interfaces change Provider base class signature changes"},{"location":"TEST_STRATEGY/#test-fixture-refresh","title":"Test Fixture Refresh","text":"<pre><code># Manual script to record real API responses (run locally, not in CI)\npython scripts/record_fixtures.py --provider fmp --endpoint profile --symbol AAPL\npython scripts/record_fixtures.py --provider polygon --endpoint aggs_daily --symbol SPY\npython scripts/record_fixtures.py --provider fred --series CPIAUCSL\n\n# Stores responses in tests/fixtures/{provider}/\n# Commit updated fixtures to version control\n</code></pre>"},{"location":"TEST_STRATEGY/#14-success-metrics","title":"14. Success Metrics","text":"Metric Target Current Status Measurement Overall line coverage &gt;80% TBD \u2610 pytest-cov TIER 1 coverage &gt;90% TBD \u2610 pytest-cov (filtered) Test pass rate 100% TBD \u2610 CI pipeline Test execution time &lt;2 min TBD \u2610 CI pipeline Security test coverage 100% critical TBD \u2610 Manual checklist Flaky test rate &lt;5% TBD \u2610 CI re-runs Bug escape rate &lt;10% TBD \u2610 Post-release issues <p>Definition of \"Done\" for Testing: - [ ] All TIER 1 components have &gt;90% coverage - [ ] All critical test cases (TC-001 through TC-105) passing - [ ] No API keys detected in logs/cache (TC-101 through TC-106) - [ ] CI pipeline green on main branch - [ ] Test execution time &lt;2 minutes - [ ] All E2E workflows validated</p>"},{"location":"TEST_STRATEGY/#approval","title":"Approval","text":"Role Name Date Status Author QA Agent 2026-01-31 \u2705 Generated Technical Review Architect Agent \u2610 Pending Owner Solo Developer \u2610 Pending"},{"location":"TEST_STRATEGY/#revision-history","title":"Revision History","text":"Version Date Author Changes 1.0 2026-01-31 QA Agent Initial test strategy for OmniData Nexus Core <p>Testing Philosophy: Scale-appropriate for a solo developer research tool. Focus on high-risk components (Circuit Breaker, QoS, Security) while accepting manual testing for low-risk utilities. All external APIs mocked in CI to ensure fast, reliable, cost-effective testing.</p> <p>Test strategy aligned with NFR-006 (&gt;80% coverage) and designed for fast iteration (&lt;2 min CI time).</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to OmniData Nexus Core!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/Nexus_Core.git\ncd Nexus_Core\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install -e .\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<p>Follow the coding standards below.</p>"},{"location":"contributing/#3-run-quality-checks","title":"3. Run Quality Checks","text":"<pre><code># Linting\nruff check src/ tests/\n\n# Type checking\nmypy src/\n\n# Tests with coverage\npytest --cov=src\n</code></pre>"},{"location":"contributing/#4-commit","title":"4. Commit","text":"<pre><code>git add .\ngit commit -m \"feat: add your feature description\"\n</code></pre> <p>Use conventional commit messages:</p> <ul> <li><code>feat:</code> New feature</li> <li><code>fix:</code> Bug fix</li> <li><code>docs:</code> Documentation</li> <li><code>test:</code> Tests</li> <li><code>refactor:</code> Code refactoring</li> <li><code>chore:</code> Maintenance</li> </ul>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then open a Pull Request on GitHub.</p>"},{"location":"contributing/#coding-standards","title":"Coding Standards","text":""},{"location":"contributing/#python-style","title":"Python Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Maximum line length: 100 characters</li> <li>Use <code>ruff</code> for linting and formatting</li> </ul> <pre><code># Format code\nruff format src/ tests/\n\n# Check linting\nruff check src/ tests/\n</code></pre>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def fetch_data(symbol: str, use_cache: bool = True) -&gt; DataResult:\n    \"\"\"Fetch data for a symbol.\n\n    Args:\n        symbol: The stock symbol (e.g., \"AAPL\").\n        use_cache: Whether to use cached data. Defaults to True.\n\n    Returns:\n        DataResult containing the fetched data.\n\n    Raises:\n        ProviderError: If the API request fails.\n        RateLimitError: If rate limited.\n    \"\"\"\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<pre><code>from typing import Optional\n\nasync def get_data(\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    *,\n    symbol: Optional[str] = None,\n) -&gt; DataResult:\n    ...\n</code></pre>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#write-tests-first","title":"Write Tests First","text":"<p>For new features, write tests before implementation.</p>"},{"location":"contributing/#test-categories","title":"Test Categories","text":"<p>Mark tests appropriately:</p> <pre><code>@pytest.mark.unit\ndef test_cache_key_generation():\n    ...\n\n@pytest.mark.integration\nasync def test_api_with_cache():\n    ...\n\n@pytest.mark.e2e\nasync def test_full_workflow():\n    ...\n</code></pre>"},{"location":"contributing/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>New code must have &gt;90% coverage</li> <li>Don't decrease overall coverage</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#update-docs","title":"Update Docs","text":"<p>When adding features, update:</p> <ol> <li>Docstrings in code</li> <li>Relevant guide pages in <code>docs/guide/</code></li> <li>API reference in <code>docs/api/</code></li> <li>CHANGELOG.md</li> </ol>"},{"location":"contributing/#build-docs-locally","title":"Build Docs Locally","text":"<pre><code>mkdocs serve\n# Open http://127.0.0.1:8000\n</code></pre>"},{"location":"contributing/#adding-a-new-provider","title":"Adding a New Provider","text":"<ol> <li>Create provider class in <code>src/data_loader/providers/</code></li> <li>Inherit from <code>BaseDataProvider</code></li> <li>Define <code>ENDPOINTS</code> mapping</li> <li>Implement <code>fetch()</code> method</li> <li>Add to <code>DataLoader</code> class</li> <li>Write tests</li> <li>Document endpoints</li> </ol> <p>Example:</p> <pre><code># src/data_loader/providers/new_provider.py\nfrom .base import BaseDataProvider\n\nclass NewProvider(BaseDataProvider):\n    ENDPOINTS = {\n        \"endpoint1\": \"/api/endpoint1\",\n        \"endpoint2\": \"/api/endpoint2\",\n    }\n\n    async def fetch(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        **params,\n    ) -&gt; dict:\n        url = self._build_url(endpoint, params)\n        return await self._request(session, url)\n</code></pre>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<ul> <li>[ ] Tests pass (<code>pytest</code>)</li> <li>[ ] Linting passes (<code>ruff check</code>)</li> <li>[ ] Type checking passes (<code>mypy</code>)</li> <li>[ ] Coverage maintained (&gt;90%)</li> <li>[ ] Documentation updated</li> <li>[ ] CHANGELOG.md updated</li> <li>[ ] Commits follow conventional format</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub: Nexus_Core Issues</p>"},{"location":"api/cache/","title":"Cache API Reference","text":""},{"location":"api/cache/#data_loader.cache.CacheManager","title":"<code>data_loader.cache.CacheManager</code>","text":"<p>Filesystem-based JSON cache with atomic writes.</p> <p>Features: - Provider-specific directories (fmp_cache, polygon_cache, fred_cache) - Atomic writes using temp file + rename pattern - TTL-based expiration - Thread-safe file operations</p> Usage <p>cache = CacheManager(base_dir=Path(\"./data/cache\"), ttl_days=7)</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>class CacheManager:\n    \"\"\"\n    Filesystem-based JSON cache with atomic writes.\n\n    Features:\n    - Provider-specific directories (fmp_cache, polygon_cache, fred_cache)\n    - Atomic writes using temp file + rename pattern\n    - TTL-based expiration\n    - Thread-safe file operations\n\n    Usage:\n        cache = CacheManager(base_dir=Path(\"./data/cache\"), ttl_days=7)\n\n        # Write to cache\n        cache.set(\"fmp\", \"profile_AAPL\", {\"symbol\": \"AAPL\", ...})\n\n        # Read from cache\n        entry = cache.get(\"fmp\", \"profile_AAPL\")\n        if entry and not entry.is_expired:\n            print(entry.data)\n\n        # Delete from cache\n        cache.delete(\"fmp\", \"profile_AAPL\")\n    \"\"\"\n\n    def __init__(\n        self,\n        base_dir: Path,\n        ttl_days: int = 7,\n        enabled: bool = True,\n    ):\n        \"\"\"\n        Initialize cache manager.\n\n        Args:\n            base_dir: Base directory for cache storage\n            ttl_days: Default TTL for cache entries\n            enabled: Whether caching is enabled\n        \"\"\"\n        self.base_dir = Path(base_dir)\n        self.ttl_days = ttl_days\n        self.enabled = enabled\n\n        # Create base directory if enabled\n        if self.enabled:\n            self.base_dir.mkdir(parents=True, exist_ok=True)\n\n    def _get_provider_dir(self, provider: str) -&gt; Path:\n        \"\"\"Get directory for a specific provider.\"\"\"\n        return self.base_dir / f\"{provider}_cache\"\n\n    def _get_cache_path(self, provider: str, key: str) -&gt; Path:\n        \"\"\"Get full path for a cache entry.\"\"\"\n        # Sanitize key to be filesystem-safe\n        safe_key = self._sanitize_key(key)\n        return self._get_provider_dir(provider) / f\"{safe_key}.json\"\n\n    @staticmethod\n    def _sanitize_key(key: str) -&gt; str:\n        \"\"\"\n        Sanitize cache key for filesystem safety.\n\n        Replaces unsafe characters with underscores.\n        \"\"\"\n        # Replace common unsafe characters\n        unsafe_chars = ['/', '\\\\', ':', '*', '?', '\"', '&lt;', '&gt;', '|', ' ']\n        result = key\n        for char in unsafe_chars:\n            result = result.replace(char, '_')\n        return result\n\n    def set(\n        self,\n        provider: str,\n        key: str,\n        data: Any,\n        ttl_days: Optional[int] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Store data in cache.\n\n        Uses atomic write pattern (temp file + rename) to prevent\n        corruption from interrupted writes.\n\n        Args:\n            provider: Provider name (fmp, polygon, fred)\n            key: Cache key\n            data: Data to cache (must be JSON-serializable)\n            ttl_days: Optional TTL override\n\n        Returns:\n            True if successfully cached, False otherwise\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        try:\n            # Ensure provider directory exists\n            provider_dir = self._get_provider_dir(provider)\n            provider_dir.mkdir(parents=True, exist_ok=True)\n\n            # Create cache entry\n            entry = CacheEntry(\n                data=data,\n                timestamp=time.time(),\n                ttl_days=ttl_days or self.ttl_days,\n                provider=provider,\n                key=key,\n            )\n\n            cache_path = self._get_cache_path(provider, key)\n\n            # Atomic write: write to temp file, then rename\n            fd, temp_path = tempfile.mkstemp(\n                suffix=\".json.tmp\",\n                dir=provider_dir,\n            )\n\n            try:\n                with os.fdopen(fd, 'w', encoding='utf-8') as f:\n                    json.dump(entry.to_dict(), f, indent=2, ensure_ascii=False)\n\n                # Atomic rename (on POSIX systems)\n                os.replace(temp_path, cache_path)\n                return True\n\n            except Exception:\n                # Clean up temp file on failure\n                if os.path.exists(temp_path):\n                    os.unlink(temp_path)\n                raise\n\n        except Exception:\n            return False\n\n    def get(\n        self,\n        provider: str,\n        key: str,\n        ignore_expired: bool = False,\n    ) -&gt; Optional[CacheEntry]:\n        \"\"\"\n        Retrieve data from cache.\n\n        Args:\n            provider: Provider name\n            key: Cache key\n            ignore_expired: If True, return expired entries\n\n        Returns:\n            CacheEntry if found and valid, None otherwise\n        \"\"\"\n        if not self.enabled:\n            return None\n\n        cache_path = self._get_cache_path(provider, key)\n\n        if not cache_path.exists():\n            return None\n\n        try:\n            with open(cache_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n\n            entry = CacheEntry.from_dict(data)\n\n            # Check expiration\n            if entry.is_expired and not ignore_expired:\n                return None\n\n            return entry\n\n        except (json.JSONDecodeError, KeyError, IOError):\n            # Invalid or corrupted cache file\n            return None\n\n    def delete(self, provider: str, key: str) -&gt; bool:\n        \"\"\"\n        Delete a cache entry.\n\n        Args:\n            provider: Provider name\n            key: Cache key\n\n        Returns:\n            True if deleted, False if not found or error\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        cache_path = self._get_cache_path(provider, key)\n\n        try:\n            if cache_path.exists():\n                cache_path.unlink()\n                return True\n            return False\n        except IOError:\n            return False\n\n    def clear_provider(self, provider: str) -&gt; int:\n        \"\"\"\n        Clear all cache entries for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            Number of entries deleted\n        \"\"\"\n        if not self.enabled:\n            return 0\n\n        provider_dir = self._get_provider_dir(provider)\n\n        if not provider_dir.exists():\n            return 0\n\n        count = 0\n        for cache_file in provider_dir.glob(\"*.json\"):\n            try:\n                cache_file.unlink()\n                count += 1\n            except IOError:\n                pass\n\n        return count\n\n    def clear_all(self) -&gt; int:\n        \"\"\"\n        Clear all cache entries for all providers.\n\n        Returns:\n            Total number of entries deleted\n        \"\"\"\n        if not self.enabled:\n            return 0\n\n        total = 0\n        for provider in [\"fmp\", \"polygon\", \"fred\"]:\n            total += self.clear_provider(provider)\n\n        return total\n\n    def clear_expired(self, provider: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Delete expired cache entries.\n\n        Args:\n            provider: Optional provider to limit cleanup\n\n        Returns:\n            Number of expired entries deleted\n        \"\"\"\n        if not self.enabled:\n            return 0\n\n        providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n        count = 0\n\n        for prov in providers:\n            provider_dir = self._get_provider_dir(prov)\n\n            if not provider_dir.exists():\n                continue\n\n            for cache_file in provider_dir.glob(\"*.json\"):\n                try:\n                    with open(cache_file, 'r', encoding='utf-8') as f:\n                        data = json.load(f)\n\n                    entry = CacheEntry.from_dict(data)\n\n                    if entry.is_expired:\n                        cache_file.unlink()\n                        count += 1\n\n                except (json.JSONDecodeError, KeyError, IOError):\n                    # Remove corrupted files\n                    try:\n                        cache_file.unlink()\n                        count += 1\n                    except IOError:\n                        pass\n\n        return count\n\n    def get_stats(self, provider: Optional[str] = None) -&gt; dict:\n        \"\"\"\n        Get cache statistics.\n\n        Args:\n            provider: Optional provider to limit stats\n\n        Returns:\n            Dictionary with cache statistics\n        \"\"\"\n        if not self.enabled:\n            return {\"enabled\": False}\n\n        providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n        stats = {\n            \"enabled\": True,\n            \"base_dir\": str(self.base_dir),\n            \"ttl_days\": self.ttl_days,\n            \"providers\": {},\n        }\n\n        for prov in providers:\n            provider_dir = self._get_provider_dir(prov)\n\n            prov_stats = {\n                \"total_entries\": 0,\n                \"expired_entries\": 0,\n                \"valid_entries\": 0,\n                \"total_size_bytes\": 0,\n            }\n\n            if provider_dir.exists():\n                for cache_file in provider_dir.glob(\"*.json\"):\n                    prov_stats[\"total_entries\"] += 1\n                    prov_stats[\"total_size_bytes\"] += cache_file.stat().st_size\n\n                    try:\n                        with open(cache_file, 'r', encoding='utf-8') as f:\n                            data = json.load(f)\n                        entry = CacheEntry.from_dict(data)\n\n                        if entry.is_expired:\n                            prov_stats[\"expired_entries\"] += 1\n                        else:\n                            prov_stats[\"valid_entries\"] += 1\n\n                    except (json.JSONDecodeError, KeyError, IOError):\n                        prov_stats[\"expired_entries\"] += 1\n\n            stats[\"providers\"][prov] = prov_stats\n\n        return stats\n\n    def exists(self, provider: str, key: str) -&gt; bool:\n        \"\"\"\n        Check if a cache entry exists (ignoring expiration).\n\n        Args:\n            provider: Provider name\n            key: Cache key\n\n        Returns:\n            True if entry exists\n        \"\"\"\n        if not self.enabled:\n            return False\n\n        return self._get_cache_path(provider, key).exists()\n\n    def is_valid(self, provider: str, key: str) -&gt; bool:\n        \"\"\"\n        Check if a valid (non-expired) cache entry exists.\n\n        Args:\n            provider: Provider name\n            key: Cache key\n\n        Returns:\n            True if valid entry exists\n        \"\"\"\n        entry = self.get(provider, key)\n        return entry is not None and not entry.is_expired\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager--write-to-cache","title":"Write to cache","text":"<p>cache.set(\"fmp\", \"profile_AAPL\", {\"symbol\": \"AAPL\", ...})</p>"},{"location":"api/cache/#data_loader.cache.CacheManager--read-from-cache","title":"Read from cache","text":"<p>entry = cache.get(\"fmp\", \"profile_AAPL\") if entry and not entry.is_expired:     print(entry.data)</p>"},{"location":"api/cache/#data_loader.cache.CacheManager--delete-from-cache","title":"Delete from cache","text":"<p>cache.delete(\"fmp\", \"profile_AAPL\")</p>"},{"location":"api/cache/#data_loader.cache.CacheManager.__init__","title":"<code>__init__(base_dir, ttl_days=7, enabled=True)</code>","text":"<p>Initialize cache manager.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>Path</code> <p>Base directory for cache storage</p> required <code>ttl_days</code> <code>int</code> <p>Default TTL for cache entries</p> <code>7</code> <code>enabled</code> <code>bool</code> <p>Whether caching is enabled</p> <code>True</code> Source code in <code>src/data_loader/cache.py</code> <pre><code>def __init__(\n    self,\n    base_dir: Path,\n    ttl_days: int = 7,\n    enabled: bool = True,\n):\n    \"\"\"\n    Initialize cache manager.\n\n    Args:\n        base_dir: Base directory for cache storage\n        ttl_days: Default TTL for cache entries\n        enabled: Whether caching is enabled\n    \"\"\"\n    self.base_dir = Path(base_dir)\n    self.ttl_days = ttl_days\n    self.enabled = enabled\n\n    # Create base directory if enabled\n    if self.enabled:\n        self.base_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.set","title":"<code>set(provider, key, data, ttl_days=None)</code>","text":"<p>Store data in cache.</p> <p>Uses atomic write pattern (temp file + rename) to prevent corruption from interrupted writes.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (fmp, polygon, fred)</p> required <code>key</code> <code>str</code> <p>Cache key</p> required <code>data</code> <code>Any</code> <p>Data to cache (must be JSON-serializable)</p> required <code>ttl_days</code> <code>Optional[int]</code> <p>Optional TTL override</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if successfully cached, False otherwise</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def set(\n    self,\n    provider: str,\n    key: str,\n    data: Any,\n    ttl_days: Optional[int] = None,\n) -&gt; bool:\n    \"\"\"\n    Store data in cache.\n\n    Uses atomic write pattern (temp file + rename) to prevent\n    corruption from interrupted writes.\n\n    Args:\n        provider: Provider name (fmp, polygon, fred)\n        key: Cache key\n        data: Data to cache (must be JSON-serializable)\n        ttl_days: Optional TTL override\n\n    Returns:\n        True if successfully cached, False otherwise\n    \"\"\"\n    if not self.enabled:\n        return False\n\n    try:\n        # Ensure provider directory exists\n        provider_dir = self._get_provider_dir(provider)\n        provider_dir.mkdir(parents=True, exist_ok=True)\n\n        # Create cache entry\n        entry = CacheEntry(\n            data=data,\n            timestamp=time.time(),\n            ttl_days=ttl_days or self.ttl_days,\n            provider=provider,\n            key=key,\n        )\n\n        cache_path = self._get_cache_path(provider, key)\n\n        # Atomic write: write to temp file, then rename\n        fd, temp_path = tempfile.mkstemp(\n            suffix=\".json.tmp\",\n            dir=provider_dir,\n        )\n\n        try:\n            with os.fdopen(fd, 'w', encoding='utf-8') as f:\n                json.dump(entry.to_dict(), f, indent=2, ensure_ascii=False)\n\n            # Atomic rename (on POSIX systems)\n            os.replace(temp_path, cache_path)\n            return True\n\n        except Exception:\n            # Clean up temp file on failure\n            if os.path.exists(temp_path):\n                os.unlink(temp_path)\n            raise\n\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.get","title":"<code>get(provider, key, ignore_expired=False)</code>","text":"<p>Retrieve data from cache.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>key</code> <code>str</code> <p>Cache key</p> required <code>ignore_expired</code> <code>bool</code> <p>If True, return expired entries</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[CacheEntry]</code> <p>CacheEntry if found and valid, None otherwise</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def get(\n    self,\n    provider: str,\n    key: str,\n    ignore_expired: bool = False,\n) -&gt; Optional[CacheEntry]:\n    \"\"\"\n    Retrieve data from cache.\n\n    Args:\n        provider: Provider name\n        key: Cache key\n        ignore_expired: If True, return expired entries\n\n    Returns:\n        CacheEntry if found and valid, None otherwise\n    \"\"\"\n    if not self.enabled:\n        return None\n\n    cache_path = self._get_cache_path(provider, key)\n\n    if not cache_path.exists():\n        return None\n\n    try:\n        with open(cache_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        entry = CacheEntry.from_dict(data)\n\n        # Check expiration\n        if entry.is_expired and not ignore_expired:\n            return None\n\n        return entry\n\n    except (json.JSONDecodeError, KeyError, IOError):\n        # Invalid or corrupted cache file\n        return None\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.delete","title":"<code>delete(provider, key)</code>","text":"<p>Delete a cache entry.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>key</code> <code>str</code> <p>Cache key</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if deleted, False if not found or error</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def delete(self, provider: str, key: str) -&gt; bool:\n    \"\"\"\n    Delete a cache entry.\n\n    Args:\n        provider: Provider name\n        key: Cache key\n\n    Returns:\n        True if deleted, False if not found or error\n    \"\"\"\n    if not self.enabled:\n        return False\n\n    cache_path = self._get_cache_path(provider, key)\n\n    try:\n        if cache_path.exists():\n            cache_path.unlink()\n            return True\n        return False\n    except IOError:\n        return False\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.clear_provider","title":"<code>clear_provider(provider)</code>","text":"<p>Clear all cache entries for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of entries deleted</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def clear_provider(self, provider: str) -&gt; int:\n    \"\"\"\n    Clear all cache entries for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        Number of entries deleted\n    \"\"\"\n    if not self.enabled:\n        return 0\n\n    provider_dir = self._get_provider_dir(provider)\n\n    if not provider_dir.exists():\n        return 0\n\n    count = 0\n    for cache_file in provider_dir.glob(\"*.json\"):\n        try:\n            cache_file.unlink()\n            count += 1\n        except IOError:\n            pass\n\n    return count\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all cache entries for all providers.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of entries deleted</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def clear_all(self) -&gt; int:\n    \"\"\"\n    Clear all cache entries for all providers.\n\n    Returns:\n        Total number of entries deleted\n    \"\"\"\n    if not self.enabled:\n        return 0\n\n    total = 0\n    for provider in [\"fmp\", \"polygon\", \"fred\"]:\n        total += self.clear_provider(provider)\n\n    return total\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.clear_expired","title":"<code>clear_expired(provider=None)</code>","text":"<p>Delete expired cache entries.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Optional provider to limit cleanup</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of expired entries deleted</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def clear_expired(self, provider: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Delete expired cache entries.\n\n    Args:\n        provider: Optional provider to limit cleanup\n\n    Returns:\n        Number of expired entries deleted\n    \"\"\"\n    if not self.enabled:\n        return 0\n\n    providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n    count = 0\n\n    for prov in providers:\n        provider_dir = self._get_provider_dir(prov)\n\n        if not provider_dir.exists():\n            continue\n\n        for cache_file in provider_dir.glob(\"*.json\"):\n            try:\n                with open(cache_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n\n                entry = CacheEntry.from_dict(data)\n\n                if entry.is_expired:\n                    cache_file.unlink()\n                    count += 1\n\n            except (json.JSONDecodeError, KeyError, IOError):\n                # Remove corrupted files\n                try:\n                    cache_file.unlink()\n                    count += 1\n                except IOError:\n                    pass\n\n    return count\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.get_stats","title":"<code>get_stats(provider=None)</code>","text":"<p>Get cache statistics.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Optional provider to limit stats</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with cache statistics</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def get_stats(self, provider: Optional[str] = None) -&gt; dict:\n    \"\"\"\n    Get cache statistics.\n\n    Args:\n        provider: Optional provider to limit stats\n\n    Returns:\n        Dictionary with cache statistics\n    \"\"\"\n    if not self.enabled:\n        return {\"enabled\": False}\n\n    providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n    stats = {\n        \"enabled\": True,\n        \"base_dir\": str(self.base_dir),\n        \"ttl_days\": self.ttl_days,\n        \"providers\": {},\n    }\n\n    for prov in providers:\n        provider_dir = self._get_provider_dir(prov)\n\n        prov_stats = {\n            \"total_entries\": 0,\n            \"expired_entries\": 0,\n            \"valid_entries\": 0,\n            \"total_size_bytes\": 0,\n        }\n\n        if provider_dir.exists():\n            for cache_file in provider_dir.glob(\"*.json\"):\n                prov_stats[\"total_entries\"] += 1\n                prov_stats[\"total_size_bytes\"] += cache_file.stat().st_size\n\n                try:\n                    with open(cache_file, 'r', encoding='utf-8') as f:\n                        data = json.load(f)\n                    entry = CacheEntry.from_dict(data)\n\n                    if entry.is_expired:\n                        prov_stats[\"expired_entries\"] += 1\n                    else:\n                        prov_stats[\"valid_entries\"] += 1\n\n                except (json.JSONDecodeError, KeyError, IOError):\n                    prov_stats[\"expired_entries\"] += 1\n\n        stats[\"providers\"][prov] = prov_stats\n\n    return stats\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.exists","title":"<code>exists(provider, key)</code>","text":"<p>Check if a cache entry exists (ignoring expiration).</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>key</code> <code>str</code> <p>Cache key</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if entry exists</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def exists(self, provider: str, key: str) -&gt; bool:\n    \"\"\"\n    Check if a cache entry exists (ignoring expiration).\n\n    Args:\n        provider: Provider name\n        key: Cache key\n\n    Returns:\n        True if entry exists\n    \"\"\"\n    if not self.enabled:\n        return False\n\n    return self._get_cache_path(provider, key).exists()\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheManager.is_valid","title":"<code>is_valid(provider, key)</code>","text":"<p>Check if a valid (non-expired) cache entry exists.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>key</code> <code>str</code> <p>Cache key</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid entry exists</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def is_valid(self, provider: str, key: str) -&gt; bool:\n    \"\"\"\n    Check if a valid (non-expired) cache entry exists.\n\n    Args:\n        provider: Provider name\n        key: Cache key\n\n    Returns:\n        True if valid entry exists\n    \"\"\"\n    entry = self.get(provider, key)\n    return entry is not None and not entry.is_expired\n</code></pre>"},{"location":"api/cache/#cacheentry","title":"CacheEntry","text":""},{"location":"api/cache/#data_loader.cache.CacheEntry","title":"<code>data_loader.cache.CacheEntry</code>  <code>dataclass</code>","text":"<p>Represents a cached item with metadata.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Any</code> <p>The cached data</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp when cached</p> <code>ttl_days</code> <code>int</code> <p>Time-to-live in days</p> <code>provider</code> <code>str</code> <p>Source provider name</p> <code>key</code> <code>str</code> <p>Cache key</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>@dataclass\nclass CacheEntry:\n    \"\"\"\n    Represents a cached item with metadata.\n\n    Attributes:\n        data: The cached data\n        timestamp: Unix timestamp when cached\n        ttl_days: Time-to-live in days\n        provider: Source provider name\n        key: Cache key\n    \"\"\"\n\n    data: Any\n    timestamp: float\n    ttl_days: int\n    provider: str\n    key: str\n\n    @property\n    def expires_at(self) -&gt; datetime:\n        \"\"\"Get expiration datetime.\"\"\"\n        return datetime.fromtimestamp(self.timestamp) + timedelta(days=self.ttl_days)\n\n    @property\n    def is_expired(self) -&gt; bool:\n        \"\"\"Check if entry has expired.\"\"\"\n        return time.time() &gt; (self.timestamp + (self.ttl_days * 86400))\n\n    @property\n    def age_seconds(self) -&gt; float:\n        \"\"\"Get age of entry in seconds.\"\"\"\n        return time.time() - self.timestamp\n\n    @property\n    def age_hours(self) -&gt; float:\n        \"\"\"Get age of entry in hours.\"\"\"\n        return self.age_seconds / 3600\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Serialize to dictionary for JSON storage.\"\"\"\n        return {\n            \"data\": self.data,\n            \"timestamp\": self.timestamp,\n            \"ttl_days\": self.ttl_days,\n            \"provider\": self.provider,\n            \"key\": self.key,\n        }\n\n    @classmethod\n    def from_dict(cls, d: dict) -&gt; \"CacheEntry\":\n        \"\"\"Deserialize from dictionary.\"\"\"\n        return cls(\n            data=d[\"data\"],\n            timestamp=d[\"timestamp\"],\n            ttl_days=d[\"ttl_days\"],\n            provider=d[\"provider\"],\n            key=d[\"key\"],\n        )\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheEntry.expires_at","title":"<code>expires_at</code>  <code>property</code>","text":"<p>Get expiration datetime.</p>"},{"location":"api/cache/#data_loader.cache.CacheEntry.is_expired","title":"<code>is_expired</code>  <code>property</code>","text":"<p>Check if entry has expired.</p>"},{"location":"api/cache/#data_loader.cache.CacheEntry.age_seconds","title":"<code>age_seconds</code>  <code>property</code>","text":"<p>Get age of entry in seconds.</p>"},{"location":"api/cache/#data_loader.cache.CacheEntry.age_hours","title":"<code>age_hours</code>  <code>property</code>","text":"<p>Get age of entry in hours.</p>"},{"location":"api/cache/#data_loader.cache.CacheEntry.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to dictionary for JSON storage.</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Serialize to dictionary for JSON storage.\"\"\"\n    return {\n        \"data\": self.data,\n        \"timestamp\": self.timestamp,\n        \"ttl_days\": self.ttl_days,\n        \"provider\": self.provider,\n        \"key\": self.key,\n    }\n</code></pre>"},{"location":"api/cache/#data_loader.cache.CacheEntry.from_dict","title":"<code>from_dict(d)</code>  <code>classmethod</code>","text":"<p>Deserialize from dictionary.</p> Source code in <code>src/data_loader/cache.py</code> <pre><code>@classmethod\ndef from_dict(cls, d: dict) -&gt; \"CacheEntry\":\n    \"\"\"Deserialize from dictionary.\"\"\"\n    return cls(\n        data=d[\"data\"],\n        timestamp=d[\"timestamp\"],\n        ttl_days=d[\"ttl_days\"],\n        provider=d[\"provider\"],\n        key=d[\"key\"],\n    )\n</code></pre>"},{"location":"api/cache/#usage-examples","title":"Usage Examples","text":""},{"location":"api/cache/#getting-cache-stats","title":"Getting Cache Stats","text":"<pre><code>from data_loader import DataLoader\n\nloader = DataLoader()\ncache = loader._cache\n\nstats = cache.get_stats()\nprint(f\"Total entries: {stats['total_entries']}\")\nprint(f\"Total size: {stats['total_size_mb']:.2f} MB\")\n</code></pre>"},{"location":"api/cache/#cache-configuration","title":"Cache Configuration","text":"<pre><code># Set TTL\ncache.ttl_days = 14\n\n# Get cache directory\nprint(cache.cache_dir)\n</code></pre>"},{"location":"api/cache/#cache-file-structure","title":"Cache File Structure","text":"<p>Each cache entry is stored as a JSON file:</p> <pre><code>{\n  \"data\": { ... },\n  \"timestamp\": 1706698200.0,\n  \"provider\": \"fmp\",\n  \"key\": \"profile_AAPL\",\n  \"ttl_days\": 7\n}\n</code></pre>"},{"location":"api/health/","title":"Health Monitoring API Reference","text":""},{"location":"api/health/#data_loader.health.HealthMonitor","title":"<code>data_loader.health.HealthMonitor</code>","text":"<p>Tracks health and metrics for all data providers.</p> <p>Features: - Per-provider request tracking - Rolling window for recent metrics - Error rate calculation - Health status determination - Thread-safe operations</p> Usage <p>monitor = HealthMonitor()</p> Source code in <code>src/data_loader/health.py</code> <pre><code>class HealthMonitor:\n    \"\"\"\n    Tracks health and metrics for all data providers.\n\n    Features:\n    - Per-provider request tracking\n    - Rolling window for recent metrics\n    - Error rate calculation\n    - Health status determination\n    - Thread-safe operations\n\n    Usage:\n        monitor = HealthMonitor()\n\n        # Record a successful request\n        monitor.record_request(\n            provider=\"fmp\",\n            endpoint=\"profile\",\n            success=True,\n            status_code=200,\n            latency_ms=150.5\n        )\n\n        # Get health report\n        report = monitor.get_health_report()\n        print(report[\"fmp\"][\"status\"])  # \"healthy\"\n    \"\"\"\n\n    # Thresholds for status determination\n    ERROR_RATE_DEGRADED = 0.1  # 10% errors = degraded\n    ERROR_RATE_UNHEALTHY = 0.2  # 20% errors = unhealthy\n    MIN_REQUESTS_FOR_STATUS = 10  # Minimum requests before evaluating\n\n    def __init__(self, window_size: int = 100):\n        \"\"\"\n        Initialize health monitor.\n\n        Args:\n            window_size: Number of recent requests to track per provider\n        \"\"\"\n        self.window_size = window_size\n        self._lock = Lock()\n\n        # Per-provider request history (rolling window)\n        self._history: dict[str, deque[RequestMetrics]] = {\n            \"fmp\": deque(maxlen=window_size),\n            \"polygon\": deque(maxlen=window_size),\n            \"fred\": deque(maxlen=window_size),\n        }\n\n        # Cumulative counters (total since startup)\n        self._total_counters: dict[str, dict] = {\n            provider: {\n                \"total\": 0,\n                \"success\": 0,\n                \"failed\": 0,\n                \"rate_limited\": 0,\n                \"timeout\": 0,\n            }\n            for provider in [\"fmp\", \"polygon\", \"fred\"]\n        }\n\n    def record_request(\n        self,\n        provider: str,\n        endpoint: str,\n        success: bool,\n        status_code: Optional[int],\n        latency_ms: float,\n        error_type: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Record a request for metrics tracking.\n\n        Args:\n            provider: Provider name (fmp, polygon, fred)\n            endpoint: API endpoint called\n            success: Whether request succeeded\n            status_code: HTTP status code (if any)\n            latency_ms: Request latency in milliseconds\n            error_type: Type of error (if failed)\n        \"\"\"\n        metrics = RequestMetrics(\n            provider=provider,\n            endpoint=endpoint,\n            success=success,\n            status_code=status_code,\n            latency_ms=latency_ms,\n            timestamp=time.time(),\n            error_type=error_type,\n        )\n\n        with self._lock:\n            if provider not in self._history:\n                self._history[provider] = deque(maxlen=self.window_size)\n                self._total_counters[provider] = {\n                    \"total\": 0, \"success\": 0, \"failed\": 0,\n                    \"rate_limited\": 0, \"timeout\": 0,\n                }\n\n            self._history[provider].append(metrics)\n\n            # Update cumulative counters\n            counters = self._total_counters[provider]\n            counters[\"total\"] += 1\n\n            if success:\n                counters[\"success\"] += 1\n            else:\n                counters[\"failed\"] += 1\n                if status_code == 429:\n                    counters[\"rate_limited\"] += 1\n                if error_type == \"timeout\":\n                    counters[\"timeout\"] += 1\n\n    def get_provider_metrics(self, provider: str) -&gt; ProviderMetrics:\n        \"\"\"\n        Get aggregated metrics for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            ProviderMetrics with current statistics\n        \"\"\"\n        with self._lock:\n            history = self._history.get(provider, deque())\n            counters = self._total_counters.get(provider, {\n                \"total\": 0, \"success\": 0, \"failed\": 0,\n                \"rate_limited\": 0, \"timeout\": 0,\n            })\n\n            metrics = ProviderMetrics(provider=provider)\n\n            # Use cumulative counters for totals\n            metrics.total_requests = counters[\"total\"]\n            metrics.successful_requests = counters[\"success\"]\n            metrics.failed_requests = counters[\"failed\"]\n            metrics.rate_limited_requests = counters[\"rate_limited\"]\n            metrics.timeout_requests = counters[\"timeout\"]\n\n            # Calculate error rate from recent history (rolling window)\n            if history:\n                recent_failures = sum(1 for m in history if not m.success)\n                metrics.error_rate = recent_failures / len(history)\n\n                # Calculate latency stats from history\n                latencies = [m.latency_ms for m in history]\n                metrics.avg_latency_ms = sum(latencies) / len(latencies)\n                metrics.min_latency_ms = min(latencies)\n                metrics.max_latency_ms = max(latencies)\n\n                # Find last success/error\n                for m in reversed(history):\n                    if m.success and metrics.last_success is None:\n                        metrics.last_success = m.timestamp\n                    if not m.success and metrics.last_error is None:\n                        metrics.last_error = m.timestamp\n                        metrics.last_error_type = m.error_type\n                    if metrics.last_success and metrics.last_error:\n                        break\n\n            # Determine status\n            metrics.status = self._determine_status(metrics)\n\n            return metrics\n\n    def _determine_status(self, metrics: ProviderMetrics) -&gt; ProviderStatus:\n        \"\"\"Determine health status based on metrics.\"\"\"\n        if metrics.total_requests &lt; self.MIN_REQUESTS_FOR_STATUS:\n            return ProviderStatus.UNKNOWN\n\n        if metrics.error_rate &gt;= self.ERROR_RATE_UNHEALTHY:\n            return ProviderStatus.UNHEALTHY\n\n        if metrics.error_rate &gt;= self.ERROR_RATE_DEGRADED:\n            return ProviderStatus.DEGRADED\n\n        return ProviderStatus.HEALTHY\n\n    def get_health_report(self) -&gt; dict:\n        \"\"\"\n        Get health report for all providers.\n\n        Returns:\n            Dictionary with metrics for each provider and overall status\n        \"\"\"\n        report = {\n            \"timestamp\": time.time(),\n            \"providers\": {},\n            \"overall_status\": ProviderStatus.HEALTHY.value,\n        }\n\n        statuses = []\n        for provider in [\"fmp\", \"polygon\", \"fred\"]:\n            metrics = self.get_provider_metrics(provider)\n            report[\"providers\"][provider] = metrics.to_dict()\n            statuses.append(metrics.status)\n\n        # Overall status is the worst of all providers\n        if ProviderStatus.UNHEALTHY in statuses:\n            report[\"overall_status\"] = ProviderStatus.UNHEALTHY.value\n        elif ProviderStatus.DEGRADED in statuses:\n            report[\"overall_status\"] = ProviderStatus.DEGRADED.value\n        elif all(s == ProviderStatus.UNKNOWN for s in statuses):\n            report[\"overall_status\"] = ProviderStatus.UNKNOWN.value\n\n        return report\n\n    def get_provider_status(self, provider: str) -&gt; ProviderStatus:\n        \"\"\"\n        Get current health status for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            ProviderStatus enum value\n        \"\"\"\n        metrics = self.get_provider_metrics(provider)\n        return metrics.status\n\n    def is_healthy(self, provider: str) -&gt; bool:\n        \"\"\"\n        Check if a provider is healthy.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            True if provider is healthy or unknown (not enough data)\n        \"\"\"\n        status = self.get_provider_status(provider)\n        return status in (ProviderStatus.HEALTHY, ProviderStatus.UNKNOWN)\n\n    def get_error_rate(self, provider: str) -&gt; float:\n        \"\"\"\n        Get current error rate for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            Error rate as a float (0.0 to 1.0)\n        \"\"\"\n        metrics = self.get_provider_metrics(provider)\n        return metrics.error_rate\n\n    def get_avg_latency(self, provider: str) -&gt; float:\n        \"\"\"\n        Get average latency for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            Average latency in milliseconds\n        \"\"\"\n        metrics = self.get_provider_metrics(provider)\n        return metrics.avg_latency_ms\n\n    def reset(self, provider: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Reset metrics for a provider or all providers.\n\n        Args:\n            provider: Optional provider to reset, or None for all\n        \"\"\"\n        with self._lock:\n            providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n\n            for p in providers:\n                if p in self._history:\n                    self._history[p].clear()\n                if p in self._total_counters:\n                    self._total_counters[p] = {\n                        \"total\": 0, \"success\": 0, \"failed\": 0,\n                        \"rate_limited\": 0, \"timeout\": 0,\n                    }\n\n    def record_success(\n        self,\n        provider: str,\n        endpoint: str,\n        latency_ms: float,\n        status_code: int = 200,\n    ) -&gt; None:\n        \"\"\"\n        Convenience method to record a successful request.\n\n        Args:\n            provider: Provider name\n            endpoint: API endpoint\n            latency_ms: Request latency\n            status_code: HTTP status code\n        \"\"\"\n        self.record_request(\n            provider=provider,\n            endpoint=endpoint,\n            success=True,\n            status_code=status_code,\n            latency_ms=latency_ms,\n        )\n\n    def record_failure(\n        self,\n        provider: str,\n        endpoint: str,\n        latency_ms: float,\n        status_code: Optional[int] = None,\n        error_type: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Convenience method to record a failed request.\n\n        Args:\n            provider: Provider name\n            endpoint: API endpoint\n            latency_ms: Request latency\n            status_code: HTTP status code (if any)\n            error_type: Type of error\n        \"\"\"\n        self.record_request(\n            provider=provider,\n            endpoint=endpoint,\n            success=False,\n            status_code=status_code,\n            latency_ms=latency_ms,\n            error_type=error_type,\n        )\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor--record-a-successful-request","title":"Record a successful request","text":"<p>monitor.record_request(     provider=\"fmp\",     endpoint=\"profile\",     success=True,     status_code=200,     latency_ms=150.5 )</p>"},{"location":"api/health/#data_loader.health.HealthMonitor--get-health-report","title":"Get health report","text":"<p>report = monitor.get_health_report() print(report[\"fmp\"][\"status\"])  # \"healthy\"</p>"},{"location":"api/health/#data_loader.health.HealthMonitor.__init__","title":"<code>__init__(window_size=100)</code>","text":"<p>Initialize health monitor.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Number of recent requests to track per provider</p> <code>100</code> Source code in <code>src/data_loader/health.py</code> <pre><code>def __init__(self, window_size: int = 100):\n    \"\"\"\n    Initialize health monitor.\n\n    Args:\n        window_size: Number of recent requests to track per provider\n    \"\"\"\n    self.window_size = window_size\n    self._lock = Lock()\n\n    # Per-provider request history (rolling window)\n    self._history: dict[str, deque[RequestMetrics]] = {\n        \"fmp\": deque(maxlen=window_size),\n        \"polygon\": deque(maxlen=window_size),\n        \"fred\": deque(maxlen=window_size),\n    }\n\n    # Cumulative counters (total since startup)\n    self._total_counters: dict[str, dict] = {\n        provider: {\n            \"total\": 0,\n            \"success\": 0,\n            \"failed\": 0,\n            \"rate_limited\": 0,\n            \"timeout\": 0,\n        }\n        for provider in [\"fmp\", \"polygon\", \"fred\"]\n    }\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.record_request","title":"<code>record_request(provider, endpoint, success, status_code, latency_ms, error_type=None)</code>","text":"<p>Record a request for metrics tracking.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name (fmp, polygon, fred)</p> required <code>endpoint</code> <code>str</code> <p>API endpoint called</p> required <code>success</code> <code>bool</code> <p>Whether request succeeded</p> required <code>status_code</code> <code>Optional[int]</code> <p>HTTP status code (if any)</p> required <code>latency_ms</code> <code>float</code> <p>Request latency in milliseconds</p> required <code>error_type</code> <code>Optional[str]</code> <p>Type of error (if failed)</p> <code>None</code> Source code in <code>src/data_loader/health.py</code> <pre><code>def record_request(\n    self,\n    provider: str,\n    endpoint: str,\n    success: bool,\n    status_code: Optional[int],\n    latency_ms: float,\n    error_type: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Record a request for metrics tracking.\n\n    Args:\n        provider: Provider name (fmp, polygon, fred)\n        endpoint: API endpoint called\n        success: Whether request succeeded\n        status_code: HTTP status code (if any)\n        latency_ms: Request latency in milliseconds\n        error_type: Type of error (if failed)\n    \"\"\"\n    metrics = RequestMetrics(\n        provider=provider,\n        endpoint=endpoint,\n        success=success,\n        status_code=status_code,\n        latency_ms=latency_ms,\n        timestamp=time.time(),\n        error_type=error_type,\n    )\n\n    with self._lock:\n        if provider not in self._history:\n            self._history[provider] = deque(maxlen=self.window_size)\n            self._total_counters[provider] = {\n                \"total\": 0, \"success\": 0, \"failed\": 0,\n                \"rate_limited\": 0, \"timeout\": 0,\n            }\n\n        self._history[provider].append(metrics)\n\n        # Update cumulative counters\n        counters = self._total_counters[provider]\n        counters[\"total\"] += 1\n\n        if success:\n            counters[\"success\"] += 1\n        else:\n            counters[\"failed\"] += 1\n            if status_code == 429:\n                counters[\"rate_limited\"] += 1\n            if error_type == \"timeout\":\n                counters[\"timeout\"] += 1\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.get_provider_metrics","title":"<code>get_provider_metrics(provider)</code>","text":"<p>Get aggregated metrics for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>ProviderMetrics</code> <p>ProviderMetrics with current statistics</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def get_provider_metrics(self, provider: str) -&gt; ProviderMetrics:\n    \"\"\"\n    Get aggregated metrics for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        ProviderMetrics with current statistics\n    \"\"\"\n    with self._lock:\n        history = self._history.get(provider, deque())\n        counters = self._total_counters.get(provider, {\n            \"total\": 0, \"success\": 0, \"failed\": 0,\n            \"rate_limited\": 0, \"timeout\": 0,\n        })\n\n        metrics = ProviderMetrics(provider=provider)\n\n        # Use cumulative counters for totals\n        metrics.total_requests = counters[\"total\"]\n        metrics.successful_requests = counters[\"success\"]\n        metrics.failed_requests = counters[\"failed\"]\n        metrics.rate_limited_requests = counters[\"rate_limited\"]\n        metrics.timeout_requests = counters[\"timeout\"]\n\n        # Calculate error rate from recent history (rolling window)\n        if history:\n            recent_failures = sum(1 for m in history if not m.success)\n            metrics.error_rate = recent_failures / len(history)\n\n            # Calculate latency stats from history\n            latencies = [m.latency_ms for m in history]\n            metrics.avg_latency_ms = sum(latencies) / len(latencies)\n            metrics.min_latency_ms = min(latencies)\n            metrics.max_latency_ms = max(latencies)\n\n            # Find last success/error\n            for m in reversed(history):\n                if m.success and metrics.last_success is None:\n                    metrics.last_success = m.timestamp\n                if not m.success and metrics.last_error is None:\n                    metrics.last_error = m.timestamp\n                    metrics.last_error_type = m.error_type\n                if metrics.last_success and metrics.last_error:\n                    break\n\n        # Determine status\n        metrics.status = self._determine_status(metrics)\n\n        return metrics\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.get_health_report","title":"<code>get_health_report()</code>","text":"<p>Get health report for all providers.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with metrics for each provider and overall status</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def get_health_report(self) -&gt; dict:\n    \"\"\"\n    Get health report for all providers.\n\n    Returns:\n        Dictionary with metrics for each provider and overall status\n    \"\"\"\n    report = {\n        \"timestamp\": time.time(),\n        \"providers\": {},\n        \"overall_status\": ProviderStatus.HEALTHY.value,\n    }\n\n    statuses = []\n    for provider in [\"fmp\", \"polygon\", \"fred\"]:\n        metrics = self.get_provider_metrics(provider)\n        report[\"providers\"][provider] = metrics.to_dict()\n        statuses.append(metrics.status)\n\n    # Overall status is the worst of all providers\n    if ProviderStatus.UNHEALTHY in statuses:\n        report[\"overall_status\"] = ProviderStatus.UNHEALTHY.value\n    elif ProviderStatus.DEGRADED in statuses:\n        report[\"overall_status\"] = ProviderStatus.DEGRADED.value\n    elif all(s == ProviderStatus.UNKNOWN for s in statuses):\n        report[\"overall_status\"] = ProviderStatus.UNKNOWN.value\n\n    return report\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.get_provider_status","title":"<code>get_provider_status(provider)</code>","text":"<p>Get current health status for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>ProviderStatus</code> <p>ProviderStatus enum value</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def get_provider_status(self, provider: str) -&gt; ProviderStatus:\n    \"\"\"\n    Get current health status for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        ProviderStatus enum value\n    \"\"\"\n    metrics = self.get_provider_metrics(provider)\n    return metrics.status\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.is_healthy","title":"<code>is_healthy(provider)</code>","text":"<p>Check if a provider is healthy.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if provider is healthy or unknown (not enough data)</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def is_healthy(self, provider: str) -&gt; bool:\n    \"\"\"\n    Check if a provider is healthy.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        True if provider is healthy or unknown (not enough data)\n    \"\"\"\n    status = self.get_provider_status(provider)\n    return status in (ProviderStatus.HEALTHY, ProviderStatus.UNKNOWN)\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.get_error_rate","title":"<code>get_error_rate(provider)</code>","text":"<p>Get current error rate for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>float</code> <p>Error rate as a float (0.0 to 1.0)</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def get_error_rate(self, provider: str) -&gt; float:\n    \"\"\"\n    Get current error rate for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        Error rate as a float (0.0 to 1.0)\n    \"\"\"\n    metrics = self.get_provider_metrics(provider)\n    return metrics.error_rate\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.get_avg_latency","title":"<code>get_avg_latency(provider)</code>","text":"<p>Get average latency for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average latency in milliseconds</p> Source code in <code>src/data_loader/health.py</code> <pre><code>def get_avg_latency(self, provider: str) -&gt; float:\n    \"\"\"\n    Get average latency for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        Average latency in milliseconds\n    \"\"\"\n    metrics = self.get_provider_metrics(provider)\n    return metrics.avg_latency_ms\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.reset","title":"<code>reset(provider=None)</code>","text":"<p>Reset metrics for a provider or all providers.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Optional provider to reset, or None for all</p> <code>None</code> Source code in <code>src/data_loader/health.py</code> <pre><code>def reset(self, provider: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Reset metrics for a provider or all providers.\n\n    Args:\n        provider: Optional provider to reset, or None for all\n    \"\"\"\n    with self._lock:\n        providers = [provider] if provider else [\"fmp\", \"polygon\", \"fred\"]\n\n        for p in providers:\n            if p in self._history:\n                self._history[p].clear()\n            if p in self._total_counters:\n                self._total_counters[p] = {\n                    \"total\": 0, \"success\": 0, \"failed\": 0,\n                    \"rate_limited\": 0, \"timeout\": 0,\n                }\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.record_success","title":"<code>record_success(provider, endpoint, latency_ms, status_code=200)</code>","text":"<p>Convenience method to record a successful request.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>latency_ms</code> <code>float</code> <p>Request latency</p> required <code>status_code</code> <code>int</code> <p>HTTP status code</p> <code>200</code> Source code in <code>src/data_loader/health.py</code> <pre><code>def record_success(\n    self,\n    provider: str,\n    endpoint: str,\n    latency_ms: float,\n    status_code: int = 200,\n) -&gt; None:\n    \"\"\"\n    Convenience method to record a successful request.\n\n    Args:\n        provider: Provider name\n        endpoint: API endpoint\n        latency_ms: Request latency\n        status_code: HTTP status code\n    \"\"\"\n    self.record_request(\n        provider=provider,\n        endpoint=endpoint,\n        success=True,\n        status_code=status_code,\n        latency_ms=latency_ms,\n    )\n</code></pre>"},{"location":"api/health/#data_loader.health.HealthMonitor.record_failure","title":"<code>record_failure(provider, endpoint, latency_ms, status_code=None, error_type=None)</code>","text":"<p>Convenience method to record a failed request.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>latency_ms</code> <code>float</code> <p>Request latency</p> required <code>status_code</code> <code>Optional[int]</code> <p>HTTP status code (if any)</p> <code>None</code> <code>error_type</code> <code>Optional[str]</code> <p>Type of error</p> <code>None</code> Source code in <code>src/data_loader/health.py</code> <pre><code>def record_failure(\n    self,\n    provider: str,\n    endpoint: str,\n    latency_ms: float,\n    status_code: Optional[int] = None,\n    error_type: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Convenience method to record a failed request.\n\n    Args:\n        provider: Provider name\n        endpoint: API endpoint\n        latency_ms: Request latency\n        status_code: HTTP status code (if any)\n        error_type: Type of error\n    \"\"\"\n    self.record_request(\n        provider=provider,\n        endpoint=endpoint,\n        success=False,\n        status_code=status_code,\n        latency_ms=latency_ms,\n        error_type=error_type,\n    )\n</code></pre>"},{"location":"api/health/#providerstatus","title":"ProviderStatus","text":""},{"location":"api/health/#data_loader.health.ProviderStatus","title":"<code>data_loader.health.ProviderStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Health status for a provider.</p> Source code in <code>src/data_loader/health.py</code> <pre><code>class ProviderStatus(Enum):\n    \"\"\"Health status for a provider.\"\"\"\n\n    HEALTHY = \"healthy\"  # Normal operation\n    DEGRADED = \"degraded\"  # Elevated errors but still functional\n    UNHEALTHY = \"unhealthy\"  # Circuit breaker open or critical failure\n    UNKNOWN = \"unknown\"  # Not enough data\n</code></pre>"},{"location":"api/health/#health-report-structure","title":"Health Report Structure","text":"<pre><code>report = loader.get_api_health_report()\n</code></pre> <p>Returns:</p> <pre><code>{\n    \"operating_mode\": \"LIVE\",\n    \"overall_status\": \"OK\",  # or \"DEGRADED\" or \"FAIL\"\n    \"providers\": {\n        \"fmp\": {\n            \"status\": \"OK\",\n            \"total_requests\": 150,\n            \"successful_requests\": 148,\n            \"failed_requests\": 2,\n            \"error_rate\": 0.013,\n            \"last_success\": \"2025-01-31T10:30:00Z\",\n            \"last_error\": \"2025-01-31T09:15:00Z\"\n        },\n        ...\n    },\n    \"circuit_breakers\": {\n        \"fmp\": {\n            \"state\": \"CLOSED\",\n            \"failure_count\": 0\n        },\n        ...\n    }\n}\n</code></pre>"},{"location":"api/health/#status-levels","title":"Status Levels","text":"Status Meaning <code>OK</code> Provider functioning normally <code>DEGRADED</code> Some errors but still operational <code>FAIL</code> Provider unavailable"},{"location":"api/health/#usage-examples","title":"Usage Examples","text":""},{"location":"api/health/#check-before-requests","title":"Check Before Requests","text":"<pre><code>report = loader.get_api_health_report()\n\nif report['providers']['fmp']['status'] == 'FAIL':\n    print(\"FMP is down, using cached data only\")\n    loader.set_operating_mode(OperatingMode.READ_ONLY)\n</code></pre>"},{"location":"api/health/#monitor-error-rates","title":"Monitor Error Rates","text":"<pre><code>for provider, metrics in report['providers'].items():\n    if metrics['error_rate'] &gt; 0.1:  # More than 10% errors\n        print(f\"\u26a0\ufe0f {provider} has high error rate: {metrics['error_rate']:.1%}\")\n</code></pre>"},{"location":"api/loader/","title":"DataLoader API Reference","text":""},{"location":"api/loader/#data_loader.loader.DataLoader","title":"<code>data_loader.loader.DataLoader</code>","text":"<p>Unified interface for financial data retrieval.</p> <p>Orchestrates all components to provide resilient, cached data access: - Providers: FMP, Polygon, FRED - QoS Router: Provider-specific concurrency limits - Circuit Breaker: Failure isolation and recovery - Retry Handler: Exponential backoff for transient failures - Cache: Filesystem JSON cache with atomic writes - Health Monitor: Request tracking and health reports</p> <p>Operating Modes: - LIVE: Normal operation, fetches from APIs and caches results - READ_ONLY: Only serves from cache, no API calls allowed</p> Usage Source code in <code>src/data_loader/loader.py</code> <pre><code>class DataLoader:\n    \"\"\"\n    Unified interface for financial data retrieval.\n\n    Orchestrates all components to provide resilient, cached data access:\n    - Providers: FMP, Polygon, FRED\n    - QoS Router: Provider-specific concurrency limits\n    - Circuit Breaker: Failure isolation and recovery\n    - Retry Handler: Exponential backoff for transient failures\n    - Cache: Filesystem JSON cache with atomic writes\n    - Health Monitor: Request tracking and health reports\n\n    Operating Modes:\n    - LIVE: Normal operation, fetches from APIs and caches results\n    - READ_ONLY: Only serves from cache, no API calls allowed\n\n    Usage:\n        # Create with default configuration\n        loader = DataLoader()\n\n        # Or with custom config\n        config = load_config()\n        loader = DataLoader(config)\n\n        # Fetch data\n        async with aiohttp.ClientSession() as session:\n            # FMP data\n            response = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n            # Polygon data\n            response = await loader.get_polygon_data(\n                session, \"aggs_daily\",\n                ticker=\"SPY\", from_date=\"2024-01-01\", to_date=\"2024-12-31\"\n            )\n\n            # FRED data\n            response = await loader.get_fred_data(\n                session, \"series\", series_id=\"CPIAUCSL\"\n            )\n\n        # Health check\n        report = loader.get_api_health_report()\n        print(report[\"overall_status\"])\n\n        # Switch modes\n        loader.set_operating_mode(OperatingMode.READ_ONLY)\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Config] = None,\n        qos_router: Optional[QoSSemaphoreRouter] = None,\n        circuit_breaker: Optional[CircuitBreakerManager] = None,\n        retry_handler: Optional[RetryHandler] = None,\n        cache: Optional[CacheManager] = None,\n        health_monitor: Optional[HealthMonitor] = None,\n    ):\n        \"\"\"\n        Initialize DataLoader.\n\n        Args:\n            config: Configuration object. If None, loads from environment.\n            qos_router: Optional QoS router. If None, creates default.\n            circuit_breaker: Optional circuit breaker manager. If None, creates default.\n            retry_handler: Optional retry handler. If None, creates default.\n            cache: Optional cache manager. If None, creates from config.\n            health_monitor: Optional health monitor. If None, creates default.\n        \"\"\"\n        # Load configuration\n        self.config = config or load_config()\n\n        # Initialize components\n        self._qos_router = qos_router or QoSSemaphoreRouter(\n            limits={\n                \"fmp\": self.config.fmp.max_concurrency,\n                \"polygon\": self.config.polygon.max_concurrency,\n                \"fred\": self.config.fred.max_concurrency,\n            }\n        )\n\n        cb_config = CircuitBreakerConfig(\n            error_threshold=self.config.circuit_breaker.error_threshold,\n            recovery_timeout=self.config.circuit_breaker.recovery_timeout,\n            min_requests=self.config.circuit_breaker.min_requests,\n        )\n        self._circuit_breaker = circuit_breaker or CircuitBreakerManager(\n            default_config=cb_config\n        )\n\n        retry_config = RetryConfig(\n            max_retries=self.config.retry.max_retries,\n            base_delay=self.config.retry.base_delay,\n            max_delay=self.config.retry.max_delay,\n            exponential_base=self.config.retry.exponential_base,\n        )\n        self._retry_handler = retry_handler or RetryHandler(retry_config)\n\n        self._cache = cache or CacheManager(\n            base_dir=self.config.cache.base_dir,\n            ttl_days=self.config.cache.ttl_days,\n        )\n\n        self._health_monitor = health_monitor or HealthMonitor()\n\n        # Create HTTP client\n        self._http_client = HttpClient(timeout=self.config.fmp.timeout)\n\n        # Initialize providers\n        self._providers: dict[str, Any] = {}\n        self._init_providers()\n\n        # Operating mode\n        self._operating_mode = self.config.operating_mode\n\n        # Statistics\n        self._stats = DataLoaderStats()\n\n    def _init_providers(self) -&gt; None:\n        \"\"\"Initialize provider instances.\"\"\"\n        self._providers[\"fmp\"] = FMPProvider(\n            config=self.config.fmp,\n            http_client=self._http_client,\n            cache=self._cache,\n            health_monitor=self._health_monitor,\n        )\n\n        self._providers[\"polygon\"] = PolygonProvider(\n            config=self.config.polygon,\n            http_client=self._http_client,\n            cache=self._cache,\n            health_monitor=self._health_monitor,\n        )\n\n        self._providers[\"fred\"] = FREDProvider(\n            config=self.config.fred,\n            http_client=self._http_client,\n            cache=self._cache,\n            health_monitor=self._health_monitor,\n        )\n\n    @property\n    def operating_mode(self) -&gt; OperatingMode:\n        \"\"\"Get current operating mode.\"\"\"\n        return self._operating_mode\n\n    def set_operating_mode(self, mode: OperatingMode) -&gt; None:\n        \"\"\"\n        Set operating mode.\n\n        Args:\n            mode: New operating mode (LIVE or READ_ONLY)\n        \"\"\"\n        self._operating_mode = mode\n\n    async def _fetch_with_resilience(\n        self,\n        session: aiohttp.ClientSession,\n        provider_name: str,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Fetch data with full resilience stack.\n\n        Flow:\n        1. Check operating mode\n        2. Check cache (if enabled)\n        3. In READ_ONLY mode, return cache or raise error\n        4. Check circuit breaker\n        5. Acquire QoS semaphore\n        6. Execute with retry\n        7. Update cache\n        8. Record metrics\n\n        Args:\n            session: aiohttp ClientSession\n            provider_name: Provider name (fmp, polygon, fred)\n            endpoint: API endpoint\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters\n\n        Returns:\n            ProviderResponse with data or error\n\n        Raises:\n            ReadOnlyError: If in READ_ONLY mode and cache miss\n            CircuitBreakerError: If circuit is open\n        \"\"\"\n        start_time = time.perf_counter()\n        self._stats.total_requests += 1\n\n        provider = self._providers.get(provider_name)\n        if not provider:\n            return ProviderResponse(\n                success=False,\n                data=None,\n                provider=provider_name,\n                endpoint=endpoint,\n                error=f\"Unknown provider: {provider_name}\",\n            )\n\n        # Generate cache key\n        cache_key = provider.cache_key(endpoint, **params)\n\n        # Step 1: Check cache first\n        if use_cache:\n            cached = self._cache.get(provider_name, cache_key)\n            if cached and not cached.is_expired:\n                self._stats.cache_hits += 1\n                return ProviderResponse(\n                    success=True,\n                    data=cached.data,\n                    provider=provider_name,\n                    endpoint=endpoint,\n                    from_cache=True,\n                    latency_ms=0.0,\n                )\n            self._stats.cache_misses += 1\n\n        # Step 2: Check operating mode\n        if self._operating_mode == OperatingMode.READ_ONLY:\n            raise ReadOnlyError(provider_name, endpoint)\n\n        # Step 3: Check circuit breaker\n        if not self._circuit_breaker.can_execute(provider_name):\n            self._stats.circuit_breaker_rejections += 1\n            state = self._circuit_breaker.get_state(provider_name)\n            raise CircuitBreakerError(\n                f\"Circuit breaker {state.value} for {provider_name}\",\n                state=state,\n                provider=provider_name,\n            )\n\n        # Step 4: Acquire QoS slot and execute with retry\n        async with self._qos_router.acquire(provider_name):\n            try:\n                self._stats.api_calls += 1\n\n                # Execute with retry handler\n                response = await self._retry_handler.execute(\n                    provider.get, session, endpoint, use_cache=False, **params\n                )\n\n                # Record circuit breaker success\n                self._circuit_breaker.record_success(provider_name)\n                self._stats.api_successes += 1\n\n                # Cache the result (provider.get() was called with use_cache=False\n                # to skip redundant cache check, so we cache here)\n                if use_cache and response.success:\n                    self._cache.set(provider_name, cache_key, response.data)\n\n                return response\n\n            except RetryError as e:\n                # Retries exhausted\n                elapsed_ms = (time.perf_counter() - start_time) * 1000\n                self._circuit_breaker.record_failure(provider_name)\n                self._stats.api_failures += 1\n\n                return ProviderResponse(\n                    success=False,\n                    data=None,\n                    provider=provider_name,\n                    endpoint=endpoint,\n                    latency_ms=elapsed_ms,\n                    error=f\"All retries exhausted: {e.last_exception}\",\n                )\n\n            except Exception as e:\n                # Unexpected error\n                elapsed_ms = (time.perf_counter() - start_time) * 1000\n                self._circuit_breaker.record_failure(provider_name)\n                self._stats.api_failures += 1\n\n                return ProviderResponse(\n                    success=False,\n                    data=None,\n                    provider=provider_name,\n                    endpoint=endpoint,\n                    latency_ms=elapsed_ms,\n                    error=f\"Unexpected error: {e}\",\n                )\n\n    async def get_fmp_data(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Fetch data from FMP provider.\n\n        Supported endpoints:\n        - screener: Stock screening with filters\n        - profile: Company profile\n        - quote: Real-time quote\n        - historical_price: Historical OHLCV\n        - earnings_calendar: Earnings dates\n        - balance_sheet: Balance sheet statements\n        - income_statement: Income statements\n        - cash_flow: Cash flow statements\n        - ratios: Financial ratios\n        - growth: Financial growth metrics\n        - key_metrics: Key financial metrics\n        - insider_trading: Insider trading activity\n        - institutional_ownership: Institutional holdings\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters (e.g., symbol=\"AAPL\")\n\n        Returns:\n            ProviderResponse with data or error\n        \"\"\"\n        return await self._fetch_with_resilience(\n            session, \"fmp\", endpoint, use_cache, **params\n        )\n\n    async def get_polygon_data(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Fetch data from Polygon provider.\n\n        Supported endpoints:\n        - aggs_daily: Daily OHLCV aggregates\n        - trades: Trade data\n        - options_snapshot: Options chain snapshot\n        - market_snapshot: Market-wide snapshot\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters\n\n        Returns:\n            ProviderResponse with data or error\n        \"\"\"\n        return await self._fetch_with_resilience(\n            session, \"polygon\", endpoint, use_cache, **params\n        )\n\n    async def get_fred_data(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Fetch data from FRED provider.\n\n        Supported endpoints:\n        - series: Economic time series data\n        - series_info: Series metadata\n        - releases: Economic releases list\n\n        Supported series (32 total):\n        - Inflation: CPIAUCSL, CPILFESL, PCEPI, PCEPILFE, PPIFIS\n        - Labor: UNRATE, PAYEMS, CIVPART, AHETPI, ICSA, CCSA, JTSJOL\n        - GDP: GDP, GDPC1, GDI, INDPRO, UMCSENT\n        - Housing: CSUSHPINSA, HOUST, PERMIT, HSN1F, EXHOSLUSM495S\n        - Interest Rates: FEDFUNDS, DFF, DGS2, DGS10, DGS30, T10Y2Y, T10Y3M\n        - Money: M2SL, TOTALSL\n        - Financial: VIXCLS\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters (e.g., series_id=\"CPIAUCSL\")\n\n        Returns:\n            ProviderResponse with data or error\n        \"\"\"\n        return await self._fetch_with_resilience(\n            session, \"fred\", endpoint, use_cache, **params\n        )\n\n    async def get_data(\n        self,\n        session: aiohttp.ClientSession,\n        provider: str,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Generic data fetching method.\n\n        Args:\n            session: aiohttp ClientSession\n            provider: Provider name (fmp, polygon, fred)\n            endpoint: API endpoint\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters\n\n        Returns:\n            ProviderResponse with data or error\n        \"\"\"\n        return await self._fetch_with_resilience(\n            session, provider, endpoint, use_cache, **params\n        )\n\n    def get_api_health_report(self) -&gt; dict:\n        \"\"\"\n        Get comprehensive health report.\n\n        Returns:\n            Dictionary with:\n            - timestamp: Report generation time\n            - operating_mode: Current mode\n            - overall_status: Aggregate health status\n            - providers: Per-provider health metrics\n            - circuit_breakers: Circuit breaker states\n            - qos: QoS statistics\n            - loader_stats: DataLoader statistics\n        \"\"\"\n        health_report = self._health_monitor.get_health_report()\n\n        # Add circuit breaker states\n        cb_states = {}\n        for provider in [\"fmp\", \"polygon\", \"fred\"]:\n            cb_states[provider] = self._circuit_breaker.get_stats(provider)\n\n        # Add QoS stats\n        qos_stats = self._qos_router.get_stats()\n\n        return {\n            \"timestamp\": health_report[\"timestamp\"],\n            \"operating_mode\": self._operating_mode.value,\n            \"overall_status\": health_report[\"overall_status\"],\n            \"providers\": health_report[\"providers\"],\n            \"circuit_breakers\": cb_states,\n            \"qos\": qos_stats,\n            \"loader_stats\": self._stats.to_dict(),\n        }\n\n    def get_provider_status(self, provider: str) -&gt; ProviderStatus:\n        \"\"\"\n        Get health status for a specific provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            ProviderStatus enum value\n        \"\"\"\n        return self._health_monitor.get_provider_status(provider)\n\n    def is_provider_healthy(self, provider: str) -&gt; bool:\n        \"\"\"\n        Check if a provider is healthy.\n\n        Considers both health monitor status and circuit breaker state.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            True if provider is healthy\n        \"\"\"\n        health_ok = self._health_monitor.is_healthy(provider)\n        circuit_ok = self._circuit_breaker.is_healthy(provider)\n        return health_ok and circuit_ok\n\n    def get_stats(self) -&gt; DataLoaderStats:\n        \"\"\"Get DataLoader statistics.\"\"\"\n        return self._stats\n\n    def reset_stats(self) -&gt; None:\n        \"\"\"Reset DataLoader statistics.\"\"\"\n        self._stats = DataLoaderStats()\n\n    def reset_circuit_breaker(self, provider: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Reset circuit breaker(s).\n\n        Args:\n            provider: Optional provider to reset, or None for all\n        \"\"\"\n        self._circuit_breaker.reset(provider)\n\n    def reset_health_monitor(self, provider: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Reset health monitor metrics.\n\n        Args:\n            provider: Optional provider to reset, or None for all\n        \"\"\"\n        self._health_monitor.reset(provider)\n\n    def get_supported_endpoints(self, provider: str) -&gt; list[str]:\n        \"\"\"\n        Get supported endpoints for a provider.\n\n        Args:\n            provider: Provider name\n\n        Returns:\n            List of endpoint names\n        \"\"\"\n        prov = self._providers.get(provider)\n        if prov:\n            return prov.get_supported_endpoints()\n        return []\n\n    async def close(self) -&gt; None:\n        \"\"\"Close the DataLoader and release resources.\"\"\"\n        await self._http_client.close()\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader--create-with-default-configuration","title":"Create with default configuration","text":"<p>loader = DataLoader()</p>"},{"location":"api/loader/#data_loader.loader.DataLoader--or-with-custom-config","title":"Or with custom config","text":"<p>config = load_config() loader = DataLoader(config)</p>"},{"location":"api/loader/#data_loader.loader.DataLoader--fetch-data","title":"Fetch data","text":"<p>async with aiohttp.ClientSession() as session:     # FMP data     response = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")</p> <pre><code># Polygon data\nresponse = await loader.get_polygon_data(\n    session, \"aggs_daily\",\n    ticker=\"SPY\", from_date=\"2024-01-01\", to_date=\"2024-12-31\"\n)\n\n# FRED data\nresponse = await loader.get_fred_data(\n    session, \"series\", series_id=\"CPIAUCSL\"\n)\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader--health-check","title":"Health check","text":"<p>report = loader.get_api_health_report() print(report[\"overall_status\"])</p>"},{"location":"api/loader/#data_loader.loader.DataLoader--switch-modes","title":"Switch modes","text":"<p>loader.set_operating_mode(OperatingMode.READ_ONLY)</p>"},{"location":"api/loader/#data_loader.loader.DataLoader.operating_mode","title":"<code>operating_mode</code>  <code>property</code>","text":"<p>Get current operating mode.</p>"},{"location":"api/loader/#data_loader.loader.DataLoader.__init__","title":"<code>__init__(config=None, qos_router=None, circuit_breaker=None, retry_handler=None, cache=None, health_monitor=None)</code>","text":"<p>Initialize DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Config]</code> <p>Configuration object. If None, loads from environment.</p> <code>None</code> <code>qos_router</code> <code>Optional[QoSSemaphoreRouter]</code> <p>Optional QoS router. If None, creates default.</p> <code>None</code> <code>circuit_breaker</code> <code>Optional[CircuitBreakerManager]</code> <p>Optional circuit breaker manager. If None, creates default.</p> <code>None</code> <code>retry_handler</code> <code>Optional[RetryHandler]</code> <p>Optional retry handler. If None, creates default.</p> <code>None</code> <code>cache</code> <code>Optional[CacheManager]</code> <p>Optional cache manager. If None, creates from config.</p> <code>None</code> <code>health_monitor</code> <code>Optional[HealthMonitor]</code> <p>Optional health monitor. If None, creates default.</p> <code>None</code> Source code in <code>src/data_loader/loader.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Config] = None,\n    qos_router: Optional[QoSSemaphoreRouter] = None,\n    circuit_breaker: Optional[CircuitBreakerManager] = None,\n    retry_handler: Optional[RetryHandler] = None,\n    cache: Optional[CacheManager] = None,\n    health_monitor: Optional[HealthMonitor] = None,\n):\n    \"\"\"\n    Initialize DataLoader.\n\n    Args:\n        config: Configuration object. If None, loads from environment.\n        qos_router: Optional QoS router. If None, creates default.\n        circuit_breaker: Optional circuit breaker manager. If None, creates default.\n        retry_handler: Optional retry handler. If None, creates default.\n        cache: Optional cache manager. If None, creates from config.\n        health_monitor: Optional health monitor. If None, creates default.\n    \"\"\"\n    # Load configuration\n    self.config = config or load_config()\n\n    # Initialize components\n    self._qos_router = qos_router or QoSSemaphoreRouter(\n        limits={\n            \"fmp\": self.config.fmp.max_concurrency,\n            \"polygon\": self.config.polygon.max_concurrency,\n            \"fred\": self.config.fred.max_concurrency,\n        }\n    )\n\n    cb_config = CircuitBreakerConfig(\n        error_threshold=self.config.circuit_breaker.error_threshold,\n        recovery_timeout=self.config.circuit_breaker.recovery_timeout,\n        min_requests=self.config.circuit_breaker.min_requests,\n    )\n    self._circuit_breaker = circuit_breaker or CircuitBreakerManager(\n        default_config=cb_config\n    )\n\n    retry_config = RetryConfig(\n        max_retries=self.config.retry.max_retries,\n        base_delay=self.config.retry.base_delay,\n        max_delay=self.config.retry.max_delay,\n        exponential_base=self.config.retry.exponential_base,\n    )\n    self._retry_handler = retry_handler or RetryHandler(retry_config)\n\n    self._cache = cache or CacheManager(\n        base_dir=self.config.cache.base_dir,\n        ttl_days=self.config.cache.ttl_days,\n    )\n\n    self._health_monitor = health_monitor or HealthMonitor()\n\n    # Create HTTP client\n    self._http_client = HttpClient(timeout=self.config.fmp.timeout)\n\n    # Initialize providers\n    self._providers: dict[str, Any] = {}\n    self._init_providers()\n\n    # Operating mode\n    self._operating_mode = self.config.operating_mode\n\n    # Statistics\n    self._stats = DataLoaderStats()\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.set_operating_mode","title":"<code>set_operating_mode(mode)</code>","text":"<p>Set operating mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>OperatingMode</code> <p>New operating mode (LIVE or READ_ONLY)</p> required Source code in <code>src/data_loader/loader.py</code> <pre><code>def set_operating_mode(self, mode: OperatingMode) -&gt; None:\n    \"\"\"\n    Set operating mode.\n\n    Args:\n        mode: New operating mode (LIVE or READ_ONLY)\n    \"\"\"\n    self._operating_mode = mode\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_fmp_data","title":"<code>get_fmp_data(session, endpoint, use_cache=True, **params)</code>  <code>async</code>","text":"<p>Fetch data from FMP provider.</p> <p>Supported endpoints: - screener: Stock screening with filters - profile: Company profile - quote: Real-time quote - historical_price: Historical OHLCV - earnings_calendar: Earnings dates - balance_sheet: Balance sheet statements - income_statement: Income statements - cash_flow: Cash flow statements - ratios: Financial ratios - growth: Financial growth metrics - key_metrics: Key financial metrics - insider_trading: Insider trading activity - institutional_ownership: Institutional holdings</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <code>**params</code> <p>Endpoint-specific parameters (e.g., symbol=\"AAPL\")</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProviderResponse</code> <p>ProviderResponse with data or error</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>async def get_fmp_data(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    use_cache: bool = True,\n    **params,\n) -&gt; ProviderResponse:\n    \"\"\"\n    Fetch data from FMP provider.\n\n    Supported endpoints:\n    - screener: Stock screening with filters\n    - profile: Company profile\n    - quote: Real-time quote\n    - historical_price: Historical OHLCV\n    - earnings_calendar: Earnings dates\n    - balance_sheet: Balance sheet statements\n    - income_statement: Income statements\n    - cash_flow: Cash flow statements\n    - ratios: Financial ratios\n    - growth: Financial growth metrics\n    - key_metrics: Key financial metrics\n    - insider_trading: Insider trading activity\n    - institutional_ownership: Institutional holdings\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name\n        use_cache: Whether to use caching\n        **params: Endpoint-specific parameters (e.g., symbol=\"AAPL\")\n\n    Returns:\n        ProviderResponse with data or error\n    \"\"\"\n    return await self._fetch_with_resilience(\n        session, \"fmp\", endpoint, use_cache, **params\n    )\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_polygon_data","title":"<code>get_polygon_data(session, endpoint, use_cache=True, **params)</code>  <code>async</code>","text":"<p>Fetch data from Polygon provider.</p> <p>Supported endpoints: - aggs_daily: Daily OHLCV aggregates - trades: Trade data - options_snapshot: Options chain snapshot - market_snapshot: Market-wide snapshot</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProviderResponse</code> <p>ProviderResponse with data or error</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>async def get_polygon_data(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    use_cache: bool = True,\n    **params,\n) -&gt; ProviderResponse:\n    \"\"\"\n    Fetch data from Polygon provider.\n\n    Supported endpoints:\n    - aggs_daily: Daily OHLCV aggregates\n    - trades: Trade data\n    - options_snapshot: Options chain snapshot\n    - market_snapshot: Market-wide snapshot\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name\n        use_cache: Whether to use caching\n        **params: Endpoint-specific parameters\n\n    Returns:\n        ProviderResponse with data or error\n    \"\"\"\n    return await self._fetch_with_resilience(\n        session, \"polygon\", endpoint, use_cache, **params\n    )\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_fred_data","title":"<code>get_fred_data(session, endpoint, use_cache=True, **params)</code>  <code>async</code>","text":"<p>Fetch data from FRED provider.</p> <p>Supported endpoints: - series: Economic time series data - series_info: Series metadata - releases: Economic releases list</p> <p>Supported series (32 total): - Inflation: CPIAUCSL, CPILFESL, PCEPI, PCEPILFE, PPIFIS - Labor: UNRATE, PAYEMS, CIVPART, AHETPI, ICSA, CCSA, JTSJOL - GDP: GDP, GDPC1, GDI, INDPRO, UMCSENT - Housing: CSUSHPINSA, HOUST, PERMIT, HSN1F, EXHOSLUSM495S - Interest Rates: FEDFUNDS, DFF, DGS2, DGS10, DGS30, T10Y2Y, T10Y3M - Money: M2SL, TOTALSL - Financial: VIXCLS</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <code>**params</code> <p>Endpoint-specific parameters (e.g., series_id=\"CPIAUCSL\")</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProviderResponse</code> <p>ProviderResponse with data or error</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>async def get_fred_data(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    use_cache: bool = True,\n    **params,\n) -&gt; ProviderResponse:\n    \"\"\"\n    Fetch data from FRED provider.\n\n    Supported endpoints:\n    - series: Economic time series data\n    - series_info: Series metadata\n    - releases: Economic releases list\n\n    Supported series (32 total):\n    - Inflation: CPIAUCSL, CPILFESL, PCEPI, PCEPILFE, PPIFIS\n    - Labor: UNRATE, PAYEMS, CIVPART, AHETPI, ICSA, CCSA, JTSJOL\n    - GDP: GDP, GDPC1, GDI, INDPRO, UMCSENT\n    - Housing: CSUSHPINSA, HOUST, PERMIT, HSN1F, EXHOSLUSM495S\n    - Interest Rates: FEDFUNDS, DFF, DGS2, DGS10, DGS30, T10Y2Y, T10Y3M\n    - Money: M2SL, TOTALSL\n    - Financial: VIXCLS\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name\n        use_cache: Whether to use caching\n        **params: Endpoint-specific parameters (e.g., series_id=\"CPIAUCSL\")\n\n    Returns:\n        ProviderResponse with data or error\n    \"\"\"\n    return await self._fetch_with_resilience(\n        session, \"fred\", endpoint, use_cache, **params\n    )\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_data","title":"<code>get_data(session, provider, endpoint, use_cache=True, **params)</code>  <code>async</code>","text":"<p>Generic data fetching method.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>provider</code> <code>str</code> <p>Provider name (fmp, polygon, fred)</p> required <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProviderResponse</code> <p>ProviderResponse with data or error</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>async def get_data(\n    self,\n    session: aiohttp.ClientSession,\n    provider: str,\n    endpoint: str,\n    use_cache: bool = True,\n    **params,\n) -&gt; ProviderResponse:\n    \"\"\"\n    Generic data fetching method.\n\n    Args:\n        session: aiohttp ClientSession\n        provider: Provider name (fmp, polygon, fred)\n        endpoint: API endpoint\n        use_cache: Whether to use caching\n        **params: Endpoint-specific parameters\n\n    Returns:\n        ProviderResponse with data or error\n    \"\"\"\n    return await self._fetch_with_resilience(\n        session, provider, endpoint, use_cache, **params\n    )\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_api_health_report","title":"<code>get_api_health_report()</code>","text":"<p>Get comprehensive health report.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with:</p> <code>dict</code> <ul> <li>timestamp: Report generation time</li> </ul> <code>dict</code> <ul> <li>operating_mode: Current mode</li> </ul> <code>dict</code> <ul> <li>overall_status: Aggregate health status</li> </ul> <code>dict</code> <ul> <li>providers: Per-provider health metrics</li> </ul> <code>dict</code> <ul> <li>circuit_breakers: Circuit breaker states</li> </ul> <code>dict</code> <ul> <li>qos: QoS statistics</li> </ul> <code>dict</code> <ul> <li>loader_stats: DataLoader statistics</li> </ul> Source code in <code>src/data_loader/loader.py</code> <pre><code>def get_api_health_report(self) -&gt; dict:\n    \"\"\"\n    Get comprehensive health report.\n\n    Returns:\n        Dictionary with:\n        - timestamp: Report generation time\n        - operating_mode: Current mode\n        - overall_status: Aggregate health status\n        - providers: Per-provider health metrics\n        - circuit_breakers: Circuit breaker states\n        - qos: QoS statistics\n        - loader_stats: DataLoader statistics\n    \"\"\"\n    health_report = self._health_monitor.get_health_report()\n\n    # Add circuit breaker states\n    cb_states = {}\n    for provider in [\"fmp\", \"polygon\", \"fred\"]:\n        cb_states[provider] = self._circuit_breaker.get_stats(provider)\n\n    # Add QoS stats\n    qos_stats = self._qos_router.get_stats()\n\n    return {\n        \"timestamp\": health_report[\"timestamp\"],\n        \"operating_mode\": self._operating_mode.value,\n        \"overall_status\": health_report[\"overall_status\"],\n        \"providers\": health_report[\"providers\"],\n        \"circuit_breakers\": cb_states,\n        \"qos\": qos_stats,\n        \"loader_stats\": self._stats.to_dict(),\n    }\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_provider_status","title":"<code>get_provider_status(provider)</code>","text":"<p>Get health status for a specific provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>ProviderStatus</code> <p>ProviderStatus enum value</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def get_provider_status(self, provider: str) -&gt; ProviderStatus:\n    \"\"\"\n    Get health status for a specific provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        ProviderStatus enum value\n    \"\"\"\n    return self._health_monitor.get_provider_status(provider)\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.is_provider_healthy","title":"<code>is_provider_healthy(provider)</code>","text":"<p>Check if a provider is healthy.</p> <p>Considers both health monitor status and circuit breaker state.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if provider is healthy</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def is_provider_healthy(self, provider: str) -&gt; bool:\n    \"\"\"\n    Check if a provider is healthy.\n\n    Considers both health monitor status and circuit breaker state.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        True if provider is healthy\n    \"\"\"\n    health_ok = self._health_monitor.is_healthy(provider)\n    circuit_ok = self._circuit_breaker.is_healthy(provider)\n    return health_ok and circuit_ok\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_stats","title":"<code>get_stats()</code>","text":"<p>Get DataLoader statistics.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def get_stats(self) -&gt; DataLoaderStats:\n    \"\"\"Get DataLoader statistics.\"\"\"\n    return self._stats\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.reset_stats","title":"<code>reset_stats()</code>","text":"<p>Reset DataLoader statistics.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def reset_stats(self) -&gt; None:\n    \"\"\"Reset DataLoader statistics.\"\"\"\n    self._stats = DataLoaderStats()\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.reset_circuit_breaker","title":"<code>reset_circuit_breaker(provider=None)</code>","text":"<p>Reset circuit breaker(s).</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Optional provider to reset, or None for all</p> <code>None</code> Source code in <code>src/data_loader/loader.py</code> <pre><code>def reset_circuit_breaker(self, provider: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Reset circuit breaker(s).\n\n    Args:\n        provider: Optional provider to reset, or None for all\n    \"\"\"\n    self._circuit_breaker.reset(provider)\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.reset_health_monitor","title":"<code>reset_health_monitor(provider=None)</code>","text":"<p>Reset health monitor metrics.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>Optional[str]</code> <p>Optional provider to reset, or None for all</p> <code>None</code> Source code in <code>src/data_loader/loader.py</code> <pre><code>def reset_health_monitor(self, provider: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Reset health monitor metrics.\n\n    Args:\n        provider: Optional provider to reset, or None for all\n    \"\"\"\n    self._health_monitor.reset(provider)\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.get_supported_endpoints","title":"<code>get_supported_endpoints(provider)</code>","text":"<p>Get supported endpoints for a provider.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of endpoint names</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def get_supported_endpoints(self, provider: str) -&gt; list[str]:\n    \"\"\"\n    Get supported endpoints for a provider.\n\n    Args:\n        provider: Provider name\n\n    Returns:\n        List of endpoint names\n    \"\"\"\n    prov = self._providers.get(provider)\n    if prov:\n        return prov.get_supported_endpoints()\n    return []\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoader.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the DataLoader and release resources.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>async def close(self) -&gt; None:\n    \"\"\"Close the DataLoader and release resources.\"\"\"\n    await self._http_client.close()\n</code></pre>"},{"location":"api/loader/#dataloaderstats","title":"DataLoaderStats","text":""},{"location":"api/loader/#data_loader.loader.DataLoaderStats","title":"<code>data_loader.loader.DataLoaderStats</code>  <code>dataclass</code>","text":"<p>Statistics for the DataLoader.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>@dataclass\nclass DataLoaderStats:\n    \"\"\"Statistics for the DataLoader.\"\"\"\n\n    total_requests: int = 0\n    cache_hits: int = 0\n    cache_misses: int = 0\n    api_calls: int = 0\n    api_successes: int = 0\n    api_failures: int = 0\n    circuit_breaker_rejections: int = 0\n\n    @property\n    def cache_hit_rate(self) -&gt; float:\n        \"\"\"Calculate cache hit rate.\"\"\"\n        if self.total_requests == 0:\n            return 0.0\n        return self.cache_hits / self.total_requests\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"total_requests\": self.total_requests,\n            \"cache_hits\": self.cache_hits,\n            \"cache_misses\": self.cache_misses,\n            \"api_calls\": self.api_calls,\n            \"api_successes\": self.api_successes,\n            \"api_failures\": self.api_failures,\n            \"circuit_breaker_rejections\": self.circuit_breaker_rejections,\n            \"cache_hit_rate\": round(self.cache_hit_rate, 4),\n        }\n</code></pre>"},{"location":"api/loader/#data_loader.loader.DataLoaderStats.cache_hit_rate","title":"<code>cache_hit_rate</code>  <code>property</code>","text":"<p>Calculate cache hit rate.</p>"},{"location":"api/loader/#data_loader.loader.DataLoaderStats.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"total_requests\": self.total_requests,\n        \"cache_hits\": self.cache_hits,\n        \"cache_misses\": self.cache_misses,\n        \"api_calls\": self.api_calls,\n        \"api_successes\": self.api_successes,\n        \"api_failures\": self.api_failures,\n        \"circuit_breaker_rejections\": self.circuit_breaker_rejections,\n        \"cache_hit_rate\": round(self.cache_hit_rate, 4),\n    }\n</code></pre>"},{"location":"api/loader/#readonlyerror","title":"ReadOnlyError","text":""},{"location":"api/loader/#data_loader.loader.ReadOnlyError","title":"<code>data_loader.loader.ReadOnlyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when API call attempted in READ_ONLY mode with cache miss.</p> Source code in <code>src/data_loader/loader.py</code> <pre><code>class ReadOnlyError(Exception):\n    \"\"\"Raised when API call attempted in READ_ONLY mode with cache miss.\"\"\"\n\n    def __init__(self, provider: str, endpoint: str):\n        self.provider = provider\n        self.endpoint = endpoint\n        super().__init__(\n            f\"READ_ONLY mode: Cannot fetch from {provider}/{endpoint} - not in cache\"\n        )\n</code></pre>"},{"location":"api/loader/#operatingmode","title":"OperatingMode","text":""},{"location":"api/loader/#data_loader.config.OperatingMode","title":"<code>data_loader.config.OperatingMode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Operating mode for the DataLoader.</p> Source code in <code>src/data_loader/config.py</code> <pre><code>class OperatingMode(Enum):\n    \"\"\"Operating mode for the DataLoader.\"\"\"\n\n    LIVE = \"LIVE\"  # Fetch from APIs and cache\n    READ_ONLY = \"READ_ONLY\"  # Only read from cache, no API calls\n</code></pre>"},{"location":"api/loader/#usage-examples","title":"Usage Examples","text":""},{"location":"api/loader/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nimport aiohttp\nfrom data_loader import DataLoader\n\nasync def main():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/loader/#multiple-providers","title":"Multiple Providers","text":"<pre><code>async def multi_provider():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # FMP\n        profile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n        # Polygon\n        aggs = await loader.get_polygon_data(\n            session, \"aggs_daily\",\n            symbol=\"SPY\", start=\"2025-01-01\", end=\"2025-01-31\"\n        )\n\n        # FRED\n        cpi = await loader.get_fred_data(session, \"series\", series_id=\"CPIAUCSL\")\n\n        return profile, aggs, cpi\n</code></pre>"},{"location":"api/loader/#operating-mode-control","title":"Operating Mode Control","text":"<pre><code>from data_loader import DataLoader, OperatingMode\n\nloader = DataLoader()\n\n# Switch to READ_ONLY\nloader.set_operating_mode(OperatingMode.READ_ONLY)\n\n# Check current mode\nmode = loader.get_operating_mode()\nprint(f\"Current mode: {mode}\")\n</code></pre>"},{"location":"api/loader/#health-monitoring","title":"Health Monitoring","text":"<pre><code>loader = DataLoader()\n\n# ... make some requests ...\n\nreport = loader.get_api_health_report()\nprint(f\"Overall status: {report['overall_status']}\")\n\nfor provider, metrics in report['providers'].items():\n    print(f\"{provider}: {metrics['status']}\")\n</code></pre>"},{"location":"api/providers/","title":"Providers API Reference","text":""},{"location":"api/providers/#base-provider","title":"Base Provider","text":""},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider","title":"<code>data_loader.providers.base.BaseDataProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data providers.</p> <p>All providers (FMP, Polygon, FRED) inherit from this class and implement: - fetch(): Async data fetching from the API - normalize(): Response normalization to consistent format - cache_key(): Cache key generation</p> <p>Features: - Automatic caching with TTL - Health monitoring integration - Standardized response format - Error handling</p> Usage <p>class FMPProvider(BaseDataProvider):     @property     def provider_name(self) -&gt; str:         return \"fmp\"</p> <pre><code>async def fetch(self, session, endpoint, **params):\n    # Implementation\n    pass\n\ndef normalize(self, data, endpoint):\n    # Implementation\n    pass\n\ndef cache_key(self, endpoint, **params):\n    # Implementation\n    pass\n</code></pre> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>class BaseDataProvider(ABC):\n    \"\"\"\n    Abstract base class for data providers.\n\n    All providers (FMP, Polygon, FRED) inherit from this class and implement:\n    - fetch(): Async data fetching from the API\n    - normalize(): Response normalization to consistent format\n    - cache_key(): Cache key generation\n\n    Features:\n    - Automatic caching with TTL\n    - Health monitoring integration\n    - Standardized response format\n    - Error handling\n\n    Usage:\n        class FMPProvider(BaseDataProvider):\n            @property\n            def provider_name(self) -&gt; str:\n                return \"fmp\"\n\n            async def fetch(self, session, endpoint, **params):\n                # Implementation\n                pass\n\n            def normalize(self, data, endpoint):\n                # Implementation\n                pass\n\n            def cache_key(self, endpoint, **params):\n                # Implementation\n                pass\n    \"\"\"\n\n    def __init__(\n        self,\n        config: ProviderConfig,\n        http_client: HttpClient,\n        cache: CacheManager,\n        health_monitor: HealthMonitor,\n    ):\n        \"\"\"\n        Initialize base provider.\n\n        Args:\n            config: Provider-specific configuration\n            http_client: HTTP client for API requests\n            cache: Cache manager for caching responses\n            health_monitor: Health monitor for tracking metrics\n        \"\"\"\n        self.config = config\n        self.http_client = http_client\n        self.cache = cache\n        self.health_monitor = health_monitor\n\n    @property\n    @abstractmethod\n    def provider_name(self) -&gt; str:\n        \"\"\"Get the provider name (fmp, polygon, fred).\"\"\"\n        pass\n\n    @property\n    def base_url(self) -&gt; str:\n        \"\"\"Get the base URL for API requests.\"\"\"\n        return self.config.base_url\n\n    @property\n    def api_key(self) -&gt; str:\n        \"\"\"Get the API key.\"\"\"\n        return self.config.api_key\n\n    @abstractmethod\n    async def fetch(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        **params,\n    ) -&gt; HttpResponse:\n        \"\"\"\n        Fetch data from the API.\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint to call\n            **params: Endpoint-specific parameters\n\n        Returns:\n            HttpResponse with raw API response\n\n        Raises:\n            HttpError: For HTTP-related errors\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def normalize(self, data: Any, endpoint: str) -&gt; Any:\n        \"\"\"\n        Normalize API response to consistent format.\n\n        Args:\n            data: Raw API response data\n            endpoint: Endpoint that was called\n\n        Returns:\n            Normalized data structure\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def cache_key(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Generate cache key for a request.\n\n        Args:\n            endpoint: API endpoint\n            **params: Request parameters\n\n        Returns:\n            Unique cache key string\n        \"\"\"\n        pass\n\n    def _generate_cache_key(self, prefix: str, **params) -&gt; str:\n        \"\"\"\n        Generate a cache key from prefix and parameters.\n\n        Helper method for subclasses to generate consistent cache keys.\n\n        Args:\n            prefix: Key prefix (usually endpoint name)\n            **params: Parameters to include in key\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Sort params for consistent ordering\n        sorted_params = sorted(params.items())\n        param_str = \"_\".join(f\"{k}={v}\" for k, v in sorted_params if v is not None)\n\n        if param_str:\n            key = f\"{prefix}_{param_str}\"\n        else:\n            key = prefix\n\n        # Hash if too long\n        if len(key) &gt; 200:\n            hash_val = hashlib.md5(key.encode()).hexdigest()[:16]\n            key = f\"{prefix}_{hash_val}\"\n\n        return key\n\n    async def get(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        use_cache: bool = True,\n        **params,\n    ) -&gt; ProviderResponse:\n        \"\"\"\n        Get data from provider with caching and health tracking.\n\n        This is the main entry point for fetching data. It handles:\n        1. Cache lookup (if enabled)\n        2. API fetch (if not cached or cache disabled)\n        3. Response normalization\n        4. Cache storage\n        5. Health metric recording\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint to call\n            use_cache: Whether to use caching\n            **params: Endpoint-specific parameters\n\n        Returns:\n            ProviderResponse with data or error\n        \"\"\"\n        start_time = time.perf_counter()\n        cache_key = self.cache_key(endpoint, **params)\n\n        # Try cache first\n        if use_cache:\n            cached = self.cache.get(self.provider_name, cache_key)\n            if cached and not cached.is_expired:\n                return ProviderResponse(\n                    success=True,\n                    data=cached.data,\n                    provider=self.provider_name,\n                    endpoint=endpoint,\n                    from_cache=True,\n                    latency_ms=0.0,\n                )\n\n        # Fetch from API\n        try:\n            response = await self.fetch(session, endpoint, **params)\n            elapsed_ms = (time.perf_counter() - start_time) * 1000\n\n            # Normalize response\n            normalized_data = self.normalize(response.data, endpoint)\n\n            # Cache the result\n            if use_cache:\n                self.cache.set(self.provider_name, cache_key, normalized_data)\n\n            # Record success\n            self.health_monitor.record_success(\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                status_code=response.status,\n            )\n\n            return ProviderResponse(\n                success=True,\n                data=normalized_data,\n                provider=self.provider_name,\n                endpoint=endpoint,\n                from_cache=False,\n                latency_ms=elapsed_ms,\n                raw_response=response.data,\n            )\n\n        except RateLimitError as e:\n            elapsed_ms = (time.perf_counter() - start_time) * 1000\n            self.health_monitor.record_failure(\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                status_code=429,\n                error_type=\"rate_limit\",\n            )\n            return ProviderResponse(\n                success=False,\n                data=None,\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                error=f\"Rate limit exceeded. Retry after: {e.retry_after}s\",\n            )\n\n        except HttpError as e:\n            elapsed_ms = (time.perf_counter() - start_time) * 1000\n            self.health_monitor.record_failure(\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                status_code=e.status_code,\n                error_type=type(e).__name__.lower(),\n            )\n            return ProviderResponse(\n                success=False,\n                data=None,\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                error=str(e),\n            )\n\n        except Exception as e:\n            elapsed_ms = (time.perf_counter() - start_time) * 1000\n            self.health_monitor.record_failure(\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                error_type=\"unexpected\",\n            )\n            return ProviderResponse(\n                success=False,\n                data=None,\n                provider=self.provider_name,\n                endpoint=endpoint,\n                latency_ms=elapsed_ms,\n                error=f\"Unexpected error: {e}\",\n            )\n\n    def get_supported_endpoints(self) -&gt; list[str]:\n        \"\"\"\n        Get list of supported endpoints.\n\n        Override in subclass to return actual endpoints.\n\n        Returns:\n            List of endpoint names\n        \"\"\"\n        return []\n\n    def validate_endpoint(self, endpoint: str) -&gt; bool:\n        \"\"\"\n        Validate that an endpoint is supported.\n\n        Args:\n            endpoint: Endpoint name to validate\n\n        Returns:\n            True if endpoint is supported\n        \"\"\"\n        supported = self.get_supported_endpoints()\n        return not supported or endpoint in supported\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.provider_name","title":"<code>provider_name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get the provider name (fmp, polygon, fred).</p>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.base_url","title":"<code>base_url</code>  <code>property</code>","text":"<p>Get the base URL for API requests.</p>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.api_key","title":"<code>api_key</code>  <code>property</code>","text":"<p>Get the API key.</p>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.__init__","title":"<code>__init__(config, http_client, cache, health_monitor)</code>","text":"<p>Initialize base provider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ProviderConfig</code> <p>Provider-specific configuration</p> required <code>http_client</code> <code>HttpClient</code> <p>HTTP client for API requests</p> required <code>cache</code> <code>CacheManager</code> <p>Cache manager for caching responses</p> required <code>health_monitor</code> <code>HealthMonitor</code> <p>Health monitor for tracking metrics</p> required Source code in <code>src/data_loader/providers/base.py</code> <pre><code>def __init__(\n    self,\n    config: ProviderConfig,\n    http_client: HttpClient,\n    cache: CacheManager,\n    health_monitor: HealthMonitor,\n):\n    \"\"\"\n    Initialize base provider.\n\n    Args:\n        config: Provider-specific configuration\n        http_client: HTTP client for API requests\n        cache: Cache manager for caching responses\n        health_monitor: Health monitor for tracking metrics\n    \"\"\"\n    self.config = config\n    self.http_client = http_client\n    self.cache = cache\n    self.health_monitor = health_monitor\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.fetch","title":"<code>fetch(session, endpoint, **params)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Fetch data from the API.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint to call</p> required <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>HttpResponse</code> <p>HttpResponse with raw API response</p> <p>Raises:</p> Type Description <code>HttpError</code> <p>For HTTP-related errors</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>@abstractmethod\nasync def fetch(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    **params,\n) -&gt; HttpResponse:\n    \"\"\"\n    Fetch data from the API.\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint to call\n        **params: Endpoint-specific parameters\n\n    Returns:\n        HttpResponse with raw API response\n\n    Raises:\n        HttpError: For HTTP-related errors\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.normalize","title":"<code>normalize(data, endpoint)</code>  <code>abstractmethod</code>","text":"<p>Normalize API response to consistent format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Raw API response data</p> required <code>endpoint</code> <code>str</code> <p>Endpoint that was called</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Normalized data structure</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>@abstractmethod\ndef normalize(self, data: Any, endpoint: str) -&gt; Any:\n    \"\"\"\n    Normalize API response to consistent format.\n\n    Args:\n        data: Raw API response data\n        endpoint: Endpoint that was called\n\n    Returns:\n        Normalized data structure\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.cache_key","title":"<code>cache_key(endpoint, **params)</code>  <code>abstractmethod</code>","text":"<p>Generate cache key for a request.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>**params</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Unique cache key string</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>@abstractmethod\ndef cache_key(self, endpoint: str, **params) -&gt; str:\n    \"\"\"\n    Generate cache key for a request.\n\n    Args:\n        endpoint: API endpoint\n        **params: Request parameters\n\n    Returns:\n        Unique cache key string\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.get","title":"<code>get(session, endpoint, use_cache=True, **params)</code>  <code>async</code>","text":"<p>Get data from provider with caching and health tracking.</p> <p>This is the main entry point for fetching data. It handles: 1. Cache lookup (if enabled) 2. API fetch (if not cached or cache disabled) 3. Response normalization 4. Cache storage 5. Health metric recording</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint to call</p> required <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ProviderResponse</code> <p>ProviderResponse with data or error</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>async def get(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    use_cache: bool = True,\n    **params,\n) -&gt; ProviderResponse:\n    \"\"\"\n    Get data from provider with caching and health tracking.\n\n    This is the main entry point for fetching data. It handles:\n    1. Cache lookup (if enabled)\n    2. API fetch (if not cached or cache disabled)\n    3. Response normalization\n    4. Cache storage\n    5. Health metric recording\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint to call\n        use_cache: Whether to use caching\n        **params: Endpoint-specific parameters\n\n    Returns:\n        ProviderResponse with data or error\n    \"\"\"\n    start_time = time.perf_counter()\n    cache_key = self.cache_key(endpoint, **params)\n\n    # Try cache first\n    if use_cache:\n        cached = self.cache.get(self.provider_name, cache_key)\n        if cached and not cached.is_expired:\n            return ProviderResponse(\n                success=True,\n                data=cached.data,\n                provider=self.provider_name,\n                endpoint=endpoint,\n                from_cache=True,\n                latency_ms=0.0,\n            )\n\n    # Fetch from API\n    try:\n        response = await self.fetch(session, endpoint, **params)\n        elapsed_ms = (time.perf_counter() - start_time) * 1000\n\n        # Normalize response\n        normalized_data = self.normalize(response.data, endpoint)\n\n        # Cache the result\n        if use_cache:\n            self.cache.set(self.provider_name, cache_key, normalized_data)\n\n        # Record success\n        self.health_monitor.record_success(\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            status_code=response.status,\n        )\n\n        return ProviderResponse(\n            success=True,\n            data=normalized_data,\n            provider=self.provider_name,\n            endpoint=endpoint,\n            from_cache=False,\n            latency_ms=elapsed_ms,\n            raw_response=response.data,\n        )\n\n    except RateLimitError as e:\n        elapsed_ms = (time.perf_counter() - start_time) * 1000\n        self.health_monitor.record_failure(\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            status_code=429,\n            error_type=\"rate_limit\",\n        )\n        return ProviderResponse(\n            success=False,\n            data=None,\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            error=f\"Rate limit exceeded. Retry after: {e.retry_after}s\",\n        )\n\n    except HttpError as e:\n        elapsed_ms = (time.perf_counter() - start_time) * 1000\n        self.health_monitor.record_failure(\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            status_code=e.status_code,\n            error_type=type(e).__name__.lower(),\n        )\n        return ProviderResponse(\n            success=False,\n            data=None,\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            error=str(e),\n        )\n\n    except Exception as e:\n        elapsed_ms = (time.perf_counter() - start_time) * 1000\n        self.health_monitor.record_failure(\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            error_type=\"unexpected\",\n        )\n        return ProviderResponse(\n            success=False,\n            data=None,\n            provider=self.provider_name,\n            endpoint=endpoint,\n            latency_ms=elapsed_ms,\n            error=f\"Unexpected error: {e}\",\n        )\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.get_supported_endpoints","title":"<code>get_supported_endpoints()</code>","text":"<p>Get list of supported endpoints.</p> <p>Override in subclass to return actual endpoints.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of endpoint names</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>def get_supported_endpoints(self) -&gt; list[str]:\n    \"\"\"\n    Get list of supported endpoints.\n\n    Override in subclass to return actual endpoints.\n\n    Returns:\n        List of endpoint names\n    \"\"\"\n    return []\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.BaseDataProvider.validate_endpoint","title":"<code>validate_endpoint(endpoint)</code>","text":"<p>Validate that an endpoint is supported.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>Endpoint name to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if endpoint is supported</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>def validate_endpoint(self, endpoint: str) -&gt; bool:\n    \"\"\"\n    Validate that an endpoint is supported.\n\n    Args:\n        endpoint: Endpoint name to validate\n\n    Returns:\n        True if endpoint is supported\n    \"\"\"\n    supported = self.get_supported_endpoints()\n    return not supported or endpoint in supported\n</code></pre>"},{"location":"api/providers/#providerresponse","title":"ProviderResponse","text":""},{"location":"api/providers/#data_loader.providers.base.ProviderResponse","title":"<code>data_loader.providers.base.ProviderResponse</code>  <code>dataclass</code>","text":"<p>Standardized response from a data provider.</p> <p>Attributes:</p> Name Type Description <code>success</code> <code>bool</code> <p>Whether the request succeeded</p> <code>data</code> <code>Any</code> <p>The response data (normalized)</p> <code>provider</code> <code>str</code> <p>Provider name</p> <code>endpoint</code> <code>str</code> <p>API endpoint called</p> <code>from_cache</code> <code>bool</code> <p>Whether data was served from cache</p> <code>latency_ms</code> <code>float</code> <p>Request latency in milliseconds</p> <code>error</code> <code>Optional[str]</code> <p>Error message if failed</p> <code>raw_response</code> <code>Optional[Any]</code> <p>Original response (for debugging)</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>@dataclass\nclass ProviderResponse:\n    \"\"\"\n    Standardized response from a data provider.\n\n    Attributes:\n        success: Whether the request succeeded\n        data: The response data (normalized)\n        provider: Provider name\n        endpoint: API endpoint called\n        from_cache: Whether data was served from cache\n        latency_ms: Request latency in milliseconds\n        error: Error message if failed\n        raw_response: Original response (for debugging)\n    \"\"\"\n\n    success: bool\n    data: Any\n    provider: str\n    endpoint: str\n    from_cache: bool = False\n    latency_ms: float = 0.0\n    error: Optional[str] = None\n    raw_response: Optional[Any] = None\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"success\": self.success,\n            \"data\": self.data,\n            \"provider\": self.provider,\n            \"endpoint\": self.endpoint,\n            \"from_cache\": self.from_cache,\n            \"latency_ms\": round(self.latency_ms, 2),\n            \"error\": self.error,\n        }\n</code></pre>"},{"location":"api/providers/#data_loader.providers.base.ProviderResponse.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary.</p> Source code in <code>src/data_loader/providers/base.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"success\": self.success,\n        \"data\": self.data,\n        \"provider\": self.provider,\n        \"endpoint\": self.endpoint,\n        \"from_cache\": self.from_cache,\n        \"latency_ms\": round(self.latency_ms, 2),\n        \"error\": self.error,\n    }\n</code></pre>"},{"location":"api/providers/#fmp-provider","title":"FMP Provider","text":""},{"location":"api/providers/#data_loader.providers.fmp.FMPProvider","title":"<code>data_loader.providers.fmp.FMPProvider</code>","text":"<p>               Bases: <code>BaseDataProvider</code></p> <p>FMP (Financial Modeling Prep) data provider.</p> <p>Supports 13 endpoints: - screener: Stock screener - profile: Company profile - quote: Real-time quote - historical_price: Historical daily prices - earnings_calendar: Earnings announcements - balance_sheet: Balance sheet statements - income_statement: Income statements - cash_flow: Cash flow statements - ratios: Financial ratios - growth: Financial growth metrics - key_metrics: Key financial metrics - insider_trading: Insider transactions - institutional_ownership: Institutional holdings</p> Usage <p>provider = FMPProvider(config, http_client, cache, health_monitor) async with aiohttp.ClientSession() as session:     response = await provider.get(session, \"profile\", symbol=\"AAPL\")     print(response.data)</p> Source code in <code>src/data_loader/providers/fmp.py</code> <pre><code>class FMPProvider(BaseDataProvider):\n    \"\"\"\n    FMP (Financial Modeling Prep) data provider.\n\n    Supports 13 endpoints:\n    - screener: Stock screener\n    - profile: Company profile\n    - quote: Real-time quote\n    - historical_price: Historical daily prices\n    - earnings_calendar: Earnings announcements\n    - balance_sheet: Balance sheet statements\n    - income_statement: Income statements\n    - cash_flow: Cash flow statements\n    - ratios: Financial ratios\n    - growth: Financial growth metrics\n    - key_metrics: Key financial metrics\n    - insider_trading: Insider transactions\n    - institutional_ownership: Institutional holdings\n\n    Usage:\n        provider = FMPProvider(config, http_client, cache, health_monitor)\n        async with aiohttp.ClientSession() as session:\n            response = await provider.get(session, \"profile\", symbol=\"AAPL\")\n            print(response.data)\n    \"\"\"\n\n    # Endpoint configurations (updated to new /stable/ API - August 2025)\n    ENDPOINTS = {\n        \"screener\": {\n            \"path\": \"/stable/company-screener\",\n            \"params\": [\"marketCapMoreThan\", \"marketCapLowerThan\", \"priceMoreThan\",\n                      \"priceLowerThan\", \"betaMoreThan\", \"betaLowerThan\",\n                      \"volumeMoreThan\", \"volumeLowerThan\", \"dividendMoreThan\",\n                      \"dividendLowerThan\", \"isEtf\", \"isActivelyTrading\",\n                      \"sector\", \"industry\", \"country\", \"exchange\", \"limit\"],\n        },\n        \"profile\": {\n            \"path\": \"/stable/profile\",\n            \"params\": [\"symbol\"],\n        },\n        \"quote\": {\n            \"path\": \"/stable/quote\",\n            \"params\": [\"symbol\"],\n        },\n        \"historical_price\": {\n            \"path\": \"/stable/historical-price-eod/full\",\n            \"params\": [\"symbol\", \"from\", \"to\"],\n        },\n        \"earnings_calendar\": {\n            \"path\": \"/stable/earnings-calendar\",\n            \"params\": [\"symbol\", \"from\", \"to\"],\n        },\n        \"balance_sheet\": {\n            \"path\": \"/stable/balance-sheet-statement\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"income_statement\": {\n            \"path\": \"/stable/income-statement\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"cash_flow\": {\n            \"path\": \"/stable/cash-flow-statement\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"ratios\": {\n            \"path\": \"/stable/ratios\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"growth\": {\n            \"path\": \"/stable/financial-growth\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"key_metrics\": {\n            \"path\": \"/stable/key-metrics\",\n            \"params\": [\"symbol\", \"period\", \"limit\"],\n        },\n        \"insider_trading\": {\n            \"path\": \"/stable/insider-trading/search\",\n            \"params\": [\"symbol\", \"page\", \"limit\"],\n        },\n        \"institutional_ownership\": {\n            \"path\": \"/stable/institutional-ownership/latest\",\n            \"params\": [\"symbol\"],\n        },\n    }\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"fmp\"\n\n    def get_supported_endpoints(self) -&gt; list[str]:\n        return list(self.ENDPOINTS.keys())\n\n    def _build_url(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Build the full URL for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: Parameters including symbol if needed\n\n        Returns:\n            Full URL string\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint)\n        if not endpoint_config:\n            raise ValueError(f\"Unknown endpoint: {endpoint}\")\n\n        path = endpoint_config[\"path\"]\n\n        # Replace path parameters (e.g., {symbol})\n        if \"{symbol}\" in path:\n            symbol = params.get(\"symbol\")\n            if not symbol:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'symbol' parameter\")\n            path = path.replace(\"{symbol}\", symbol)\n\n        return f\"{self.base_url}{path}\"\n\n    def _build_params(self, endpoint: str, **params) -&gt; dict:\n        \"\"\"\n        Build query parameters for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: All provided parameters\n\n        Returns:\n            Dictionary of query parameters\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint, {})\n        allowed_params = endpoint_config.get(\"params\", [])\n\n        # Always include API key\n        query_params = {\"apikey\": self.api_key}\n\n        # Add allowed parameters\n        for param_name in allowed_params:\n            if param_name in params and params[param_name] is not None:\n                query_params[param_name] = params[param_name]\n\n        return query_params\n\n    async def fetch(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        **params,\n    ) -&gt; HttpResponse:\n        \"\"\"\n        Fetch data from FMP API.\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name\n            **params: Endpoint-specific parameters\n\n        Returns:\n            HttpResponse with raw API data\n\n        Raises:\n            ValueError: If endpoint is invalid or required params missing\n            HttpError: For HTTP-related errors\n        \"\"\"\n        if not self.validate_endpoint(endpoint):\n            raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n        url = self._build_url(endpoint, **params)\n        query_params = self._build_params(endpoint, **params)\n\n        return await self.http_client.get(\n            session,\n            url,\n            params=query_params,\n            timeout=self.config.timeout,\n        )\n\n    def normalize(self, data: Any, endpoint: str) -&gt; Any:\n        \"\"\"\n        Normalize FMP API response.\n\n        FMP responses vary by endpoint:\n        - Most return a list of objects\n        - Some wrap data in a dict\n        - Error responses have an \"Error Message\" field\n\n        Args:\n            data: Raw API response\n            endpoint: Endpoint that was called\n\n        Returns:\n            Normalized data structure\n        \"\"\"\n        # Check for error response\n        if isinstance(data, dict) and \"Error Message\" in data:\n            return {\"error\": data[\"Error Message\"], \"data\": None}\n\n        # Normalize based on endpoint type\n        if endpoint == \"historical_price\":\n            # Historical prices come wrapped in {\"symbol\": ..., \"historical\": [...]}\n            if isinstance(data, dict) and \"historical\" in data:\n                return {\n                    \"symbol\": data.get(\"symbol\"),\n                    \"historical\": data.get(\"historical\", []),\n                }\n            return data\n\n        if endpoint == \"profile\":\n            # Profile returns a list with single item, extract it\n            if isinstance(data, list) and len(data) == 1:\n                return data[0]\n            return data\n\n        if endpoint == \"quote\":\n            # Quote returns a list with single item, extract it\n            if isinstance(data, list) and len(data) == 1:\n                return data[0]\n            return data\n\n        # Default: return as-is (most endpoints return lists)\n        return data\n\n    def cache_key(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Generate cache key for FMP request.\n\n        Cache key format: endpoint_param1=value1_param2=value2\n\n        Args:\n            endpoint: API endpoint\n            **params: Request parameters\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Exclude API key from cache key\n        cache_params = {k: v for k, v in params.items() if k != \"apikey\"}\n        return self._generate_cache_key(endpoint, **cache_params)\n\n    def validate_symbol(self, symbol: str) -&gt; bool:\n        \"\"\"\n        Validate a stock symbol.\n\n        Args:\n            symbol: Stock symbol to validate\n\n        Returns:\n            True if valid format\n        \"\"\"\n        if not symbol:\n            return False\n\n        # Basic validation: alphanumeric, 1-10 chars\n        if not symbol.replace(\".\", \"\").replace(\"-\", \"\").isalnum():\n            return False\n\n        if len(symbol) &gt; 10:\n            return False\n\n        return True\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fmp.FMPProvider.fetch","title":"<code>fetch(session, endpoint, **params)</code>  <code>async</code>","text":"<p>Fetch data from FMP API.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name</p> required <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>HttpResponse</code> <p>HttpResponse with raw API data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If endpoint is invalid or required params missing</p> <code>HttpError</code> <p>For HTTP-related errors</p> Source code in <code>src/data_loader/providers/fmp.py</code> <pre><code>async def fetch(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    **params,\n) -&gt; HttpResponse:\n    \"\"\"\n    Fetch data from FMP API.\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name\n        **params: Endpoint-specific parameters\n\n    Returns:\n        HttpResponse with raw API data\n\n    Raises:\n        ValueError: If endpoint is invalid or required params missing\n        HttpError: For HTTP-related errors\n    \"\"\"\n    if not self.validate_endpoint(endpoint):\n        raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n    url = self._build_url(endpoint, **params)\n    query_params = self._build_params(endpoint, **params)\n\n    return await self.http_client.get(\n        session,\n        url,\n        params=query_params,\n        timeout=self.config.timeout,\n    )\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fmp.FMPProvider.normalize","title":"<code>normalize(data, endpoint)</code>","text":"<p>Normalize FMP API response.</p> <p>FMP responses vary by endpoint: - Most return a list of objects - Some wrap data in a dict - Error responses have an \"Error Message\" field</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Raw API response</p> required <code>endpoint</code> <code>str</code> <p>Endpoint that was called</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Normalized data structure</p> Source code in <code>src/data_loader/providers/fmp.py</code> <pre><code>def normalize(self, data: Any, endpoint: str) -&gt; Any:\n    \"\"\"\n    Normalize FMP API response.\n\n    FMP responses vary by endpoint:\n    - Most return a list of objects\n    - Some wrap data in a dict\n    - Error responses have an \"Error Message\" field\n\n    Args:\n        data: Raw API response\n        endpoint: Endpoint that was called\n\n    Returns:\n        Normalized data structure\n    \"\"\"\n    # Check for error response\n    if isinstance(data, dict) and \"Error Message\" in data:\n        return {\"error\": data[\"Error Message\"], \"data\": None}\n\n    # Normalize based on endpoint type\n    if endpoint == \"historical_price\":\n        # Historical prices come wrapped in {\"symbol\": ..., \"historical\": [...]}\n        if isinstance(data, dict) and \"historical\" in data:\n            return {\n                \"symbol\": data.get(\"symbol\"),\n                \"historical\": data.get(\"historical\", []),\n            }\n        return data\n\n    if endpoint == \"profile\":\n        # Profile returns a list with single item, extract it\n        if isinstance(data, list) and len(data) == 1:\n            return data[0]\n        return data\n\n    if endpoint == \"quote\":\n        # Quote returns a list with single item, extract it\n        if isinstance(data, list) and len(data) == 1:\n            return data[0]\n        return data\n\n    # Default: return as-is (most endpoints return lists)\n    return data\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fmp.FMPProvider.cache_key","title":"<code>cache_key(endpoint, **params)</code>","text":"<p>Generate cache key for FMP request.</p> <p>Cache key format: endpoint_param1=value1_param2=value2</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>**params</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Cache key string</p> Source code in <code>src/data_loader/providers/fmp.py</code> <pre><code>def cache_key(self, endpoint: str, **params) -&gt; str:\n    \"\"\"\n    Generate cache key for FMP request.\n\n    Cache key format: endpoint_param1=value1_param2=value2\n\n    Args:\n        endpoint: API endpoint\n        **params: Request parameters\n\n    Returns:\n        Cache key string\n    \"\"\"\n    # Exclude API key from cache key\n    cache_params = {k: v for k, v in params.items() if k != \"apikey\"}\n    return self._generate_cache_key(endpoint, **cache_params)\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fmp.FMPProvider.validate_symbol","title":"<code>validate_symbol(symbol)</code>","text":"<p>Validate a stock symbol.</p> <p>Parameters:</p> Name Type Description Default <code>symbol</code> <code>str</code> <p>Stock symbol to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid format</p> Source code in <code>src/data_loader/providers/fmp.py</code> <pre><code>def validate_symbol(self, symbol: str) -&gt; bool:\n    \"\"\"\n    Validate a stock symbol.\n\n    Args:\n        symbol: Stock symbol to validate\n\n    Returns:\n        True if valid format\n    \"\"\"\n    if not symbol:\n        return False\n\n    # Basic validation: alphanumeric, 1-10 chars\n    if not symbol.replace(\".\", \"\").replace(\"-\", \"\").isalnum():\n        return False\n\n    if len(symbol) &gt; 10:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/providers/#supported-endpoints","title":"Supported Endpoints","text":"Endpoint Description Required Params <code>profile</code> Company profile <code>symbol</code> <code>quote</code> Real-time quote <code>symbol</code> <code>historical_price</code> Historical OHLCV <code>symbol</code>, <code>from</code>, <code>to</code> <code>earnings_calendar</code> Earnings dates <code>from</code>, <code>to</code> <code>balance_sheet</code> Balance sheet <code>symbol</code>, <code>period</code> <code>income_statement</code> Income statement <code>symbol</code>, <code>period</code> <code>cash_flow</code> Cash flow statement <code>symbol</code>, <code>period</code> <code>ratios</code> Financial ratios <code>symbol</code> <code>growth</code> Growth metrics <code>symbol</code> <code>key_metrics</code> Key metrics <code>symbol</code> <code>insider_trading</code> Insider trades <code>symbol</code> <code>institutional_ownership</code> Institutional holders <code>symbol</code> <code>screener</code> Stock screener Various filters"},{"location":"api/providers/#polygon-provider","title":"Polygon Provider","text":""},{"location":"api/providers/#data_loader.providers.polygon.PolygonProvider","title":"<code>data_loader.providers.polygon.PolygonProvider</code>","text":"<p>               Bases: <code>BaseDataProvider</code></p> <p>Polygon.io data provider.</p> <p>Supports 4 endpoints: - aggs_daily: Daily aggregate bars (OHLCV) - trades: Trade-level data - options_snapshot: Options chain snapshot - market_snapshot: Market-wide snapshot</p> Usage <p>provider = PolygonProvider(config, http_client, cache, health_monitor) async with aiohttp.ClientSession() as session:     response = await provider.get(         session, \"aggs_daily\",         symbol=\"SPY\", start=\"2024-01-01\", end=\"2024-01-31\"     )     print(response.data)</p> Source code in <code>src/data_loader/providers/polygon.py</code> <pre><code>class PolygonProvider(BaseDataProvider):\n    \"\"\"\n    Polygon.io data provider.\n\n    Supports 4 endpoints:\n    - aggs_daily: Daily aggregate bars (OHLCV)\n    - trades: Trade-level data\n    - options_snapshot: Options chain snapshot\n    - market_snapshot: Market-wide snapshot\n\n    Usage:\n        provider = PolygonProvider(config, http_client, cache, health_monitor)\n        async with aiohttp.ClientSession() as session:\n            response = await provider.get(\n                session, \"aggs_daily\",\n                symbol=\"SPY\", start=\"2024-01-01\", end=\"2024-01-31\"\n            )\n            print(response.data)\n    \"\"\"\n\n    # Endpoint configurations\n    ENDPOINTS = {\n        \"aggs_daily\": {\n            \"path\": \"/v2/aggs/ticker/{symbol}/range/1/day/{start}/{end}\",\n            \"params\": [\"adjusted\", \"sort\", \"limit\"],\n        },\n        \"trades\": {\n            \"path\": \"/v3/trades/{symbol}\",\n            \"params\": [\"timestamp\", \"timestamp.gte\", \"timestamp.lte\",\n                      \"order\", \"limit\", \"sort\"],\n        },\n        \"options_snapshot\": {\n            \"path\": \"/v3/snapshot/options/{underlyingAsset}\",\n            \"params\": [\"strike_price\", \"expiration_date\", \"contract_type\",\n                      \"order\", \"limit\", \"sort\"],\n        },\n        \"market_snapshot\": {\n            \"path\": \"/v2/snapshot/locale/us/markets/stocks/tickers\",\n            \"params\": [\"tickers\", \"include_otc\"],\n        },\n    }\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"polygon\"\n\n    def get_supported_endpoints(self) -&gt; list[str]:\n        return list(self.ENDPOINTS.keys())\n\n    def _build_url(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Build the full URL for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: Parameters including path variables\n\n        Returns:\n            Full URL string\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint)\n        if not endpoint_config:\n            raise ValueError(f\"Unknown endpoint: {endpoint}\")\n\n        path = endpoint_config[\"path\"]\n\n        # Replace path parameters\n        if \"{symbol}\" in path:\n            symbol = params.get(\"symbol\")\n            if not symbol:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'symbol' parameter\")\n            path = path.replace(\"{symbol}\", symbol)\n\n        if \"{underlyingAsset}\" in path:\n            asset = params.get(\"underlyingAsset\") or params.get(\"symbol\")\n            if not asset:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'underlyingAsset' or 'symbol' parameter\")\n            path = path.replace(\"{underlyingAsset}\", asset)\n\n        if \"{start}\" in path:\n            start = params.get(\"start\")\n            if not start:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'start' parameter\")\n            path = path.replace(\"{start}\", start)\n\n        if \"{end}\" in path:\n            end = params.get(\"end\")\n            if not end:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'end' parameter\")\n            path = path.replace(\"{end}\", end)\n\n        return f\"{self.base_url}{path}\"\n\n    def _build_params(self, endpoint: str, **params) -&gt; dict:\n        \"\"\"\n        Build query parameters for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: All provided parameters\n\n        Returns:\n            Dictionary of query parameters\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint, {})\n        allowed_params = endpoint_config.get(\"params\", [])\n\n        # Always include API key\n        query_params = {\"apiKey\": self.api_key}\n\n        # Add allowed parameters\n        for param_name in allowed_params:\n            if param_name in params and params[param_name] is not None:\n                query_params[param_name] = params[param_name]\n\n        return query_params\n\n    async def fetch(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        **params,\n    ) -&gt; HttpResponse:\n        \"\"\"\n        Fetch data from Polygon API.\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name\n            **params: Endpoint-specific parameters\n\n        Returns:\n            HttpResponse with raw API data\n\n        Raises:\n            ValueError: If endpoint is invalid or required params missing\n            HttpError: For HTTP-related errors\n        \"\"\"\n        if not self.validate_endpoint(endpoint):\n            raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n        url = self._build_url(endpoint, **params)\n        query_params = self._build_params(endpoint, **params)\n\n        return await self.http_client.get(\n            session,\n            url,\n            params=query_params,\n            timeout=self.config.timeout,\n        )\n\n    def normalize(self, data: Any, endpoint: str) -&gt; Any:\n        \"\"\"\n        Normalize Polygon API response.\n\n        Polygon responses have a standard structure:\n        - status: \"OK\" or error\n        - results: The actual data (array or object)\n        - request_id: Request identifier\n\n        Args:\n            data: Raw API response\n            endpoint: Endpoint that was called\n\n        Returns:\n            Normalized data structure\n        \"\"\"\n        # Check for error response\n        if isinstance(data, dict):\n            status = data.get(\"status\", \"\").upper()\n            if status == \"ERROR\":\n                return {\n                    \"error\": data.get(\"error\", \"Unknown error\"),\n                    \"message\": data.get(\"message\"),\n                    \"data\": None,\n                }\n\n        # Extract results based on endpoint\n        if endpoint == \"aggs_daily\":\n            if isinstance(data, dict):\n                return {\n                    \"ticker\": data.get(\"ticker\"),\n                    \"queryCount\": data.get(\"queryCount\", 0),\n                    \"resultsCount\": data.get(\"resultsCount\", 0),\n                    \"adjusted\": data.get(\"adjusted\", False),\n                    \"results\": data.get(\"results\", []),\n                }\n            return data\n\n        if endpoint == \"trades\":\n            if isinstance(data, dict):\n                return {\n                    \"results\": data.get(\"results\", []),\n                    \"next_url\": data.get(\"next_url\"),\n                }\n            return data\n\n        if endpoint == \"options_snapshot\":\n            if isinstance(data, dict):\n                return {\n                    \"results\": data.get(\"results\", []),\n                    \"next_url\": data.get(\"next_url\"),\n                }\n            return data\n\n        if endpoint == \"market_snapshot\":\n            if isinstance(data, dict):\n                return {\n                    \"tickers\": data.get(\"tickers\", []),\n                    \"count\": data.get(\"count\", 0),\n                }\n            return data\n\n        # Default: return as-is\n        return data\n\n    def cache_key(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Generate cache key for Polygon request.\n\n        Args:\n            endpoint: API endpoint\n            **params: Request parameters\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Exclude API key from cache key\n        cache_params = {k: v for k, v in params.items() if k != \"apiKey\"}\n        return self._generate_cache_key(endpoint, **cache_params)\n\n    def validate_symbol(self, symbol: str) -&gt; bool:\n        \"\"\"\n        Validate a stock/options symbol.\n\n        Args:\n            symbol: Symbol to validate\n\n        Returns:\n            True if valid format\n        \"\"\"\n        if not symbol:\n            return False\n\n        # Basic validation: alphanumeric with allowed special chars\n        allowed_chars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.-:\")\n        if not all(c in allowed_chars for c in symbol.upper()):\n            return False\n\n        if len(symbol) &gt; 21:  # Options symbols can be long\n            return False\n\n        return True\n</code></pre>"},{"location":"api/providers/#data_loader.providers.polygon.PolygonProvider.fetch","title":"<code>fetch(session, endpoint, **params)</code>  <code>async</code>","text":"<p>Fetch data from Polygon API.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name</p> required <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>HttpResponse</code> <p>HttpResponse with raw API data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If endpoint is invalid or required params missing</p> <code>HttpError</code> <p>For HTTP-related errors</p> Source code in <code>src/data_loader/providers/polygon.py</code> <pre><code>async def fetch(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    **params,\n) -&gt; HttpResponse:\n    \"\"\"\n    Fetch data from Polygon API.\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name\n        **params: Endpoint-specific parameters\n\n    Returns:\n        HttpResponse with raw API data\n\n    Raises:\n        ValueError: If endpoint is invalid or required params missing\n        HttpError: For HTTP-related errors\n    \"\"\"\n    if not self.validate_endpoint(endpoint):\n        raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n    url = self._build_url(endpoint, **params)\n    query_params = self._build_params(endpoint, **params)\n\n    return await self.http_client.get(\n        session,\n        url,\n        params=query_params,\n        timeout=self.config.timeout,\n    )\n</code></pre>"},{"location":"api/providers/#data_loader.providers.polygon.PolygonProvider.normalize","title":"<code>normalize(data, endpoint)</code>","text":"<p>Normalize Polygon API response.</p> <p>Polygon responses have a standard structure: - status: \"OK\" or error - results: The actual data (array or object) - request_id: Request identifier</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Raw API response</p> required <code>endpoint</code> <code>str</code> <p>Endpoint that was called</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Normalized data structure</p> Source code in <code>src/data_loader/providers/polygon.py</code> <pre><code>def normalize(self, data: Any, endpoint: str) -&gt; Any:\n    \"\"\"\n    Normalize Polygon API response.\n\n    Polygon responses have a standard structure:\n    - status: \"OK\" or error\n    - results: The actual data (array or object)\n    - request_id: Request identifier\n\n    Args:\n        data: Raw API response\n        endpoint: Endpoint that was called\n\n    Returns:\n        Normalized data structure\n    \"\"\"\n    # Check for error response\n    if isinstance(data, dict):\n        status = data.get(\"status\", \"\").upper()\n        if status == \"ERROR\":\n            return {\n                \"error\": data.get(\"error\", \"Unknown error\"),\n                \"message\": data.get(\"message\"),\n                \"data\": None,\n            }\n\n    # Extract results based on endpoint\n    if endpoint == \"aggs_daily\":\n        if isinstance(data, dict):\n            return {\n                \"ticker\": data.get(\"ticker\"),\n                \"queryCount\": data.get(\"queryCount\", 0),\n                \"resultsCount\": data.get(\"resultsCount\", 0),\n                \"adjusted\": data.get(\"adjusted\", False),\n                \"results\": data.get(\"results\", []),\n            }\n        return data\n\n    if endpoint == \"trades\":\n        if isinstance(data, dict):\n            return {\n                \"results\": data.get(\"results\", []),\n                \"next_url\": data.get(\"next_url\"),\n            }\n        return data\n\n    if endpoint == \"options_snapshot\":\n        if isinstance(data, dict):\n            return {\n                \"results\": data.get(\"results\", []),\n                \"next_url\": data.get(\"next_url\"),\n            }\n        return data\n\n    if endpoint == \"market_snapshot\":\n        if isinstance(data, dict):\n            return {\n                \"tickers\": data.get(\"tickers\", []),\n                \"count\": data.get(\"count\", 0),\n            }\n        return data\n\n    # Default: return as-is\n    return data\n</code></pre>"},{"location":"api/providers/#data_loader.providers.polygon.PolygonProvider.cache_key","title":"<code>cache_key(endpoint, **params)</code>","text":"<p>Generate cache key for Polygon request.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>**params</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Cache key string</p> Source code in <code>src/data_loader/providers/polygon.py</code> <pre><code>def cache_key(self, endpoint: str, **params) -&gt; str:\n    \"\"\"\n    Generate cache key for Polygon request.\n\n    Args:\n        endpoint: API endpoint\n        **params: Request parameters\n\n    Returns:\n        Cache key string\n    \"\"\"\n    # Exclude API key from cache key\n    cache_params = {k: v for k, v in params.items() if k != \"apiKey\"}\n    return self._generate_cache_key(endpoint, **cache_params)\n</code></pre>"},{"location":"api/providers/#data_loader.providers.polygon.PolygonProvider.validate_symbol","title":"<code>validate_symbol(symbol)</code>","text":"<p>Validate a stock/options symbol.</p> <p>Parameters:</p> Name Type Description Default <code>symbol</code> <code>str</code> <p>Symbol to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid format</p> Source code in <code>src/data_loader/providers/polygon.py</code> <pre><code>def validate_symbol(self, symbol: str) -&gt; bool:\n    \"\"\"\n    Validate a stock/options symbol.\n\n    Args:\n        symbol: Symbol to validate\n\n    Returns:\n        True if valid format\n    \"\"\"\n    if not symbol:\n        return False\n\n    # Basic validation: alphanumeric with allowed special chars\n    allowed_chars = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.-:\")\n    if not all(c in allowed_chars for c in symbol.upper()):\n        return False\n\n    if len(symbol) &gt; 21:  # Options symbols can be long\n        return False\n\n    return True\n</code></pre>"},{"location":"api/providers/#supported-endpoints_1","title":"Supported Endpoints","text":"Endpoint Description Required Params <code>aggs_daily</code> Daily aggregates <code>symbol</code>, <code>start</code>, <code>end</code> <code>trades</code> Tick-level trades <code>symbol</code> <code>options_snapshot</code> Options chain <code>underlying</code> <code>market_snapshot</code> Market overview -"},{"location":"api/providers/#fred-provider","title":"FRED Provider","text":""},{"location":"api/providers/#data_loader.providers.fred.FREDProvider","title":"<code>data_loader.providers.fred.FREDProvider</code>","text":"<p>               Bases: <code>BaseDataProvider</code></p> <p>FRED (Federal Reserve Economic Data) provider.</p> <p>Supports 32 macroeconomic series across categories: - Inflation: CPI, PCE, PPI, Core CPI - Labor Market: Unemployment, Payrolls, LFPR, Wages - GDP &amp; Growth: GDP, GDI, Industrial Production - Housing: Case-Shiller, Housing Starts, Permits - Interest Rates: Fed Funds, Treasury Yields - Money &amp; Credit: M2, Consumer Credit - Financial Conditions: VIX, Yield Curve - Leading Indicators: LEI, Consumer Sentiment</p> Usage <p>provider = FREDProvider(config, http_client, cache, health_monitor) async with aiohttp.ClientSession() as session:     response = await provider.get(session, \"series\", series_id=\"CPIAUCSL\")     print(response.data)</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>class FREDProvider(BaseDataProvider):\n    \"\"\"\n    FRED (Federal Reserve Economic Data) provider.\n\n    Supports 32 macroeconomic series across categories:\n    - Inflation: CPI, PCE, PPI, Core CPI\n    - Labor Market: Unemployment, Payrolls, LFPR, Wages\n    - GDP &amp; Growth: GDP, GDI, Industrial Production\n    - Housing: Case-Shiller, Housing Starts, Permits\n    - Interest Rates: Fed Funds, Treasury Yields\n    - Money &amp; Credit: M2, Consumer Credit\n    - Financial Conditions: VIX, Yield Curve\n    - Leading Indicators: LEI, Consumer Sentiment\n\n    Usage:\n        provider = FREDProvider(config, http_client, cache, health_monitor)\n        async with aiohttp.ClientSession() as session:\n            response = await provider.get(session, \"series\", series_id=\"CPIAUCSL\")\n            print(response.data)\n    \"\"\"\n\n    # Supported FRED series organized by category\n    SERIES = {\n        # Inflation\n        \"CPIAUCSL\": \"Consumer Price Index\",\n        \"CPILFESL\": \"Core CPI (Less Food and Energy)\",\n        \"PCEPI\": \"PCE Price Index\",\n        \"PCEPILFE\": \"Core PCE Price Index\",\n        \"PPIFIS\": \"Producer Price Index\",\n        # Labor Market\n        \"UNRATE\": \"Unemployment Rate\",\n        \"PAYEMS\": \"Nonfarm Payrolls\",\n        \"CIVPART\": \"Labor Force Participation Rate\",\n        \"AHETPI\": \"Average Hourly Earnings\",\n        \"ICSA\": \"Initial Jobless Claims\",\n        \"CCSA\": \"Continued Jobless Claims\",\n        \"JTSJOL\": \"Job Openings (JOLTS)\",\n        # GDP &amp; Growth\n        \"GDP\": \"Gross Domestic Product\",\n        \"GDPC1\": \"Real GDP\",\n        \"GDI\": \"Gross Domestic Income\",\n        \"INDPRO\": \"Industrial Production Index\",\n        \"UMCSENT\": \"Consumer Sentiment (UMich)\",\n        # Housing\n        \"CSUSHPINSA\": \"Case-Shiller Home Price Index\",\n        \"HOUST\": \"Housing Starts\",\n        \"PERMIT\": \"Building Permits\",\n        \"HSN1F\": \"New Home Sales\",\n        \"EXHOSLUSM495S\": \"Existing Home Sales\",\n        # Interest Rates\n        \"FEDFUNDS\": \"Federal Funds Rate\",\n        \"DFF\": \"Effective Federal Funds Rate\",\n        \"DGS2\": \"2-Year Treasury Yield\",\n        \"DGS10\": \"10-Year Treasury Yield\",\n        \"DGS30\": \"30-Year Treasury Yield\",\n        \"T10Y2Y\": \"10Y-2Y Treasury Spread\",\n        \"T10Y3M\": \"10Y-3M Treasury Spread\",\n        # Money &amp; Credit\n        \"M2SL\": \"M2 Money Supply\",\n        \"TOTALSL\": \"Consumer Credit\",\n        # Financial Conditions\n        \"VIXCLS\": \"VIX Volatility Index\",\n    }\n\n    # Endpoint configurations\n    ENDPOINTS = {\n        \"series\": {\n            \"path\": \"/series/observations\",\n            \"params\": [\"series_id\", \"observation_start\", \"observation_end\",\n                      \"units\", \"frequency\", \"aggregation_method\", \"sort_order\",\n                      \"limit\", \"offset\"],\n        },\n        \"series_info\": {\n            \"path\": \"/series\",\n            \"params\": [\"series_id\"],\n        },\n        \"releases\": {\n            \"path\": \"/releases\",\n            \"params\": [\"limit\", \"offset\", \"order_by\", \"sort_order\"],\n        },\n    }\n\n    @property\n    def provider_name(self) -&gt; str:\n        return \"fred\"\n\n    def get_supported_endpoints(self) -&gt; list[str]:\n        return list(self.ENDPOINTS.keys())\n\n    def get_supported_series(self) -&gt; dict[str, str]:\n        \"\"\"Get dictionary of supported series IDs and descriptions.\"\"\"\n        return self.SERIES.copy()\n\n    def _build_url(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Build the full URL for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: Parameters\n\n        Returns:\n            Full URL string\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint)\n        if not endpoint_config:\n            raise ValueError(f\"Unknown endpoint: {endpoint}\")\n\n        path = endpoint_config[\"path\"]\n        return f\"{self.base_url}{path}\"\n\n    def _build_params(self, endpoint: str, **params) -&gt; dict:\n        \"\"\"\n        Build query parameters for an endpoint.\n\n        Args:\n            endpoint: Endpoint name\n            **params: All provided parameters\n\n        Returns:\n            Dictionary of query parameters\n        \"\"\"\n        endpoint_config = self.ENDPOINTS.get(endpoint, {})\n        allowed_params = endpoint_config.get(\"params\", [])\n\n        # FRED requires api_key (must be lowercase) and file_type\n        query_params = {\n            \"api_key\": self.api_key.lower(),\n            \"file_type\": \"json\",\n        }\n\n        # Add allowed parameters\n        for param_name in allowed_params:\n            if param_name in params and params[param_name] is not None:\n                query_params[param_name] = params[param_name]\n\n        return query_params\n\n    async def fetch(\n        self,\n        session: aiohttp.ClientSession,\n        endpoint: str,\n        **params,\n    ) -&gt; HttpResponse:\n        \"\"\"\n        Fetch data from FRED API.\n\n        Args:\n            session: aiohttp ClientSession\n            endpoint: API endpoint name (series, series_info, releases)\n            **params: Endpoint-specific parameters\n\n        Returns:\n            HttpResponse with raw API data\n\n        Raises:\n            ValueError: If endpoint is invalid or required params missing\n            HttpError: For HTTP-related errors\n        \"\"\"\n        if not self.validate_endpoint(endpoint):\n            raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n        # Series endpoints require series_id\n        if endpoint in (\"series\", \"series_info\"):\n            if \"series_id\" not in params:\n                raise ValueError(f\"Endpoint '{endpoint}' requires 'series_id' parameter\")\n\n        url = self._build_url(endpoint, **params)\n        query_params = self._build_params(endpoint, **params)\n\n        return await self.http_client.get(\n            session,\n            url,\n            params=query_params,\n            timeout=self.config.timeout,\n        )\n\n    def normalize(self, data: Any, endpoint: str) -&gt; Any:\n        \"\"\"\n        Normalize FRED API response.\n\n        FRED responses have a standard structure:\n        - observations: Array of data points (for series)\n        - seriess: Array of series info (for series_info)\n        - releases: Array of releases (for releases)\n\n        Args:\n            data: Raw API response\n            endpoint: Endpoint that was called\n\n        Returns:\n            Normalized data structure\n        \"\"\"\n        # Check for error response\n        if isinstance(data, dict) and \"error_code\" in data:\n            return {\n                \"error\": data.get(\"error_message\", \"Unknown error\"),\n                \"error_code\": data.get(\"error_code\"),\n                \"data\": None,\n            }\n\n        if endpoint == \"series\":\n            if isinstance(data, dict):\n                return {\n                    \"realtime_start\": data.get(\"realtime_start\"),\n                    \"realtime_end\": data.get(\"realtime_end\"),\n                    \"observation_start\": data.get(\"observation_start\"),\n                    \"observation_end\": data.get(\"observation_end\"),\n                    \"units\": data.get(\"units\"),\n                    \"output_type\": data.get(\"output_type\"),\n                    \"order_by\": data.get(\"order_by\"),\n                    \"sort_order\": data.get(\"sort_order\"),\n                    \"count\": data.get(\"count\", 0),\n                    \"observations\": data.get(\"observations\", []),\n                }\n            return data\n\n        if endpoint == \"series_info\":\n            if isinstance(data, dict):\n                seriess = data.get(\"seriess\", [])\n                # Return single series info if only one\n                if len(seriess) == 1:\n                    return seriess[0]\n                return {\"seriess\": seriess}\n            return data\n\n        if endpoint == \"releases\":\n            if isinstance(data, dict):\n                return {\n                    \"releases\": data.get(\"releases\", []),\n                    \"count\": data.get(\"count\", 0),\n                }\n            return data\n\n        # Default: return as-is\n        return data\n\n    def cache_key(self, endpoint: str, **params) -&gt; str:\n        \"\"\"\n        Generate cache key for FRED request.\n\n        Args:\n            endpoint: API endpoint\n            **params: Request parameters\n\n        Returns:\n            Cache key string\n        \"\"\"\n        # Exclude API key from cache key\n        cache_params = {k: v for k, v in params.items() if k != \"api_key\"}\n        return self._generate_cache_key(endpoint, **cache_params)\n\n    def validate_series_id(self, series_id: str) -&gt; bool:\n        \"\"\"\n        Validate a FRED series ID.\n\n        Args:\n            series_id: Series ID to validate\n\n        Returns:\n            True if valid format (alphanumeric, uppercase)\n        \"\"\"\n        if not series_id:\n            return False\n\n        # FRED series IDs are uppercase alphanumeric with some special chars\n        if not series_id.replace(\"_\", \"\").isalnum():\n            return False\n\n        return True\n\n    def is_supported_series(self, series_id: str) -&gt; bool:\n        \"\"\"\n        Check if a series ID is in our predefined list.\n\n        Args:\n            series_id: Series ID to check\n\n        Returns:\n            True if in supported list\n        \"\"\"\n        return series_id.upper() in self.SERIES\n\n    async def get_series(\n        self,\n        session: aiohttp.ClientSession,\n        series_id: str,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        use_cache: bool = True,\n    ):\n        \"\"\"\n        Convenience method to fetch a specific series.\n\n        Args:\n            session: aiohttp ClientSession\n            series_id: FRED series ID (e.g., \"CPIAUCSL\")\n            start_date: Optional start date (YYYY-MM-DD)\n            end_date: Optional end date (YYYY-MM-DD)\n            use_cache: Whether to use caching\n\n        Returns:\n            ProviderResponse with series data\n        \"\"\"\n        params = {\"series_id\": series_id}\n        if start_date:\n            params[\"observation_start\"] = start_date\n        if end_date:\n            params[\"observation_end\"] = end_date\n\n        return await self.get(session, \"series\", use_cache=use_cache, **params)\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.get_supported_series","title":"<code>get_supported_series()</code>","text":"<p>Get dictionary of supported series IDs and descriptions.</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>def get_supported_series(self) -&gt; dict[str, str]:\n    \"\"\"Get dictionary of supported series IDs and descriptions.\"\"\"\n    return self.SERIES.copy()\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.fetch","title":"<code>fetch(session, endpoint, **params)</code>  <code>async</code>","text":"<p>Fetch data from FRED API.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>endpoint</code> <code>str</code> <p>API endpoint name (series, series_info, releases)</p> required <code>**params</code> <p>Endpoint-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>HttpResponse</code> <p>HttpResponse with raw API data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If endpoint is invalid or required params missing</p> <code>HttpError</code> <p>For HTTP-related errors</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>async def fetch(\n    self,\n    session: aiohttp.ClientSession,\n    endpoint: str,\n    **params,\n) -&gt; HttpResponse:\n    \"\"\"\n    Fetch data from FRED API.\n\n    Args:\n        session: aiohttp ClientSession\n        endpoint: API endpoint name (series, series_info, releases)\n        **params: Endpoint-specific parameters\n\n    Returns:\n        HttpResponse with raw API data\n\n    Raises:\n        ValueError: If endpoint is invalid or required params missing\n        HttpError: For HTTP-related errors\n    \"\"\"\n    if not self.validate_endpoint(endpoint):\n        raise ValueError(f\"Invalid endpoint: {endpoint}\")\n\n    # Series endpoints require series_id\n    if endpoint in (\"series\", \"series_info\"):\n        if \"series_id\" not in params:\n            raise ValueError(f\"Endpoint '{endpoint}' requires 'series_id' parameter\")\n\n    url = self._build_url(endpoint, **params)\n    query_params = self._build_params(endpoint, **params)\n\n    return await self.http_client.get(\n        session,\n        url,\n        params=query_params,\n        timeout=self.config.timeout,\n    )\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.normalize","title":"<code>normalize(data, endpoint)</code>","text":"<p>Normalize FRED API response.</p> <p>FRED responses have a standard structure: - observations: Array of data points (for series) - seriess: Array of series info (for series_info) - releases: Array of releases (for releases)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Raw API response</p> required <code>endpoint</code> <code>str</code> <p>Endpoint that was called</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Normalized data structure</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>def normalize(self, data: Any, endpoint: str) -&gt; Any:\n    \"\"\"\n    Normalize FRED API response.\n\n    FRED responses have a standard structure:\n    - observations: Array of data points (for series)\n    - seriess: Array of series info (for series_info)\n    - releases: Array of releases (for releases)\n\n    Args:\n        data: Raw API response\n        endpoint: Endpoint that was called\n\n    Returns:\n        Normalized data structure\n    \"\"\"\n    # Check for error response\n    if isinstance(data, dict) and \"error_code\" in data:\n        return {\n            \"error\": data.get(\"error_message\", \"Unknown error\"),\n            \"error_code\": data.get(\"error_code\"),\n            \"data\": None,\n        }\n\n    if endpoint == \"series\":\n        if isinstance(data, dict):\n            return {\n                \"realtime_start\": data.get(\"realtime_start\"),\n                \"realtime_end\": data.get(\"realtime_end\"),\n                \"observation_start\": data.get(\"observation_start\"),\n                \"observation_end\": data.get(\"observation_end\"),\n                \"units\": data.get(\"units\"),\n                \"output_type\": data.get(\"output_type\"),\n                \"order_by\": data.get(\"order_by\"),\n                \"sort_order\": data.get(\"sort_order\"),\n                \"count\": data.get(\"count\", 0),\n                \"observations\": data.get(\"observations\", []),\n            }\n        return data\n\n    if endpoint == \"series_info\":\n        if isinstance(data, dict):\n            seriess = data.get(\"seriess\", [])\n            # Return single series info if only one\n            if len(seriess) == 1:\n                return seriess[0]\n            return {\"seriess\": seriess}\n        return data\n\n    if endpoint == \"releases\":\n        if isinstance(data, dict):\n            return {\n                \"releases\": data.get(\"releases\", []),\n                \"count\": data.get(\"count\", 0),\n            }\n        return data\n\n    # Default: return as-is\n    return data\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.cache_key","title":"<code>cache_key(endpoint, **params)</code>","text":"<p>Generate cache key for FRED request.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint</p> required <code>**params</code> <p>Request parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Cache key string</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>def cache_key(self, endpoint: str, **params) -&gt; str:\n    \"\"\"\n    Generate cache key for FRED request.\n\n    Args:\n        endpoint: API endpoint\n        **params: Request parameters\n\n    Returns:\n        Cache key string\n    \"\"\"\n    # Exclude API key from cache key\n    cache_params = {k: v for k, v in params.items() if k != \"api_key\"}\n    return self._generate_cache_key(endpoint, **cache_params)\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.validate_series_id","title":"<code>validate_series_id(series_id)</code>","text":"<p>Validate a FRED series ID.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <code>str</code> <p>Series ID to validate</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid format (alphanumeric, uppercase)</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>def validate_series_id(self, series_id: str) -&gt; bool:\n    \"\"\"\n    Validate a FRED series ID.\n\n    Args:\n        series_id: Series ID to validate\n\n    Returns:\n        True if valid format (alphanumeric, uppercase)\n    \"\"\"\n    if not series_id:\n        return False\n\n    # FRED series IDs are uppercase alphanumeric with some special chars\n    if not series_id.replace(\"_\", \"\").isalnum():\n        return False\n\n    return True\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.is_supported_series","title":"<code>is_supported_series(series_id)</code>","text":"<p>Check if a series ID is in our predefined list.</p> <p>Parameters:</p> Name Type Description Default <code>series_id</code> <code>str</code> <p>Series ID to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if in supported list</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>def is_supported_series(self, series_id: str) -&gt; bool:\n    \"\"\"\n    Check if a series ID is in our predefined list.\n\n    Args:\n        series_id: Series ID to check\n\n    Returns:\n        True if in supported list\n    \"\"\"\n    return series_id.upper() in self.SERIES\n</code></pre>"},{"location":"api/providers/#data_loader.providers.fred.FREDProvider.get_series","title":"<code>get_series(session, series_id, start_date=None, end_date=None, use_cache=True)</code>  <code>async</code>","text":"<p>Convenience method to fetch a specific series.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>ClientSession</code> <p>aiohttp ClientSession</p> required <code>series_id</code> <code>str</code> <p>FRED series ID (e.g., \"CPIAUCSL\")</p> required <code>start_date</code> <code>Optional[str]</code> <p>Optional start date (YYYY-MM-DD)</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>Optional end date (YYYY-MM-DD)</p> <code>None</code> <code>use_cache</code> <code>bool</code> <p>Whether to use caching</p> <code>True</code> <p>Returns:</p> Type Description <p>ProviderResponse with series data</p> Source code in <code>src/data_loader/providers/fred.py</code> <pre><code>async def get_series(\n    self,\n    session: aiohttp.ClientSession,\n    series_id: str,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    use_cache: bool = True,\n):\n    \"\"\"\n    Convenience method to fetch a specific series.\n\n    Args:\n        session: aiohttp ClientSession\n        series_id: FRED series ID (e.g., \"CPIAUCSL\")\n        start_date: Optional start date (YYYY-MM-DD)\n        end_date: Optional end date (YYYY-MM-DD)\n        use_cache: Whether to use caching\n\n    Returns:\n        ProviderResponse with series data\n    \"\"\"\n    params = {\"series_id\": series_id}\n    if start_date:\n        params[\"observation_start\"] = start_date\n    if end_date:\n        params[\"observation_end\"] = end_date\n\n    return await self.get(session, \"series\", use_cache=use_cache, **params)\n</code></pre>"},{"location":"api/providers/#supported-series","title":"Supported Series","text":"<p>See the Providers Guide for the full list of supported FRED series.</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>OmniData Nexus Core uses environment variables for configuration. Copy the template and edit:</p> <pre><code>cp .env.example .env\nnano .env  # or your preferred editor\n</code></pre>"},{"location":"getting-started/configuration/#required-variables","title":"Required Variables","text":""},{"location":"getting-started/configuration/#api-keys","title":"API Keys","text":"<pre><code># Required for LIVE mode\nFMP_KEY=your_fmp_api_key\nPOLYGON_KEY=your_polygon_api_key\nFRED_KEY=your_fred_api_key\n</code></pre> <p>Security</p> <p>Never commit API keys to version control. The <code>.env</code> file is in <code>.gitignore</code>.</p>"},{"location":"getting-started/configuration/#getting-api-keys","title":"Getting API Keys","text":"Provider Free Tier Link FMP 250 calls/day financialmodelingprep.com Polygon Limited polygon.io FRED Unlimited fred.stlouisfed.org"},{"location":"getting-started/configuration/#optional-variables","title":"Optional Variables","text":"<pre><code># Cache settings\nCACHE_TTL_DAYS=7          # Cache expiration (default: 7)\n\n# Logging\nLOG_LEVEL=INFO            # DEBUG, INFO, WARNING, ERROR\n\n# Operating mode\nOPERATING_MODE=LIVE       # LIVE or READ_ONLY\n\n# Network\nREQUEST_TIMEOUT=30        # API timeout in seconds\n</code></pre>"},{"location":"getting-started/configuration/#operating-modes","title":"Operating Modes","text":""},{"location":"getting-started/configuration/#live-mode-default","title":"LIVE Mode (Default)","text":"<ul> <li>Makes API calls when data not cached</li> <li>Caches responses automatically</li> <li>Requires valid API keys</li> </ul>"},{"location":"getting-started/configuration/#read_only-mode","title":"READ_ONLY Mode","text":"<ul> <li>Only serves cached data</li> <li>No API calls made</li> <li>Useful for offline analysis or testing</li> </ul> <pre><code>from data_loader import DataLoader, OperatingMode\n\nloader = DataLoader()\nloader.set_operating_mode(OperatingMode.READ_ONLY)\n</code></pre>"},{"location":"getting-started/configuration/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Secure the .env file (Linux/macOS)\nchmod 600 .env\n\n# Verify permissions\nls -la .env\n# Should show: -rw-------\n</code></pre>"},{"location":"getting-started/configuration/#configuration-in-code","title":"Configuration in Code","text":"<p>You can also configure programmatically:</p> <pre><code>from data_loader import DataLoader\n\nloader = DataLoader()\n\n# Override cache TTL\nloader._cache.ttl_days = 14\n\n# Check current mode\nprint(loader.get_operating_mode())\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pip package manager</li> </ul>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code># Clone repository\ngit clone https://github.com/Maeshowe/Nexus_Core.git\ncd Nexus_Core\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n\n# Or install dependencies directly\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development (tests, linting, documentation):</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre> <p>This includes:</p> <ul> <li>pytest - Testing framework</li> <li>pytest-asyncio - Async test support</li> <li>pytest-cov - Coverage reporting</li> <li>ruff - Linting and formatting</li> <li>mypy - Type checking</li> <li>mkdocs - Documentation generation</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Test import\npython -c \"from data_loader import DataLoader; print('OK')\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#modulenotfounderror-no-module-named-data_loader","title":"ModuleNotFoundError: No module named 'data_loader'","text":"<p>Cause: Package not installed or PYTHONPATH not set.</p> <p>Solution:</p> <pre><code># Option 1: Install in development mode\npip install -e .\n\n# Option 2: Set PYTHONPATH\nexport PYTHONPATH=\"${PYTHONPATH}:$(pwd)/src\"\n</code></pre>"},{"location":"getting-started/installation/#permission-denied","title":"Permission denied","text":"<p>Cause: Virtual environment not activated or wrong Python version.</p> <p>Solution:</p> <pre><code># Ensure virtual environment is active\nsource venv/bin/activate\n\n# Check Python version\npython --version  # Should be 3.9+\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide gets you fetching financial data in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installation complete</li> <li>Configuration with API keys</li> </ul>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nimport aiohttp\nfrom data_loader import DataLoader\n\nasync def main():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # Fetch company profile\n        profile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        print(f\"Company: {profile.data['companyName']}\")\n        print(f\"Sector: {profile.data['sector']}\")\n        print(f\"Market Cap: ${profile.data['mktCap']:,.0f}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#fetching-from-multiple-providers","title":"Fetching from Multiple Providers","text":"<pre><code>async def comprehensive_analysis():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # Company data (FMP)\n        profile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        ratios = await loader.get_fmp_data(session, \"ratios\", symbol=\"AAPL\")\n\n        # Market data (Polygon)\n        aggs = await loader.get_polygon_data(\n            session, \"aggs_daily\",\n            symbol=\"SPY\",\n            start=\"2025-01-01\",\n            end=\"2025-01-31\"\n        )\n\n        # Economic context (FRED)\n        cpi = await loader.get_fred_data(session, \"series\", series_id=\"CPIAUCSL\")\n        fed_rate = await loader.get_fred_data(session, \"series\", series_id=\"FEDFUNDS\")\n\n        return {\n            \"company\": profile.data,\n            \"ratios\": ratios.data,\n            \"market\": aggs.data,\n            \"inflation\": cpi.data,\n            \"rates\": fed_rate.data,\n        }\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-response","title":"Understanding the Response","text":"<p>Each API call returns a <code>DataResult</code> object:</p> <pre><code>result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\nprint(result.data)        # The actual data (dict or list)\nprint(result.from_cache)  # True if served from cache\nprint(result.timestamp)   # When data was fetched\nprint(result.provider)    # 'fmp', 'polygon', or 'fred'\n</code></pre>"},{"location":"getting-started/quickstart/#caching-behavior","title":"Caching Behavior","text":"<p>Data is automatically cached:</p> <pre><code>async def caching_demo():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # First call: API request, cached\n        result1 = await loader.get_fmp_data(session, \"profile\", symbol=\"MSFT\")\n        print(f\"From cache: {result1.from_cache}\")  # False\n\n        # Second call: served from cache\n        result2 = await loader.get_fmp_data(session, \"profile\", symbol=\"MSFT\")\n        print(f\"From cache: {result2.from_cache}\")  # True\n\n        # Force fresh data\n        result3 = await loader.get_fmp_data(\n            session, \"profile\", symbol=\"MSFT\", use_cache=False\n        )\n        print(f\"From cache: {result3.from_cache}\")  # False\n</code></pre>"},{"location":"getting-started/quickstart/#error-handling","title":"Error Handling","text":"<pre><code>from data_loader import DataLoader\nfrom data_loader.exceptions import (\n    ProviderError,\n    RateLimitError,\n    ReadOnlyError,\n)\n\nasync def safe_fetch():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        try:\n            result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n            return result.data\n        except RateLimitError:\n            print(\"Rate limited - waiting...\")\n        except ProviderError as e:\n            print(f\"Provider error: {e}\")\n        except ReadOnlyError:\n            print(\"Data not in cache (READ_ONLY mode)\")\n</code></pre>"},{"location":"getting-started/quickstart/#health-monitoring","title":"Health Monitoring","text":"<pre><code>async def check_status():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        # Make some requests\n        await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        await loader.get_fred_data(session, \"series\", series_id=\"GDP\")\n\n    # Get health report\n    report = loader.get_api_health_report()\n\n    print(f\"Mode: {report['operating_mode']}\")\n    print(f\"Status: {report['overall_status']}\")\n\n    for provider, metrics in report['providers'].items():\n        success_rate = metrics['successful_requests'] / max(metrics['total_requests'], 1)\n        print(f\"{provider}: {success_rate:.0%} success rate\")\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>DataLoader Guide - Detailed usage patterns</li> <li>Providers - All available endpoints</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"guide/caching/","title":"Caching","text":"<p>OmniData Nexus Core includes an intelligent caching system to reduce API calls and improve performance.</p>"},{"location":"guide/caching/#overview","title":"Overview","text":"<ul> <li>Storage: Filesystem-based JSON cache</li> <li>TTL: Configurable time-to-live (default: 7 days)</li> <li>Atomic Writes: Safe concurrent access</li> <li>Per-Provider: Separate cache directories</li> </ul>"},{"location":"guide/caching/#cache-structure","title":"Cache Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 fmp_cache/\n\u2502   \u251c\u2500\u2500 profile_AAPL.json\n\u2502   \u251c\u2500\u2500 quote_MSFT.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 polygon_cache/\n\u2502   \u251c\u2500\u2500 aggs_daily_SPY_2025-01-01_2025-01-31.json\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 fred_cache/\n    \u251c\u2500\u2500 series_CPIAUCSL.json\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/caching/#automatic-caching","title":"Automatic Caching","text":"<p>All API responses are automatically cached:</p> <pre><code># First call: API request, result cached\nresult1 = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nprint(result1.from_cache)  # False\n\n# Second call: served from cache\nresult2 = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nprint(result2.from_cache)  # True\n</code></pre>"},{"location":"guide/caching/#cache-control","title":"Cache Control","text":""},{"location":"guide/caching/#bypass-cache","title":"Bypass Cache","text":"<pre><code># Force fresh data from API\nresult = await loader.get_fmp_data(\n    session, \"profile\", symbol=\"AAPL\", use_cache=False\n)\n</code></pre>"},{"location":"guide/caching/#cache-stats","title":"Cache Stats","text":"<pre><code>stats = loader._cache.get_stats()\n\nprint(f\"Total entries: {stats['total_entries']}\")\nprint(f\"Total size: {stats['total_size_mb']:.2f} MB\")\nprint(f\"Oldest entry: {stats['oldest_entry']}\")\nprint(f\"Newest entry: {stats['newest_entry']}\")\n</code></pre>"},{"location":"guide/caching/#clear-cache","title":"Clear Cache","text":"<pre><code># Clear all cache\nrm -rf data/fmp_cache data/polygon_cache data/fred_cache\n\n# Clear specific provider\nrm -rf data/fmp_cache\n</code></pre>"},{"location":"guide/caching/#ttl-configuration","title":"TTL Configuration","text":""},{"location":"guide/caching/#environment-variable","title":"Environment Variable","text":"<pre><code># In .env\nCACHE_TTL_DAYS=14  # 2 weeks\n</code></pre>"},{"location":"guide/caching/#programmatic","title":"Programmatic","text":"<pre><code>loader = DataLoader()\nloader._cache.ttl_days = 14\n</code></pre>"},{"location":"guide/caching/#cache-entry-format","title":"Cache Entry Format","text":"<p>Each cached file contains:</p> <pre><code>{\n  \"data\": { ... },\n  \"timestamp\": \"2025-01-31T10:30:00Z\",\n  \"provider\": \"fmp\",\n  \"endpoint\": \"profile\",\n  \"params\": {\n    \"symbol\": \"AAPL\"\n  },\n  \"ttl_days\": 7\n}\n</code></pre>"},{"location":"guide/caching/#cache-key-generation","title":"Cache Key Generation","text":"<p>Cache keys are generated from:</p> <ol> <li>Provider name</li> <li>Endpoint name</li> <li>Sorted parameters</li> </ol> <pre><code># These generate the same cache key:\nawait loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nawait loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n# This generates a different key:\nawait loader.get_fmp_data(session, \"profile\", symbol=\"MSFT\")\n</code></pre>"},{"location":"guide/caching/#read_only-mode","title":"READ_ONLY Mode","text":"<p>In READ_ONLY mode, only cached data is served:</p> <pre><code>from data_loader import DataLoader, OperatingMode\n\nloader = DataLoader()\nloader.set_operating_mode(OperatingMode.READ_ONLY)\n\n# Works if cached\nresult = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n# Raises ReadOnlyError if not cached\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"UNKNOWN\")\nexcept ReadOnlyError:\n    print(\"Data not in cache\")\n</code></pre>"},{"location":"guide/caching/#best-practices","title":"Best Practices","text":""},{"location":"guide/caching/#1-pre-warm-cache","title":"1. Pre-warm Cache","text":"<p>For batch analysis, pre-warm the cache:</p> <pre><code>async def prewarm_cache(symbols: list[str]):\n    loader = DataLoader()\n    async with aiohttp.ClientSession() as session:\n        for symbol in symbols:\n            await loader.get_fmp_data(session, \"profile\", symbol=symbol)\n            await loader.get_fmp_data(session, \"ratios\", symbol=symbol)\n    print(f\"Cached data for {len(symbols)} symbols\")\n</code></pre>"},{"location":"guide/caching/#2-use-appropriate-ttl","title":"2. Use Appropriate TTL","text":"<ul> <li>Real-time data (quotes): Short TTL or <code>use_cache=False</code></li> <li>Fundamentals (profiles): Default TTL (7 days)</li> <li>Historical data: Long TTL (30+ days)</li> </ul>"},{"location":"guide/caching/#3-monitor-cache-size","title":"3. Monitor Cache Size","text":"<pre><code>stats = loader._cache.get_stats()\nif stats['total_size_mb'] &gt; 1000:  # 1 GB\n    print(\"Consider clearing old cache entries\")\n</code></pre>"},{"location":"guide/caching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/caching/#cache-not-working","title":"Cache not working","text":"<pre><code># Check directory exists and is writable\nls -la data/\n\n# Check permissions\nchmod 755 data/\n</code></pre>"},{"location":"guide/caching/#stale-data","title":"Stale data","text":"<pre><code># Force refresh\nresult = await loader.get_fmp_data(\n    session, \"profile\", symbol=\"AAPL\", use_cache=False\n)\n</code></pre>"},{"location":"guide/caching/#cache-corruption","title":"Cache corruption","text":"<pre><code># Remove corrupted entry\nrm data/fmp_cache/profile_AAPL.json\n\n# Or clear all\nrm -rf data/\n</code></pre>"},{"location":"guide/dataloader/","title":"DataLoader Guide","text":"<p>The <code>DataLoader</code> class provides a unified interface for fetching data from all supported providers.</p>"},{"location":"guide/dataloader/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nimport aiohttp\nfrom data_loader import DataLoader\n\nasync def main():\n    loader = DataLoader()\n\n    async with aiohttp.ClientSession() as session:\n        result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n        print(result.data)\n\nasyncio.run(main())\n</code></pre>"},{"location":"guide/dataloader/#provider-methods","title":"Provider Methods","text":""},{"location":"guide/dataloader/#fmp-financial-modeling-prep","title":"FMP (Financial Modeling Prep)","text":"<pre><code># Company profile\nprofile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n# Real-time quote\nquote = await loader.get_fmp_data(session, \"quote\", symbol=\"AAPL\")\n\n# Financial statements\nincome = await loader.get_fmp_data(session, \"income_statement\", symbol=\"AAPL\", period=\"annual\")\nbalance = await loader.get_fmp_data(session, \"balance_sheet\", symbol=\"AAPL\", period=\"quarterly\")\ncashflow = await loader.get_fmp_data(session, \"cash_flow\", symbol=\"AAPL\")\n\n# Metrics and ratios\nratios = await loader.get_fmp_data(session, \"ratios\", symbol=\"AAPL\")\ngrowth = await loader.get_fmp_data(session, \"growth\", symbol=\"AAPL\")\nkey_metrics = await loader.get_fmp_data(session, \"key_metrics\", symbol=\"AAPL\")\n\n# Ownership data\ninsider = await loader.get_fmp_data(session, \"insider_trading\", symbol=\"AAPL\")\ninstitutional = await loader.get_fmp_data(session, \"institutional_ownership\", symbol=\"AAPL\")\n\n# Calendar and screening\nearnings = await loader.get_fmp_data(session, \"earnings_calendar\", **{\"from\": \"2025-01-01\", \"to\": \"2025-01-31\"})\nscreened = await loader.get_fmp_data(session, \"screener\", marketCapMoreThan=1000000000, sector=\"Technology\")\n</code></pre>"},{"location":"guide/dataloader/#polygon","title":"Polygon","text":"<pre><code># Daily aggregates\naggs = await loader.get_polygon_data(\n    session, \"aggs_daily\",\n    symbol=\"SPY\",\n    start=\"2025-01-01\",\n    end=\"2025-01-31\"\n)\n\n# Tick-level trades\ntrades = await loader.get_polygon_data(session, \"trades\", symbol=\"AAPL\")\n\n# Options data\noptions = await loader.get_polygon_data(session, \"options_snapshot\", underlying=\"AAPL\")\n\n# Market snapshot\nsnapshot = await loader.get_polygon_data(session, \"market_snapshot\")\n</code></pre>"},{"location":"guide/dataloader/#fred","title":"FRED","text":"<pre><code># Economic series\ngdp = await loader.get_fred_data(session, \"series\", series_id=\"GDP\")\ncpi = await loader.get_fred_data(session, \"series\", series_id=\"CPIAUCSL\")\nunemployment = await loader.get_fred_data(session, \"series\", series_id=\"UNRATE\")\nfed_funds = await loader.get_fred_data(session, \"series\", series_id=\"FEDFUNDS\")\n</code></pre>"},{"location":"guide/dataloader/#dataresult-object","title":"DataResult Object","text":"<p>All provider methods return a <code>DataResult</code> object:</p> <pre><code>@dataclass\nclass DataResult:\n    data: Any           # The fetched data\n    from_cache: bool    # True if served from cache\n    timestamp: str      # ISO timestamp\n    provider: str       # 'fmp', 'polygon', or 'fred'\n    endpoint: str       # The endpoint name\n</code></pre>"},{"location":"guide/dataloader/#accessing-data","title":"Accessing Data","text":"<pre><code>result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n\n# Access the data\ncompany_name = result.data['companyName']\nmarket_cap = result.data['mktCap']\n\n# Check if from cache\nif result.from_cache:\n    print(\"Served from cache\")\n</code></pre>"},{"location":"guide/dataloader/#caching-control","title":"Caching Control","text":""},{"location":"guide/dataloader/#default-behavior","title":"Default Behavior","text":"<p>Data is automatically cached for the configured TTL (default: 7 days).</p> <pre><code># First call: fetches from API\nresult1 = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nprint(result1.from_cache)  # False\n\n# Second call: served from cache\nresult2 = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nprint(result2.from_cache)  # True\n</code></pre>"},{"location":"guide/dataloader/#bypass-cache","title":"Bypass Cache","text":"<pre><code># Force fresh data\nresult = await loader.get_fmp_data(\n    session, \"profile\", symbol=\"AAPL\", use_cache=False\n)\n</code></pre>"},{"location":"guide/dataloader/#cache-stats","title":"Cache Stats","text":"<pre><code>stats = loader._cache.get_stats()\nprint(f\"Total entries: {stats['total_entries']}\")\nprint(f\"Total size: {stats['total_size_mb']:.2f} MB\")\n</code></pre>"},{"location":"guide/dataloader/#operating-modes","title":"Operating Modes","text":""},{"location":"guide/dataloader/#live-mode-default","title":"LIVE Mode (Default)","text":"<p>Makes API calls when data is not cached:</p> <pre><code>loader = DataLoader()  # LIVE mode by default\n</code></pre>"},{"location":"guide/dataloader/#read_only-mode","title":"READ_ONLY Mode","text":"<p>Only serves cached data, never makes API calls:</p> <pre><code>from data_loader import DataLoader, OperatingMode\n\nloader = DataLoader()\nloader.set_operating_mode(OperatingMode.READ_ONLY)\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nexcept ReadOnlyError:\n    print(\"Data not in cache\")\n</code></pre>"},{"location":"guide/dataloader/#checking-mode","title":"Checking Mode","text":"<pre><code>current_mode = loader.get_operating_mode()\nprint(f\"Mode: {current_mode}\")\n</code></pre>"},{"location":"guide/dataloader/#concurrent-requests","title":"Concurrent Requests","text":"<p>The DataLoader manages concurrency limits per provider:</p> Provider Concurrency Limit FMP 3 Polygon 10 FRED 1 <pre><code># These are automatically throttled\nawait asyncio.gather(\n    loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\"),\n    loader.get_fmp_data(session, \"profile\", symbol=\"MSFT\"),\n    loader.get_fmp_data(session, \"profile\", symbol=\"GOOGL\"),\n    loader.get_fmp_data(session, \"profile\", symbol=\"AMZN\"),  # Waits for slot\n)\n</code></pre>"},{"location":"guide/dataloader/#error-handling","title":"Error Handling","text":"<pre><code>from data_loader.exceptions import (\n    ProviderError,\n    RateLimitError,\n    CircuitBreakerOpenError,\n    ReadOnlyError,\n)\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nexcept RateLimitError:\n    # Rate limited - automatic backoff applied\n    pass\nexcept CircuitBreakerOpenError:\n    # Too many failures - circuit is open\n    pass\nexcept ReadOnlyError:\n    # READ_ONLY mode and data not cached\n    pass\nexcept ProviderError as e:\n    # General provider error\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"guide/error-handling/","title":"Error Handling","text":"<p>OmniData Nexus Core includes a comprehensive resilience layer for handling API errors.</p>"},{"location":"guide/error-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>DataLoaderError (base)\n\u251c\u2500\u2500 ProviderError\n\u2502   \u251c\u2500\u2500 RateLimitError\n\u2502   \u251c\u2500\u2500 AuthenticationError\n\u2502   \u2514\u2500\u2500 EndpointNotFoundError\n\u251c\u2500\u2500 CircuitBreakerOpenError\n\u251c\u2500\u2500 ReadOnlyError\n\u2514\u2500\u2500 CacheError\n</code></pre>"},{"location":"guide/error-handling/#common-errors","title":"Common Errors","text":""},{"location":"guide/error-handling/#ratelimiterror","title":"RateLimitError","text":"<p>Raised when the API returns HTTP 429 (Too Many Requests).</p> <pre><code>from data_loader.exceptions import RateLimitError\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nexcept RateLimitError as e:\n    print(f\"Rate limited: {e}\")\n    print(f\"Retry after: {e.retry_after} seconds\")\n</code></pre> <p>The system automatically applies exponential backoff before raising this error.</p>"},{"location":"guide/error-handling/#circuitbreakeropenerror","title":"CircuitBreakerOpenError","text":"<p>Raised when too many consecutive failures occur.</p> <pre><code>from data_loader.exceptions import CircuitBreakerOpenError\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nexcept CircuitBreakerOpenError as e:\n    print(f\"Circuit open for {e.provider}\")\n    print(f\"Recovery in: {e.recovery_time} seconds\")\n</code></pre>"},{"location":"guide/error-handling/#readonlyerror","title":"ReadOnlyError","text":"<p>Raised in READ_ONLY mode when data is not cached.</p> <pre><code>from data_loader.exceptions import ReadOnlyError\n\nloader.set_operating_mode(OperatingMode.READ_ONLY)\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"UNKNOWN\")\nexcept ReadOnlyError as e:\n    print(f\"Not cached: {e.provider}:{e.endpoint}\")\n</code></pre>"},{"location":"guide/error-handling/#providererror","title":"ProviderError","text":"<p>General provider errors (5xx, network issues).</p> <pre><code>from data_loader.exceptions import ProviderError\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nexcept ProviderError as e:\n    print(f\"Provider error: {e}\")\n    print(f\"Status code: {e.status_code}\")\n</code></pre>"},{"location":"guide/error-handling/#resilience-patterns","title":"Resilience Patterns","text":""},{"location":"guide/error-handling/#circuit-breaker","title":"Circuit Breaker","text":"<p>Prevents cascading failures by opening the circuit after consecutive errors.</p> <pre><code>CLOSED \u2192 (failures \u2265 threshold) \u2192 OPEN\nOPEN \u2192 (timeout expires) \u2192 HALF_OPEN\nHALF_OPEN \u2192 (success) \u2192 CLOSED\nHALF_OPEN \u2192 (failure) \u2192 OPEN\n</code></pre> <p>Configuration:</p> Parameter Default Description Failure threshold 5 Opens circuit after N failures Recovery timeout 60s Time before trying again Success threshold 2 Successes to close circuit"},{"location":"guide/error-handling/#exponential-backoff","title":"Exponential Backoff","text":"<p>Automatically retries failed requests with increasing delays.</p> <pre><code>Attempt 1: immediate\nAttempt 2: wait 1s + jitter\nAttempt 3: wait 2s + jitter\nAttempt 4: wait 4s + jitter\n(max 3 retries by default)\n</code></pre>"},{"location":"guide/error-handling/#rate-limiting","title":"Rate Limiting","text":"<p>Respects <code>Retry-After</code> headers and applies QoS limits:</p> Provider Concurrency FMP 3 Polygon 10 FRED 1"},{"location":"guide/error-handling/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"guide/error-handling/#basic-tryexcept","title":"Basic Try/Except","text":"<pre><code>from data_loader.exceptions import DataLoaderError\n\ntry:\n    result = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\n    process(result.data)\nexcept DataLoaderError as e:\n    log.error(f\"Failed to fetch data: {e}\")\n</code></pre>"},{"location":"guide/error-handling/#specific-error-handling","title":"Specific Error Handling","text":"<pre><code>from data_loader.exceptions import (\n    RateLimitError,\n    CircuitBreakerOpenError,\n    ReadOnlyError,\n    ProviderError,\n)\n\nasync def fetch_with_fallback(session, symbol: str):\n    try:\n        return await loader.get_fmp_data(session, \"profile\", symbol=symbol)\n    except RateLimitError:\n        # Wait and retry\n        await asyncio.sleep(60)\n        return await loader.get_fmp_data(session, \"profile\", symbol=symbol)\n    except CircuitBreakerOpenError:\n        # Try alternative provider or return cached\n        return get_from_backup(symbol)\n    except ReadOnlyError:\n        # Data not available offline\n        return None\n    except ProviderError:\n        # Log and continue\n        log.warning(f\"Could not fetch {symbol}\")\n        return None\n</code></pre>"},{"location":"guide/error-handling/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>async def fetch_company_data(session, symbol: str) -&gt; dict:\n    \"\"\"Fetch as much data as possible, even if some fails.\"\"\"\n    data = {}\n\n    # Required data\n    try:\n        profile = await loader.get_fmp_data(session, \"profile\", symbol=symbol)\n        data['profile'] = profile.data\n    except DataLoaderError:\n        raise  # Can't continue without profile\n\n    # Optional data - continue if fails\n    try:\n        ratios = await loader.get_fmp_data(session, \"ratios\", symbol=symbol)\n        data['ratios'] = ratios.data\n    except DataLoaderError:\n        data['ratios'] = None\n\n    try:\n        insider = await loader.get_fmp_data(session, \"insider_trading\", symbol=symbol)\n        data['insider'] = insider.data\n    except DataLoaderError:\n        data['insider'] = None\n\n    return data\n</code></pre>"},{"location":"guide/error-handling/#health-monitoring","title":"Health Monitoring","text":"<p>Check provider status before making requests:</p> <pre><code>report = loader.get_api_health_report()\n\nfor provider, metrics in report['providers'].items():\n    if metrics['status'] == 'FAIL':\n        print(f\"\u26a0\ufe0f {provider} is unhealthy\")\n        print(f\"  Error rate: {metrics['error_rate']:.1%}\")\n        print(f\"  Last error: {metrics['last_error']}\")\n</code></pre>"},{"location":"guide/error-handling/#debugging","title":"Debugging","text":"<p>Enable debug logging for detailed error information:</p> <pre><code>import logging\n\nlogging.getLogger('data_loader').setLevel(logging.DEBUG)\n</code></pre> <p>Or in <code>.env</code>:</p> <pre><code>LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"guide/providers/","title":"Providers","text":"<p>OmniData Nexus Core supports three data providers, each with specialized endpoints.</p>"},{"location":"guide/providers/#fmp-financial-modeling-prep","title":"FMP (Financial Modeling Prep)","text":"<p>13 endpoints for company fundamentals and financial data.</p>"},{"location":"guide/providers/#company-information","title":"Company Information","text":"Endpoint Description Parameters <code>profile</code> Company profile <code>symbol</code> <code>quote</code> Real-time quote <code>symbol</code> <pre><code>profile = await loader.get_fmp_data(session, \"profile\", symbol=\"AAPL\")\nquote = await loader.get_fmp_data(session, \"quote\", symbol=\"AAPL\")\n</code></pre>"},{"location":"guide/providers/#financial-statements","title":"Financial Statements","text":"Endpoint Description Parameters <code>income_statement</code> Income statement <code>symbol</code>, <code>period</code> <code>balance_sheet</code> Balance sheet <code>symbol</code>, <code>period</code> <code>cash_flow</code> Cash flow statement <code>symbol</code>, <code>period</code> <pre><code># Annual statements\nincome = await loader.get_fmp_data(\n    session, \"income_statement\", symbol=\"AAPL\", period=\"annual\"\n)\n\n# Quarterly statements\nbalance = await loader.get_fmp_data(\n    session, \"balance_sheet\", symbol=\"AAPL\", period=\"quarterly\"\n)\n</code></pre>"},{"location":"guide/providers/#metrics-and-ratios","title":"Metrics and Ratios","text":"Endpoint Description Parameters <code>ratios</code> Financial ratios <code>symbol</code> <code>growth</code> Growth metrics <code>symbol</code> <code>key_metrics</code> Key metrics <code>symbol</code> <pre><code>ratios = await loader.get_fmp_data(session, \"ratios\", symbol=\"AAPL\")\ngrowth = await loader.get_fmp_data(session, \"growth\", symbol=\"AAPL\")\nmetrics = await loader.get_fmp_data(session, \"key_metrics\", symbol=\"AAPL\")\n</code></pre>"},{"location":"guide/providers/#historical-data","title":"Historical Data","text":"Endpoint Description Parameters <code>historical_price</code> Historical OHLCV <code>symbol</code>, <code>from</code>, <code>to</code> <code>earnings_calendar</code> Earnings dates <code>from</code>, <code>to</code> <pre><code>history = await loader.get_fmp_data(\n    session, \"historical_price\",\n    symbol=\"AAPL\",\n    **{\"from\": \"2024-01-01\", \"to\": \"2024-12-31\"}\n)\n\nearnings = await loader.get_fmp_data(\n    session, \"earnings_calendar\",\n    **{\"from\": \"2025-01-01\", \"to\": \"2025-03-31\"}\n)\n</code></pre>"},{"location":"guide/providers/#ownership","title":"Ownership","text":"Endpoint Description Parameters <code>insider_trading</code> Insider trades <code>symbol</code> <code>institutional_ownership</code> Institutional holders <code>symbol</code> <pre><code>insider = await loader.get_fmp_data(session, \"insider_trading\", symbol=\"AAPL\")\ninst = await loader.get_fmp_data(session, \"institutional_ownership\", symbol=\"AAPL\")\n</code></pre>"},{"location":"guide/providers/#screening","title":"Screening","text":"Endpoint Description Parameters <code>screener</code> Stock screener Multiple filters <pre><code># Find large tech companies\nresults = await loader.get_fmp_data(\n    session, \"screener\",\n    marketCapMoreThan=100000000000,\n    sector=\"Technology\",\n    isActivelyTrading=True\n)\n</code></pre>"},{"location":"guide/providers/#polygonio","title":"Polygon.io","text":"<p>4 endpoints for market data and options.</p> Endpoint Description Parameters <code>aggs_daily</code> Daily aggregates <code>symbol</code>, <code>start</code>, <code>end</code> <code>trades</code> Tick-level trades <code>symbol</code> <code>options_snapshot</code> Options chain <code>underlying</code> <code>market_snapshot</code> Market overview - <pre><code># Daily bars\naggs = await loader.get_polygon_data(\n    session, \"aggs_daily\",\n    symbol=\"SPY\",\n    start=\"2025-01-01\",\n    end=\"2025-01-31\"\n)\n\n# Recent trades\ntrades = await loader.get_polygon_data(session, \"trades\", symbol=\"AAPL\")\n\n# Options chain\noptions = await loader.get_polygon_data(\n    session, \"options_snapshot\", underlying=\"SPY\"\n)\n\n# Market overview\nsnapshot = await loader.get_polygon_data(session, \"market_snapshot\")\n</code></pre>"},{"location":"guide/providers/#fred-federal-reserve-economic-data","title":"FRED (Federal Reserve Economic Data)","text":"<p>32+ macroeconomic series.</p>"},{"location":"guide/providers/#inflation","title":"Inflation","text":"Series ID Description <code>CPIAUCSL</code> Consumer Price Index <code>PCEPI</code> PCE Price Index <code>CPILFESL</code> Core CPI <code>T10YIE</code> 10-Year Breakeven Inflation"},{"location":"guide/providers/#labor-market","title":"Labor Market","text":"Series ID Description <code>UNRATE</code> Unemployment Rate <code>PAYEMS</code> Nonfarm Payrolls <code>ICSA</code> Initial Claims <code>AWHAETP</code> Average Weekly Hours"},{"location":"guide/providers/#growth","title":"Growth","text":"Series ID Description <code>GDP</code> Gross Domestic Product <code>GDPC1</code> Real GDP <code>INDPRO</code> Industrial Production <code>RSXFS</code> Retail Sales"},{"location":"guide/providers/#interest-rates","title":"Interest Rates","text":"Series ID Description <code>FEDFUNDS</code> Federal Funds Rate <code>DGS10</code> 10-Year Treasury <code>DGS2</code> 2-Year Treasury <code>T10Y2Y</code> 10Y-2Y Spread"},{"location":"guide/providers/#housing","title":"Housing","text":"Series ID Description <code>HOUST</code> Housing Starts <code>CSUSHPINSA</code> Case-Shiller Home Price <code>MORTGAGE30US</code> 30-Year Mortgage Rate"},{"location":"guide/providers/#usage","title":"Usage","text":"<pre><code># Fetch any FRED series\ncpi = await loader.get_fred_data(session, \"series\", series_id=\"CPIAUCSL\")\ngdp = await loader.get_fred_data(session, \"series\", series_id=\"GDP\")\nrates = await loader.get_fred_data(session, \"series\", series_id=\"FEDFUNDS\")\n\n# Access observations\nfor obs in cpi.data['observations'][:5]:\n    print(f\"{obs['date']}: {obs['value']}\")\n</code></pre>"},{"location":"guide/providers/#provider-configuration","title":"Provider Configuration","text":""},{"location":"guide/providers/#rate-limits","title":"Rate Limits","text":"Provider Free Tier Concurrency FMP 250/day 3 Polygon Limited 10 FRED Unlimited 1"},{"location":"guide/providers/#api-base-urls","title":"API Base URLs","text":"Provider Base URL FMP <code>https://financialmodelingprep.com/stable/</code> Polygon <code>https://api.polygon.io/</code> FRED <code>https://api.stlouisfed.org/fred/</code>"}]}